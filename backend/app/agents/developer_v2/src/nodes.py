"""Developer V2 Graph Nodes - ReAct Agents with Multi-Tool Support."""
import json
import logging
import os
import re
import subprocess
from datetime import datetime
from pathlib import Path
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_tavily import TavilySearch


def _write_test_log(task_id: str, test_output: str, status: str = "FAIL"):
    """Write test output to logs/developer/test_log directory."""
    try:
        # Create logs directory
        log_dir = Path("logs/developer/test_log")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_task_id = re.sub(r'[^\w\-]', '_', task_id or "unknown")
        filename = f"{safe_task_id}_{timestamp}_{status}.log"
        
        log_path = log_dir / filename
        
        # Write log content
        content = f"""{'='*60}
TEST LOG - {status}
Task: {task_id}
Time: {datetime.now().isoformat()}
{'='*60}

{test_output}
"""
        log_path.write_text(content, encoding='utf-8')
        logger.info(f"[run_code] Test log saved: {log_path}")
        
    except Exception as e:
        logger.warning(f"[run_code] Failed to write test log: {e}")

from app.agents.developer_v2.src.state import DeveloperState
from app.agents.developer_v2.src.schemas import (
    StoryAnalysis, ImplementationPlan, PlanStep, CodeChange,
    RoutingDecision, SystemDesign, DebugResult
)
from app.agents.developer_v2.src.tools import set_tool_context
from app.agents.developer_v2.src.tools.filesystem_tools import (
    set_fs_context, read_file_safe, write_file_safe, list_directory_safe, edit_file
)
from app.agents.developer_v2.src.tools.shell_tools import (
    set_shell_context, execute_shell, semantic_code_search
)
from app.agents.developer_v2.src.tools.container_tools import (
    dev_container_manager, set_container_context, get_container_tools,
    container_exec, container_logs, container_status,
)
from langchain_core.messages import ToolMessage
from app.agents.developer_v2.src.utils.llm_utils import (
    get_langfuse_config as _cfg,
    execute_llm_with_tools as _llm_with_tools,
    clean_json_response as _clean_json,
    extract_json_from_messages as _extract_json_response,
)
from app.agents.developer_v2.src.utils.prompt_utils import (
    get_prompt as _get_prompt,
    format_input_template as _format_input_template,
    build_system_prompt as _build_system_prompt,
)
from app.agents.developer_v2.src.tools import (
    # Workspace management
    setup_git_worktree,
    index_workspace,
    # Project context
    get_agents_md,
    get_project_context,
    get_boilerplate_examples,
    get_markdown_code_block_type,
    # CocoIndex
    get_related_code_indexed,
    search_codebase,
    # Execution
    detect_test_command,
    execute_command_async,
    install_dependencies,
    find_test_file,
)

logger = logging.getLogger(__name__)

# LLM models
_fast_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, timeout=20)
_code_llm = ChatOpenAI(model="gpt-4.1", temperature=0.2, timeout=120)


# =============================================================================
# RULE-BASED HELPERS (No LLM)
# =============================================================================


def _analyze_test_output(stdout: str, stderr: str, project_type: str = "") -> dict:
    """Analyze test results using regex patterns (no LLM needed)."""
    combined = f"{stdout}\n{stderr}"
    
    # === PASS patterns ===
    
    # Python pytest: "X passed" or "OK"
    pytest_pass = re.search(r"(\d+) passed", combined)
    if pytest_pass:
        passed_count = pytest_pass.group(1)
        failed_match = re.search(r"(\d+) failed", combined)
        if not failed_match or failed_match.group(1) == "0":
            return {"status": "PASS", "summary": f"{passed_count} tests passed"}
    
    # Python unittest: "OK"
    if re.search(r"Ran \d+ tests? in [\d.]+s\s*\n\s*OK", combined):
        return {"status": "PASS", "summary": "All tests passed (unittest)"}
    
    # Node/Jest: "Tests: X passed"
    jest_pass = re.search(r"Tests:\s*(\d+)\s*passed", combined)
    if jest_pass and "failed" not in combined.lower():
        return {"status": "PASS", "summary": f"{jest_pass.group(1)} tests passed (Jest)"}
    
    # Node/Mocha: "X passing"
    mocha_pass = re.search(r"(\d+)\s+passing", combined)
    if mocha_pass and "failing" not in combined.lower():
        return {"status": "PASS", "summary": f"{mocha_pass.group(1)} tests passing (Mocha)"}
    
    # Rust: "test result: ok"
    if re.search(r"test result: ok\.", combined, re.IGNORECASE):
        return {"status": "PASS", "summary": "All tests passed (Cargo)"}
    
    # Go: "ok" at end or "PASS"
    if re.search(r"^ok\s+", combined, re.MULTILINE) or re.search(r"^PASS$", combined, re.MULTILINE):
        return {"status": "PASS", "summary": "All tests passed (Go)"}
    
    # Generic: "All tests passed" or similar
    if re.search(r"all\s+tests?\s+pass", combined, re.IGNORECASE):
        return {"status": "PASS", "summary": "All tests passed"}
    
    # === FAIL patterns ===
    
    # Python: "X failed" or "FAILED"
    pytest_fail = re.search(r"(\d+) failed", combined)
    if pytest_fail and int(pytest_fail.group(1)) > 0:
        return {"status": "FAIL", "summary": f"{pytest_fail.group(1)} tests failed"}
    
    if re.search(r"FAILED|AssertionError|Error:|Traceback", combined):
        return {"status": "FAIL", "summary": "Tests failed with errors"}
    
    # Node/Jest: "X failed"
    jest_fail = re.search(r"Tests:\s*\d+\s*failed", combined)
    if jest_fail:
        return {"status": "FAIL", "summary": "Tests failed (Jest)"}
    
    # Node/Mocha: "X failing"
    mocha_fail = re.search(r"(\d+)\s+failing", combined)
    if mocha_fail and int(mocha_fail.group(1)) > 0:
        return {"status": "FAIL", "summary": f"{mocha_fail.group(1)} tests failing (Mocha)"}
    
    # Rust: "test result: FAILED"
    if re.search(r"test result: FAILED", combined, re.IGNORECASE):
        return {"status": "FAIL", "summary": "Tests failed (Cargo)"}
    
    # Go: "FAIL"
    if re.search(r"^FAIL\s+", combined, re.MULTILINE):
        return {"status": "FAIL", "summary": "Tests failed (Go)"}
    
    # Generic error indicators
    if re.search(r"error:|exception:|failed|failure", combined, re.IGNORECASE):
        # But not if it's just "0 failed" or similar
        if not re.search(r"0\s+(failed|failures?|errors?)", combined, re.IGNORECASE):
            return {"status": "FAIL", "summary": "Tests failed with errors"}
    
    # === No tests or unknown ===
    
    # No tests found
    if re.search(r"no tests (found|ran|collected)", combined, re.IGNORECASE):
        return {"status": "PASS", "summary": "No tests found (skipped)"}
    
    # Exit code based (if we can detect it)
    if "exit code: 0" in combined.lower() or "exited with code 0" in combined.lower():
        return {"status": "PASS", "summary": "Command succeeded"}
    
    if "exit code:" in combined.lower() and "exit code: 0" not in combined.lower():
        return {"status": "FAIL", "summary": "Command failed with non-zero exit code"}
    
    # Default: assume PASS if no errors detected
    return {"status": "PASS", "summary": "Test execution completed"}


def _run_with_isolated_db(workspace_path: str, commands: list, timeout_per_cmd: int = 120) -> dict:
    """Run commands with isolated PostgreSQL container using testcontainers.
    
    Each agent gets its own DB container - no conflicts with parallel execution.
    Container is auto-destroyed after execution.
    
    Args:
        workspace_path: Project workspace path
        commands: List of shell commands to execute
        timeout_per_cmd: Timeout per command in seconds
        
    Returns:
        dict with status, stdout, stderr
    """
    try:
        from testcontainers.postgres import PostgresContainer
    except ImportError:
        logger.warning("[_run_with_isolated_db] testcontainers not installed, falling back to direct execution")
        return {"status": "SKIP", "stdout": "", "stderr": "testcontainers not installed"}
    
    logger.info(f"[_run_with_isolated_db] Starting isolated PostgreSQL container...")
    
    try:
        with PostgresContainer("postgres:16-alpine") as postgres:
            # Get connection URL in Prisma format
            db_url = postgres.get_connection_url()
            # testcontainers returns: postgresql+psycopg://user:pass@host:port/db
            # Prisma needs: postgresql://user:pass@host:port/db
            db_url = db_url.replace("postgresql+psycopg://", "postgresql://")
            
            logger.info(f"[_run_with_isolated_db] Container started, DB_URL: {db_url[:50]}...")
            
            stdout_all = ""
            stderr_all = ""
            status = "PASS"
            
            # Prepare environment with DATABASE_URL
            env = os.environ.copy()
            env["DATABASE_URL"] = db_url
            
            for cmd in commands:
                if not cmd:
                    continue
                    
                logger.info(f"[_run_with_isolated_db] Running: {cmd}")
                
                try:
                    # Determine shell based on OS
                    if os.name == "nt":  # Windows
                        shell_cmd = ["cmd", "/c", cmd]
                        use_shell = False
                    else:
                        shell_cmd = cmd
                        use_shell = True
                    
                    result = subprocess.run(
                        shell_cmd,
                        cwd=workspace_path,
                        shell=use_shell,
                        capture_output=True,
                        text=True,
                        timeout=timeout_per_cmd,
                        env=env,
                    )
                    
                    stdout_all += f"\n=== {cmd} ===\n{result.stdout}"
                    stderr_all += result.stderr
                    
                    if result.returncode != 0:
                        logger.warning(f"[_run_with_isolated_db] Command failed: {cmd} (exit={result.returncode})")
                        # Don't break on prisma generate/push failures, only on test failures
                        if "test" in cmd.lower():
                            status = "FAIL"
                            break
                            
                except subprocess.TimeoutExpired:
                    stderr_all += f"\nCommand timed out: {cmd}\n"
                    logger.error(f"[_run_with_isolated_db] Timeout: {cmd}")
                    status = "FAIL"
                    break
                except Exception as e:
                    stderr_all += f"\nCommand error: {cmd} - {str(e)}\n"
                    logger.error(f"[_run_with_isolated_db] Error: {cmd} - {e}")
            
            logger.info(f"[_run_with_isolated_db] Completed with status: {status}")
            return {"status": status, "stdout": stdout_all, "stderr": stderr_all}
            
    except Exception as e:
        logger.error(f"[_run_with_isolated_db] Container error: {e}", exc_info=True)
        return {"status": "ERROR", "stdout": "", "stderr": f"Container error: {str(e)}"}


# =============================================================================
# TOOL CONTEXT SETUP
# =============================================================================

def _setup_tool_context(workspace_path: str = None, project_id: str = None, task_id: str = None):
    """Set global context for all tools before agent invocation."""
    if workspace_path:
        set_fs_context(root_dir=workspace_path)
        set_shell_context(root_dir=workspace_path)
    if project_id:
        set_tool_context(project_id=project_id, task_id=task_id, workspace_path=workspace_path)

async def router(state: DeveloperState, agent=None) -> DeveloperState:
    """
    Route story to appropriate processing node.
    """
    print("[NODE] router - Analyzing story intent...")
    try:
        has_analysis = bool(state.get("analysis_result"))
        has_plan = bool(state.get("implementation_plan"))
        has_implementation = bool(state.get("code_changes"))
        
        # Check if this is a valid story task (has meaningful content)
        story_content = state.get("story_content", "")
        is_story_task = len(story_content) > 50  # Story with sufficient detail
        
        # Build input from template
        input_text = _format_input_template(
            "routing_decision",
            story_title=state.get("story_title", "Untitled"),
            story_content=story_content,
            acceptance_criteria=chr(10).join(state.get("acceptance_criteria", [])),
            has_analysis=has_analysis,
            has_plan=has_plan,
            has_implementation=has_implementation
        )

        # Use with_structured_output for reliable response
        messages = [
            SystemMessage(content=_build_system_prompt("routing_decision")),
            HumanMessage(content=input_text)
        ]
        
        structured_llm = _fast_llm.with_structured_output(RoutingDecision)
        result = await structured_llm.ainvoke(messages, config=_cfg(state, "router"))
        
        action = result.action
        task_type = result.task_type
        complexity = result.complexity
        message = result.message
        reason = result.reason
        confidence = result.confidence
        
        if is_story_task:
            # Never return RESPOND or CLARIFY for story tasks
            if action in ("RESPOND", "CLARIFY"):
                logger.info(f"[router] Story task detected, forcing ANALYZE instead of {action}")
                action = "ANALYZE"
            # Must analyze before plan/implement
            elif action in ("PLAN", "IMPLEMENT") and not has_analysis:
                logger.info(f"[router] No analysis yet, forcing ANALYZE instead of {action}")
                action = "ANALYZE"
        
        logger.info(f"[router] Decision: action={action}, type={task_type}, complexity={complexity}")
        
        return {
            **state,
            "action": action,
            "task_type": task_type,
            "complexity": complexity,
            "message": message,
            "reason": reason,
            "confidence": confidence,
        }
        
    except Exception as e:
        logger.error(f"[router] Error: {e}", exc_info=True)
        return {
            **state,
            "action": "ANALYZE",
            "task_type": "feature",
            "complexity": "medium",
            "message": "Báº¯t Ä‘áº§u phÃ¢n tÃ­ch story...",
            "reason": f"Router error, defaulting to ANALYZE: {str(e)}",
            "confidence": 0.5,
        }


async def setup_workspace(state: DeveloperState, agent=None) -> DeveloperState:
    """
    Setup git workspace/branch only when code modification is needed.
    """
    print("[NODE] setup_workspace - Setting up workspace...")
    try:
        story_id = state.get("story_id", state.get("task_id", "unknown"))
        
        # Check if workspace already setup
        if state.get("workspace_ready"):
            logger.info("[setup_workspace] Workspace already ready, skipping")
            return state
        
        # Setup workspace via agent
        short_id = story_id.split('-')[-1][:8] if '-' in story_id else story_id[:8]
        branch_name = f"story_{short_id}"
        
        logger.info(f"[setup_workspace] Setting up workspace for branch '{branch_name}'")
        
        # Setup workspace using tools directly
        
        # Get main_workspace from agent - try multiple attributes
        if hasattr(agent, 'main_workspace'):
            main_workspace = agent.main_workspace
        elif hasattr(agent, 'workspace_path'):
            main_workspace = agent.workspace_path
        else:
            logger.warning("[setup_workspace] Agent has no workspace path attribute")
            return {**state, "workspace_ready": False, "index_ready": False}
        
        workspace_info = setup_git_worktree(
            story_id=story_id,
            main_workspace=main_workspace,
            agent_name=agent.name
        )
            
            # Index workspace with CocoIndex for semantic search
        index_ready = False
        workspace_path = workspace_info.get("workspace_path", "")
        project_id = state.get("project_id", "default")
        task_id = state.get("task_id") or story_id
        
        if workspace_path:
            index_ready = index_workspace(project_id, workspace_path, task_id)
            if not index_ready:
                raise RuntimeError(f"CocoIndex indexing failed for workspace: {workspace_path}")
            logger.info(f"[setup_workspace] Indexed workspace with CocoIndex")
        
        # Load project context (AGENTS.md)
        project_context = ""
        agents_md = ""
        if workspace_path:
            try:
                agents_md = get_agents_md(workspace_path)
                project_context = get_project_context(workspace_path)
                if agents_md:
                    logger.info(f"[setup_workspace] Loaded AGENTS.md: {len(agents_md)} chars")
            except Exception as ctx_err:
                logger.warning(f"[setup_workspace] Failed to load project context: {ctx_err}")
        
        # Log project config if provided
        project_config = state.get("project_config", {})
        if project_config:
            logger.info(f"[setup_workspace] Using project_config: {project_config}")
        
        return {
            **state,
            "workspace_path": workspace_info["workspace_path"],
            "branch_name": workspace_info["branch_name"],
            "main_workspace": workspace_info["main_workspace"],
            "workspace_ready": workspace_info["workspace_ready"],
            "index_ready": index_ready,
            "agents_md": agents_md,
            "project_context": project_context,
        }
        
    except Exception as e:
        logger.error(f"[setup_workspace] Error: {e}", exc_info=True)
        return {
            **state,
            "workspace_ready": False,
            "error": f"Workspace setup failed: {str(e)}",
        }


async def analyze(state: DeveloperState, agent=None) -> DeveloperState:
    """Analyze user story requirements.
    
    Refactored: Tools for exploration + with_structured_output for response.
    """
    print("[NODE] analyze - Analyzing story requirements...")
    try:
        workspace_path = state.get("workspace_path", "")
        project_id = state.get("project_id")
        task_id = state.get("task_id") or state.get("story_id")
        
        # Setup tool context
        _setup_tool_context(workspace_path, project_id, task_id)
        
        # Build input
        input_text = _format_input_template(
            "analyze_story",
            story_title=state.get("story_title", "Untitled"),
            story_content=state.get("story_content", ""),
            acceptance_criteria=chr(10).join(f"- {ac}" for ac in state.get("acceptance_criteria", []))
        )

        # Tools for exploration
        tools = [read_file_safe, list_directory_safe, semantic_code_search]
        
        # Step 1: Explore with tools
        messages = [
            SystemMessage(content=_build_system_prompt("analyze_story")),
            HumanMessage(content=input_text)
        ]
        
        exploration = await _llm_with_tools(
            llm=_code_llm,
            tools=tools,
            messages=messages,
            state=state,
            name="analyze_explore",
            max_iterations=2
        )
        
        # Step 2: Get structured response (exploration already filtered by semantic search)
        messages.append(HumanMessage(content=f"Context gathered:\n{exploration[:5000]}\n\nNow provide your final analysis."))
        structured_llm = _code_llm.with_structured_output(StoryAnalysis)
        analysis = await structured_llm.ainvoke(messages, config=_cfg(state, "analyze"))
        
        logger.info(f"[analyze] Done: {analysis.task_type}, {analysis.complexity}")
        
        return {
            **state,
            "analysis_result": analysis.model_dump(),
            "task_type": analysis.task_type,
            "complexity": analysis.complexity,
            "estimated_hours": analysis.estimated_hours,
            "affected_files": analysis.affected_files,
            "dependencies": analysis.dependencies,
            "risks": analysis.risks,
            "action": "PLAN",
        }
        
    except Exception as e:
        logger.error(f"[analyze] Error: {e}", exc_info=True)
        return {**state, "error": str(e), "action": "RESPOND"}


async def design(state: DeveloperState, agent=None) -> DeveloperState:
    """Generate system design.
    
    Refactored: Tools for exploration + with_structured_output for response.
    """
    print("[NODE] design - Creating system design...")
    try:
        analysis = state.get("analysis_result", {})
        complexity = state.get("complexity", "medium")
        workspace_path = state.get("workspace_path", "")
        project_id = state.get("project_id")
        task_id = state.get("task_id") or state.get("story_id")
        
        # Skip design for simple tasks
        if complexity == "low":
            logger.info("[design] Skipping design for low complexity task")
            return {**state, "action": "PLAN", "message": "Task Ä‘Æ¡n giáº£n, bá» qua design phase."}
        
        # Setup tool context
        _setup_tool_context(workspace_path, project_id, task_id)
        
        # Build input
        input_text = _format_input_template(
            "system_design",
            story_title=state.get("story_title", ""),
            analysis_summary=analysis.get("summary", ""),
            task_type=state.get("task_type", "feature"),
            complexity=complexity,
            story_content=state.get("story_content", ""),
            acceptance_criteria=chr(10).join(f"- {ac}" for ac in state.get("acceptance_criteria", [])),
            existing_context="Use tools to explore codebase"
        )

        # Tools for exploration
        tools = [read_file_safe, list_directory_safe, semantic_code_search]
        
        # Step 1: Explore with tools
        messages = [
            SystemMessage(content=_build_system_prompt("system_design")),
            HumanMessage(content=input_text)
        ]
        
        exploration = await _llm_with_tools(
            llm=_code_llm,
            tools=tools,
            messages=messages,
            state=state,
            name="design_explore",
            max_iterations=2
        )
        
        # Step 2: Get structured response (exploration already filtered by semantic search)
        messages.append(HumanMessage(content=f"Context gathered:\n{exploration[:5000]}\n\nNow provide your final system design."))
        structured_llm = _code_llm.with_structured_output(SystemDesign)
        design_result = await structured_llm.ainvoke(messages, config=_cfg(state, "design"))
        
        logger.info(f"[design] Got design: {len(design_result.file_structure)} files")
        
        # Build design document
        design_doc = f"""# System Design

## Data Structures & Interfaces
{design_result.data_structures or 'N/A'}

## API Interfaces
{design_result.api_interfaces or 'N/A'}

## Call Flow
{design_result.call_flow or 'N/A'}

## Design Notes
{design_result.design_notes or 'N/A'}

## File Structure
{chr(10).join(f'- {f}' for f in design_result.file_structure)}
"""
        
        # Save design doc to workspace
        story_id = state.get("story_id", "unknown")
        design_id = state.get("task_id") or story_id
        if workspace_path:
            design_dir = f"docs/story_{story_id}"
            design_file = f"{design_dir}/design-{design_id}.md"
            try:
                result = write_file_safe.invoke({
                    "file_path": design_file,
                    "content": design_doc,
                    "mode": "w"
                })
                logger.info(f"[design] Saved design doc: {design_file}")
            except Exception as e:
                logger.warning(f"[design] Failed to save design doc: {e}")
        
        return {
            **state,
            "system_design": design_result.model_dump(),
            "data_structures": design_result.data_structures,
            "api_interfaces": design_result.api_interfaces,
            "call_flow": design_result.call_flow,
            "design_doc": design_doc,
            "action": "PLAN",
        }
        
    except Exception as e:
        logger.error(f"[design] Error: {e}", exc_info=True)
        return {
            **state,
            "message": f"âš ï¸ Design skipped: {str(e)}",
            "action": "PLAN",
        }


async def plan(state: DeveloperState, agent=None) -> DeveloperState:
    """
    Create implementation plan.
    """
    print("[NODE] plan - Creating implementation plan...")
    try:
        analysis = state.get("analysis_result") or {}
        
        # Get workspace context
        workspace_path = state.get("workspace_path", "")
        
        # Get project context (AGENTS.md, etc.)
        project_context = state.get("project_context", "")
        project_config = state.get("project_config", {})
        
        # Build structure guidance from project_config if provided
        structure_guidance = ""
        if project_config:
            tech_stack = project_config.get("tech_stack", {})
            if tech_stack and isinstance(tech_stack, dict):
                tech_name = tech_stack.get("name", "")
                services = tech_stack.get("service", [])
                services_info = ", ".join(s.get("name", "") for s in services) if services else "N/A"
                structure_guidance = f"""
=== PROJECT CONFIG ===
Tech Stack: {tech_name}
Services: {services_info}

IMPORTANT: Follow AGENTS.md conventions for file paths!
"""
        
        # Build input from template (include full project_context with AGENTS.md)
        directory_structure = f"PROJECT CONTEXT (MUST FOLLOW):{chr(10)}{project_context}" if project_context else ""
        input_text = _format_input_template(
            "create_plan",
            story_title=state.get("story_title", "Untitled"),
            analysis_summary=analysis.get("summary", ""),
            task_type=state.get("task_type", "feature"),
            complexity=state.get("complexity", "medium"),
            structure_guidance=structure_guidance,
            affected_files=", ".join(state.get("affected_files", [])),
            design_doc=state.get("design_doc", "No design document"),
            acceptance_criteria=chr(10).join(f"- {ac}" for ac in state.get("acceptance_criteria", [])),
            directory_structure=directory_structure,
            existing_code="Use search_codebase and read_file tools to explore existing code"
        )

        # Use with_structured_output for reliable response
        messages = [
            SystemMessage(content=_build_system_prompt("create_plan")),
            HumanMessage(content=input_text)
        ]
        
        structured_llm = _code_llm.with_structured_output(ImplementationPlan)
        plan_result = await structured_llm.ainvoke(messages, config=_cfg(state, "plan"))
        
        # Warning if no steps
        if not plan_result.steps:
            logger.warning(f"[plan] No steps in plan! affected_files: {state.get('affected_files', [])}")
        else:
            logger.info(f"[plan] Created {len(plan_result.steps)} steps, estimated {plan_result.total_estimated_hours}h")
        
        steps_text = "\n".join(
            f"  {s.order}. [{s.action}] {s.description} ({s.estimated_minutes}m)"
            for s in plan_result.steps
        )
        
        msg = f"""ðŸ“‹ **Implementation Plan**

**Story:** {plan_result.story_summary}
**Total Time:** {plan_result.total_estimated_hours}h
**Steps:** {len(plan_result.steps)}

{steps_text}

ðŸ”„ **Rollback Plan:** {plan_result.rollback_plan or 'N/A'}"""
        
        if agent:
            pass
        
        return {
            **state,
            "implementation_plan": [s.model_dump() for s in plan_result.steps],
            "total_steps": len(plan_result.steps),
            "current_step": 0,
            "message": msg,
            "action": "IMPLEMENT",
        }
        
    except Exception as e:
        logger.error(f"[plan] Error: {e}", exc_info=True)
        return {
            **state,
            "error": str(e),
            "message": f"âŒ Lá»—i khi táº¡o plan: {str(e)}",
            "action": "RESPOND",
        }


async def implement(state: DeveloperState, agent=None) -> DeveloperState:
    """Execute implementation based on plan.
    
    Refactored: Tools for exploration + with_structured_output for response.
    """
    current_step = state.get("current_step", 0)
    total_steps = state.get("total_steps", 0)
    print(f"[NODE] implement - Step {current_step + 1}/{total_steps}...")
    try:
        plan_steps = state.get("implementation_plan", [])
        current_step = state.get("current_step", 0)
        workspace_path = state.get("workspace_path", "")
        project_id = state.get("project_id", "default")
        task_id = state.get("task_id") or state.get("story_id", "")
        
        # React mode: increment counter when looping back
        react_loop_count = state.get("react_loop_count", 0)
        debug_count = state.get("debug_count", 0)
        summarize_feedback = state.get("summarize_feedback")
        run_status = state.get("run_status")
        
        # FIX: Check run_status == "FAIL" FIRST (regardless of current_step)
        # When coming from run_code FAIL in React mode, current_step is NOT 0
        # We need to reset it to 0 to actually re-implement
        if state.get("react_mode") and run_status == "FAIL":
            current_step = 0  # Reset to start over
            react_loop_count += 1
            debug_count = 0
            logger.info(f"[implement] React loop {react_loop_count} (from run_code FAIL, restarting from step 0)")
        # Check for summarize feedback (when current_step is already 0)
        elif state.get("react_mode") and current_step == 0 and summarize_feedback:
            react_loop_count += 1
            debug_count = 0
            logger.info(f"[implement] React loop {react_loop_count} (from summarize: {summarize_feedback[:100]}...)")
        
        if not plan_steps:
            logger.error("[implement] No implementation plan")
            return {**state, "error": "No implementation plan", "action": "RESPOND"}
        
        if current_step >= len(plan_steps):
            if agent:
                pass
            return {
                **state,
                "message": "Implementation hoÃ n táº¥t",
                "action": "VALIDATE",
            }
        
        step = plan_steps[current_step]
        current_file = step.get("file_path") or ""
        
        if agent:
            pass
        
        # Gather related code context using MetaGPT pattern
        # Get code context using CocoIndex (required)
        related_context = state.get("related_code_context", "")
        if workspace_path and not related_context:
            index_ready = state.get("index_ready", False)
            project_id = state.get("project_id", "default")
            task_id = state.get("task_id") or state.get("story_id", "")
            step_description = step.get("description", "")
            
            # CocoIndex semantic search (required)
            if index_ready:
                related_context = get_related_code_indexed(
                    project_id=project_id,
                    current_file=current_file,
                    task_description=step_description,
                    top_k=8,
                    task_id=task_id
                )
                logger.info(f"[implement] Using CocoIndex for context")
        
        # Get existing code if modifying (using read_file_safe tool)
        existing_code = ""
        if workspace_path and current_file and step.get("action") == "modify":
            try:
                result = read_file_safe.invoke({"file_path": current_file})
                if result and not result.startswith("Error:"):
                    # Extract content after "Content of {file_path}:\n\n"
                    if "\n\n" in result:
                        existing_code = result.split("\n\n", 1)[1]
                    else:
                        existing_code = result
            except Exception:
                pass
        
        # Build implementation plan context
        implementation_plan = state.get("code_plan_doc") or ""
        if not implementation_plan:
            # Fallback to step-based plan
            implementation_plan = "\n".join(
                f"{s.get('order', i+1)}. [{s.get('action')}] {s.get('description')}"
                for i, s in enumerate(plan_steps)
            )
        
        # Build full context including project config, research results, and summarize feedback
        research_context = state.get("research_context", "")
        project_context = state.get("project_context", "")
        project_config = state.get("project_config", {})
        summarize_feedback = state.get("summarize_feedback", "")
        
        full_related_context = related_context or "No related files"
        
        # Add project config info if provided
        if project_config:
            tech_stack = project_config.get('tech_stack', {})
            tech_name = tech_stack.get('name', 'unknown') if isinstance(tech_stack, dict) else 'unknown'
            services = tech_stack.get('service', []) if isinstance(tech_stack, dict) else []
            services_info = ", ".join(s.get("name", "") for s in services) if services else "N/A"
            config_info = f"""## PROJECT CONFIG
Tech Stack: {tech_name}
Services: {services_info}
"""
            full_related_context = f"{config_info}\n---\n\n{full_related_context}"
        
        # Add boilerplate examples (ACCURACY improvement)
        if workspace_path:
            # Determine task type from file path
            task_type = "page"
            if "component" in current_file.lower():
                task_type = "component"
            elif "api" in current_file.lower() or "route" in current_file.lower():
                task_type = "api"
            elif "layout" in current_file.lower():
                task_type = "layout"
            
            boilerplate = get_boilerplate_examples(workspace_path, task_type)
            if boilerplate:
                full_related_context = f"{boilerplate}\n\n---\n\n{full_related_context}"
        
        # Add project guidelines (AGENTS.md - full content, critical for conventions)
        if project_context:
            full_related_context = f"## PROJECT GUIDELINES (MUST FOLLOW)\n{project_context}\n\n---\n\n{full_related_context}"
        
        # Add research results
        if research_context:
            full_related_context += f"\n\n## Best Practices (from web research)\n{research_context[:1500]}"
        
        # Add feedback from previous summarize iteration (if looping)
        if summarize_feedback:
            full_related_context += f"\n\n## FEEDBACK FROM PREVIOUS ATTEMPT (MUST ADDRESS)\n{summarize_feedback}"
        
        # Setup tool context
        _setup_tool_context(workspace_path, project_id, task_id)
        
        # Build input from template
        error_logs_text = f"Previous Errors:{chr(10)}{state.get('error_logs', '')}" if state.get('error_logs') else ""
        input_text = _format_input_template(
            "implement_step",
            step_number=current_step + 1,
            total_steps=len(plan_steps),
            step_description=step.get("description", ""),
            file_path=current_file,
            action=step.get("action", "modify"),
            story_summary=state.get("analysis_result", {}).get("summary", ""),
            related_context=full_related_context[:8000],  # CocoIndex already filters relevant code
            existing_code=existing_code if existing_code else "No existing code (new file)",  # Full file for modification
            error_logs=error_logs_text
        )

        # Tools for exploration (read-only, writing is manual)
        tools = [read_file_safe, list_directory_safe, semantic_code_search]
        
        # Step 1: Explore with tools
        messages = [
            SystemMessage(content=_build_system_prompt("implement_step")),
            HumanMessage(content=input_text)
        ]
        
        exploration = await _llm_with_tools(
            llm=_code_llm,
            tools=tools,
            messages=messages,
            state=state,
            name="implement_explore",
            max_iterations=2
        )
        
        # Step 2: Get structured response (exploration already filtered by semantic search)
        messages.append(HumanMessage(content=f"Context gathered:\n{exploration[:5000]}\n\nNow provide the code implementation."))
        structured_llm = _code_llm.with_structured_output(CodeChange)
        code_change = await structured_llm.ainvoke(messages, config=_cfg(state, "implement"))
        
        # Fill defaults if missing
        if not code_change.file_path:
            code_change = CodeChange(
                file_path=current_file,
                action=code_change.action or step.get("action", "create"),
                code_snippet=code_change.code_snippet,
                description=code_change.description or step.get("description", "")
            )
        
        # Normalize path separators for Windows compatibility
        if code_change.file_path:
            code_change.file_path = code_change.file_path.replace("/", os.sep)
        
        logger.info(f"[implement] Got code from structured output")
        
        logger.info(f"[implement] Step {current_step + 1}: {code_change.action} {code_change.file_path}")
        
        # IMPORTANT: Write the generated code to file using write_file_safe tool
        if workspace_path and code_change.code_snippet and code_change.file_path:
            try:
                result = write_file_safe.invoke({
                    "file_path": code_change.file_path,
                    "content": code_change.code_snippet,
                    "mode": "w"
                })
                logger.info(f"[implement] {result}")
                # Note: Removed incremental_update_index - index at setup_workspace is enough
            except Exception as write_err:
                logger.warning(f"[implement] Failed to write {code_change.file_path}: {write_err}")
        
        code_changes = state.get("code_changes", [])
        code_changes.append(code_change.model_dump())
        
        files_created = state.get("files_created", [])
        files_modified = state.get("files_modified", [])
        
        if code_change.action == "create":
            files_created.append(code_change.file_path)
        elif code_change.action == "modify":
            files_modified.append(code_change.file_path)
        
        msg = f"âœ… Step {current_step + 1}: {code_change.description}"
        if agent:
            pass
        
        return {
            **state,
            "code_changes": code_changes,
            "files_created": files_created,
            "files_modified": files_modified,
            "current_step": current_step + 1,
            "react_loop_count": react_loop_count,
            "debug_count": debug_count,
            "run_status": None,  # Clear run_status to avoid re-triggering React loop
            "message": msg,
            "action": "IMPLEMENT" if current_step + 1 < len(plan_steps) else "VALIDATE",
        }
        
    except Exception as e:
        logger.error(f"[implement] Error: {e}", exc_info=True)
        return {
            **state,
            "error": str(e),
            "message": f"âŒ Lá»—i khi implement: {str(e)}",
            "action": "RESPOND",
        }


# =============================================================================
# MetaGPT-inspired nodes
# =============================================================================

async def create_code_plan(state: DeveloperState, agent=None) -> DeveloperState:
    """Create strategic code plan before implementation (MetaGPT-style).
    
    This node creates a detailed development plan with git diff format
    showing exactly what changes will be made to each file.
    """
    try:
        # Get code context via CocoIndex
        workspace_path = state.get("workspace_path", "")
        index_ready = state.get("index_ready", False)
        project_id = state.get("project_id", "default")
        task_id = state.get("task_id") or state.get("story_id", "")
        
        existing_code = ""
        if workspace_path and index_ready:
            existing_code = search_codebase(project_id, state.get("story_title", ""), top_k=10, task_id=task_id)
        
        sys_prompt = _build_system_prompt("create_code_plan", agent)
        user_prompt = _get_prompt("create_code_plan", "user_prompt").format(
            story_title=state.get("story_title", "Untitled"),
            story_content=state.get("story_content", ""),
            acceptance_criteria="\n".join(f"- {ac}" for ac in state.get("acceptance_criteria", [])),
            design_doc=state.get("design_doc") or state.get("analysis_result", {}).get("summary", ""),
            task_list="\n".join(f"- {f}" for f in state.get("affected_files", [])),
            legacy_code=existing_code or "No existing code",
        )
        
        messages = [
            SystemMessage(content=sys_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = await _code_llm.ainvoke(messages, config=_cfg(state, "create_code_plan"))
        clean_json = _clean_json(response.content)
        
        import json
        plan_data = json.loads(clean_json)
        
        logger.info(f"[create_code_plan] Created plan with {len(plan_data.get('development_plan', []))} steps")
        
        # Format plan for display
        dev_steps = plan_data.get("development_plan", [])
        steps_text = "\n".join(f"  {i+1}. {step}" for i, step in enumerate(dev_steps))
        
        files_to_create = plan_data.get("files_to_create", [])
        files_to_modify = plan_data.get("files_to_modify", [])
        
        msg = f"""ðŸ“‹ **Code Plan & Change Document**

**Development Plan:**
{steps_text}

**Files to Create:** {', '.join(files_to_create) if files_to_create else 'None'}
**Files to Modify:** {', '.join(files_to_modify) if files_to_modify else 'None'}
**Critical Path:** {' â†’ '.join(plan_data.get('critical_path', []))}"""
        
        if agent:
            pass
        
        return {
            **state,
            "code_plan_doc": clean_json,
            "development_plan": dev_steps,
            "incremental_changes": plan_data.get("incremental_changes", []),
            "message": msg,
            "action": "PLAN",
        }
        
    except Exception as e:
        logger.error(f"[create_code_plan] Error: {e}", exc_info=True)
        return {
            **state,
            "error": str(e),
            "message": f"âŒ Lá»—i khi táº¡o code plan: {str(e)}",
            "action": "PLAN",  # Continue to regular plan
        }


async def summarize_code(state: DeveloperState, agent=None) -> DeveloperState:
    """Validate implementation completeness (MetaGPT IS_PASS check).
    
    This node reviews all implemented code and determines if it passes
    quality checks. If not, it returns to IMPLEMENT for revisions.
    """
    try:
        # Get implemented code via CocoIndex
        workspace_path = state.get("workspace_path", "")
        index_ready = state.get("index_ready", False)
        project_id = state.get("project_id", "default")
        task_id = state.get("task_id") or state.get("story_id", "")
        
        code_blocks = ""
        if workspace_path and index_ready:
            code_blocks = search_codebase(project_id, "implementation code", top_k=15, task_id=task_id)
        
        sys_prompt = _build_system_prompt("summarize_code", agent)
        user_prompt = _get_prompt("summarize_code", "user_prompt").format(
            design_doc=state.get("design_doc") or state.get("analysis_result", {}).get("summary", ""),
            task_doc="\n".join(f"- {ac}" for ac in state.get("acceptance_criteria", [])),
            code_blocks=code_blocks or "No code to review",
            test_results=state.get("validation_result", {}).get("tests_passed", "Not run"),
            lint_results=state.get("validation_result", {}).get("lint_passed", "Not run"),
        )
        
        messages = [
            SystemMessage(content=sys_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = await _code_llm.ainvoke(messages, config=_cfg(state, "summarize_code"))
        clean_json = _clean_json(response.content)
        
        import json
        summary_data = json.loads(clean_json)
        
        is_pass = summary_data.get("is_pass", True)
        revision_count = state.get("revision_count", 0)
        max_revisions = state.get("max_revisions", 3)
        
        logger.info(f"[summarize_code] IS_PASS={is_pass}, revision={revision_count}/{max_revisions}")
        
        if is_pass:
            msg = f"""âœ… **Code Review: PASSED**

**Summary:**
{chr(10).join(f'  - {f}: {s}' for f, s in summary_data.get('summary', {}).items())}

**Call Flow:** {summary_data.get('call_flow', 'N/A')}
**Reason:** {summary_data.get('reason', 'All checks passed')}"""
            
            if agent:
                pass
            
            return {
                **state,
                "code_summary": summary_data,
                "is_pass": True,
                "message": msg,
                "action": "RESPOND",
            }
        else:
            # Check if we've exceeded max revisions
            if revision_count >= max_revisions:
                msg = f"""âš ï¸ **Code Review: Max revisions reached ({max_revisions})**

**Issues:**
{chr(10).join(f'  - {f}: {", ".join(issues)}' for f, issues in summary_data.get('code_review', {}).items())}

**Reason:** {summary_data.get('reason', 'Max revisions exceeded')}

Proceeding with current implementation."""
                
                if agent:
                    pass
                
                return {
                    **state,
                    "code_summary": summary_data,
                    "is_pass": False,
                    "message": msg,
                    "action": "RESPOND",
                }
            
            # Need revision
            todos = summary_data.get("todos", {})
            msg = f"""ðŸ”„ **Code Review: NEEDS REVISION** (Attempt {revision_count + 1}/{max_revisions})

**Issues Found:**
{chr(10).join(f'  - {f}: {", ".join(issues)}' for f, issues in summary_data.get('code_review', {}).items())}

**TODOs:**
{chr(10).join(f'  - {f}: {todo}' for f, todo in todos.items())}

**Reason:** {summary_data.get('reason', 'Issues need to be addressed')}

Returning to implementation for fixes..."""
            
            if agent:
                pass
            
            # Store error logs for next implementation round
            error_logs = f"Previous review issues:\n{summary_data.get('reason', '')}\n"
            error_logs += "\n".join(f"{f}: {todo}" for f, todo in todos.items())
            
            return {
                **state,
                "code_summary": summary_data,
                "is_pass": False,
                "needs_revision": True,
                "revision_count": revision_count + 1,
                "error_logs": error_logs,
                "current_step": 0,  # Reset to re-implement
                "message": msg,
                "action": "IMPLEMENT",
            }
        
    except Exception as e:
        logger.error(f"[summarize_code] Error: {e}", exc_info=True)
        return {
            **state,
            "error": str(e),
            "message": f"âŒ Lá»—i khi review code: {str(e)}",
            "action": "RESPOND",
        }


async def clarify(state: DeveloperState, agent=None) -> DeveloperState:
    """Ask for clarification when story is unclear."""
    try:
        sys_prompt = _build_system_prompt("clarify", agent)
        user_prompt = _get_prompt("clarify", "user_prompt").format(
            story_title=state.get("story_title", "Untitled"),
            story_content=state.get("story_content", ""),
            acceptance_criteria="\n".join(state.get("acceptance_criteria", [])),
            unclear_points=state.get("reason", "Story khÃ´ng rÃµ rÃ ng"),
        )
        
        messages = [
            SystemMessage(content=sys_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = await _fast_llm.ainvoke(messages, config=_cfg(state, "clarify"))
        question = response.content
        
        if agent:
            pass
        
        return {
            **state,
            "message": question,
            "action": "CLARIFY",
        }
        
    except Exception as e:
        logger.error(f"[clarify] Error: {e}", exc_info=True)
        default_msg = "ðŸ¤” MÃ¬nh cáº§n thÃªm thÃ´ng tin vá» story nÃ y. Báº¡n cÃ³ thá»ƒ mÃ´ táº£ chi tiáº¿t hÆ¡n khÃ´ng?"
        if agent:
            pass
        return {
            **state,
            "message": default_msg,
            "action": "CLARIFY",
        }


async def respond(state: DeveloperState, agent=None) -> DeveloperState:
    """Generate and send conversational response to user using LLM."""
    try:
        # If there's already a detailed message (from validate, analyze, etc.), use it
        existing_msg = state.get("message", "")
        if existing_msg and len(existing_msg) > 100:
            if agent:
                pass
            return {**state, "action": "RESPOND"}
        
        # Generate conversational response using LLM
        sys_prompt = _build_system_prompt("respond", agent)
        user_prompt = _get_prompt("respond", "user_prompt").format(
            story_title=state.get("story_title", ""),
            story_content=state.get("story_content", ""),
            router_reason=state.get("reason", "general response"),
        )
        
        messages = [
            SystemMessage(content=sys_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = await _fast_llm.ainvoke(messages, config=_cfg(state, "respond"))
        msg = response.content
        
        logger.info(f"[respond] Generated response: {msg[:100]}...")
        
        if agent:
            pass
        
        return {**state, "message": msg, "action": "RESPOND"}
        
    except Exception as e:
        logger.error(f"[respond] Error: {e}", exc_info=True)
        fallback_msg = state.get("message") or "MÃ¬nh Ä‘Ã£ nháº­n Ä‘Æ°á»£c tin nháº¯n cá»§a báº¡n! ðŸ‘‹"
        if agent:
            pass
        return {**state, "message": fallback_msg, "action": "RESPOND"}


async def merge_to_main(state: DeveloperState, agent=None) -> DeveloperState:
    """Merge feature branch to main after successful validation.
    
    This node is called after validate passes (is_pass=True).
    It commits all changes and merges the story branch into main branch.
    (Following Developer V1 pattern: auto-commit after implementation)
    
    If workspace is not a git repo (worktree creation failed), skip merge
    and just keep the generated files.
    """
    try:
        branch_name = state.get("branch_name")
        main_workspace = state.get("main_workspace")
        workspace_path = state.get("workspace_path")
        story_title = state.get("story_title", "Implementation")
        workspace_ready = state.get("workspace_ready", False)
        
        # If workspace wasn't properly set up as git worktree, skip merge
        if not workspace_ready:
            logger.info("[merge_to_main] Workspace not a git worktree, skipping merge (files already written)")
            return {**state, "merged": False, "error": "Workspace not a git worktree"}
        
        if not branch_name or not main_workspace:
            logger.warning("[merge_to_main] Missing branch_name or main_workspace")
            return {**state, "merged": False}
        
        from app.agents.developer.tools.git_python_tool import GitPythonTool
        
        # Check if workspace is actually a git repo
        if workspace_path and Path(workspace_path).exists():
            git_dir = Path(workspace_path) / ".git"
            if not git_dir.exists():
                logger.info("[merge_to_main] Workspace is not a git repo, skipping merge")
                return {**state, "merged": False, "error": "Not a git repository"}
            
            workspace_git = GitPythonTool(root_dir=workspace_path)
            
            # Stage all changes
            status_result = workspace_git._run("status")
            if "not a git repository" in status_result.lower():
                logger.info("[merge_to_main] Workspace is not a git repo, skipping merge")
                return {**state, "merged": False, "error": "Not a git repository"}
            
            if "nothing to commit" not in status_result:
                commit_msg = f"feat: {story_title[:50]}... [auto-commit by Developer V2]"
                commit_result = workspace_git._run("commit", message=commit_msg, files=["."])
                logger.info(f"[merge_to_main] Auto-commit: {commit_result}")
        
        # Check if main_workspace is a git repo
        main_git_dir = Path(main_workspace) / ".git"
        if not main_git_dir.exists():
            logger.info("[merge_to_main] Main workspace is not a git repo, skipping merge")
            return {**state, "merged": False, "error": "Main workspace not a git repository"}
        
        main_git = GitPythonTool(root_dir=main_workspace)
        
        # 1. Checkout main branch
        checkout_result = main_git._run("checkout_branch", branch_name="main")
        logger.info(f"[merge_to_main] Checkout main: {checkout_result}")
        
        # If main doesn't exist, try master
        if "does not exist" in checkout_result or "error" in checkout_result.lower():
            checkout_result = main_git._run("checkout_branch", branch_name="master")
            logger.info(f"[merge_to_main] Checkout master: {checkout_result}")
        
        # 2. Merge feature branch
        merge_result = main_git._run("merge", branch_name=branch_name)
        logger.info(f"[merge_to_main] Merge result: {merge_result}")
        
        if "conflict" in merge_result.lower() or "error" in merge_result.lower():
            return {
                **state,
                "merged": False,
                "error": merge_result,
            }
        
        return {
            **state,
            "merged": True,
        }
        
    except Exception as e:
        logger.error(f"[merge_to_main] Error: {e}", exc_info=True)
        return {
            **state,
            "merged": False,
            "error": str(e),
        }


async def cleanup_workspace(state: DeveloperState, agent=None) -> DeveloperState:
    """Cleanup workspace resources after task completion.
    
    NOTE: Everything is PRESERVED for potential future use:
    - Git worktree and branch (can resume work)
    - CocoIndex task index (can search without re-indexing)
    - Dev containers: kept running if dev server started, otherwise stopped
    """
    try:
        branch_name = state.get("branch_name")
        app_url = state.get("app_url")
        
        # Keep container running if dev server is active (user testing)
        if app_url:
            logger.info(f"[cleanup_workspace] Dev server running at {app_url} - container kept alive")
            return {
                **state,
                "workspace_ready": False,
                "index_ready": False,
                "container_stopped": False,
                "message": f"âœ… Implementation complete! Dev server running at {app_url}",
            }
        
        # Stop dev container (not remove - preserve for resume)
        if branch_name:
            try:
                dev_container_manager.stop(branch_name)
                logger.info(f"[cleanup_workspace] Stopped container for: {branch_name}")
            except Exception as container_err:
                logger.debug(f"[cleanup_workspace] Container stop: {container_err}")
        
        logger.info(f"[cleanup_workspace] Workspace preserved (branch: {branch_name})")
        
        return {
            **state,
            "workspace_ready": False,
            "index_ready": False,
            "container_stopped": True,
        }
        
    except Exception as e:
        logger.error(f"[cleanup_workspace] Error: {e}", exc_info=True)
        return state


# =============================================================================
# SUMMARIZE CODE (MetaGPT SummarizeCode + IS_PASS pattern)
# =============================================================================

IS_PASS_PROMPT = """
## Code Summary
{summary}

## Original Requirements
{requirements}

## Acceptance Criteria
{acceptance_criteria}

## Files Implemented
{files_list}

---
Analyze if this implementation meets ALL requirements and acceptance criteria.

Consider:
1. Are all acceptance criteria addressed?
2. Is the code complete (no TODOs, no placeholders)?
3. Are all required files created?
4. Is the implementation functionally correct?

Respond with JSON:
- If complete: {{"is_pass": true, "reason": "All requirements met"}}
- If incomplete: {{"is_pass": false, "reason": "Specific issues: ..."}}
"""


async def _summarize_all_code(code_changes: list, workspace_path: str = "", state: dict = None) -> str:
    """Summarize all code changes for IS_PASS validation.
    
    Uses code_snippet directly from code_changes (already contains full implementation).
    No truncation needed - code_changes comes from implement node with complete code.
    """
    if not code_changes:
        return "No code changes to summarize."
    
    summary_parts = []
    
    for change in code_changes:
        file_path = change.get("file_path", "unknown")
        action = change.get("action", "unknown")
        description = change.get("description", "") or ""
        code = change.get("code_snippet", "") or ""
        
        # Get language for markdown code block
        lang = get_markdown_code_block_type(file_path)
        
        summary_parts.append(f"""### {file_path} ({action})
{description}

```{lang}
{code}
```
""")
    
    return "\n".join(summary_parts)


async def _check_is_pass(summary: str, state: dict) -> tuple:
    """Check if implementation is complete using LLM (MetaGPT IS_PASS pattern).
    
    Returns:
        (is_pass: bool, reason: str)
    """
    requirements = state.get("story_content", "")
    acceptance_criteria = "\n".join(f"- {ac}" for ac in state.get("acceptance_criteria", []))
    files_created = state.get("files_created", [])
    files_modified = state.get("files_modified", [])
    files_list = "\n".join(f"- {f}" for f in files_created + files_modified)
    
    prompt = IS_PASS_PROMPT.format(
        summary=summary,
        requirements=requirements,
        acceptance_criteria=acceptance_criteria or "No specific criteria",
        files_list=files_list or "No files"
    )
    
    messages = [
        SystemMessage(content="You are a code reviewer checking if implementation is complete."),
        HumanMessage(content=prompt)
    ]
    
    try:
        response = await _fast_llm.ainvoke(messages)
        clean_json = _clean_json(response.content)
        
        import json
        result = json.loads(clean_json)
        is_pass = result.get("is_pass", False)
        reason = result.get("reason", "Unknown")
        
        return is_pass, reason
    except Exception as e:
        logger.warning(f"[_check_is_pass] Error: {e}, defaulting to PASS")
        return True, "Check failed, proceeding"


async def summarize_code(state: DeveloperState, agent=None) -> DeveloperState:
    """Summarize code and check IS_PASS (MetaGPT SummarizeCode pattern).
    
    This node:
    1. Summarizes all implemented code
    2. Checks if implementation meets requirements (IS_PASS)
    3. If not pass: loops back to implement with feedback
    4. If pass: proceeds to code_review
    """
    try:
        code_changes = state.get("code_changes", [])
        workspace_path = state.get("workspace_path", "")
        summarize_count = state.get("summarize_count", 0)
        max_summarize = state.get("max_summarize", 3)
        
        if not code_changes:
            logger.info("[summarize_code] No code changes, passing through")
            return {**state, "is_pass": True, "action": "CODE_REVIEW"}
        
        if agent:
            # await agent.message_user("status", f"ðŸ“ Summarizing code (iteration {summarize_count + 1}/{max_summarize})...")
            pass
        
        # 1. Summarize all code using CocoIndex
        summary = await _summarize_all_code(code_changes, workspace_path, state)
        logger.info(f"[summarize_code] Generated summary: {len(summary)} chars")
        
        # 2. Check IS_PASS
        is_pass, reason = await _check_is_pass(summary, state)
        logger.info(f"[summarize_code] IS_PASS: {is_pass}, reason: {reason[:100]}...")
        
        new_summarize_count = summarize_count + 1
        
        if is_pass:
            logger.info("[summarize_code] PASS - proceeding to code_review")
            return {
                **state,
                "is_pass": True,
                "code_summary": {"summary": summary, "is_pass": True, "reason": reason},
                "summarize_count": new_summarize_count,
                "summarize_feedback": "",
                "action": "CODE_REVIEW",
            }
        
        # Not pass - check if we should retry
        if new_summarize_count >= max_summarize:
            logger.warning(f"[summarize_code] Max iterations ({max_summarize}) reached, proceeding anyway")
            return {
                **state,
                "is_pass": False,
                "code_summary": {"summary": summary, "is_pass": False, "reason": reason},
                "summarize_count": new_summarize_count,
                "summarize_feedback": reason,
                "action": "CODE_REVIEW",  # Proceed to review even if not pass
            }
        
        # Loop back to implement with feedback
        logger.info(f"[summarize_code] NOT PASS - looping back to implement with feedback")
        return {
            **state,
            "is_pass": False,
            "code_summary": {"summary": summary, "is_pass": False, "reason": reason},
            "summarize_count": new_summarize_count,
            "summarize_feedback": reason,
            "current_step": 0,  # Reset to re-implement
            "action": "IMPLEMENT",
        }
        
    except Exception as e:
        logger.error(f"[summarize_code] Error: {e}", exc_info=True)
        return {**state, "is_pass": True, "action": "CODE_REVIEW"}  # Pass on error


# =============================================================================
# LINT AND FORMAT (Auto-fix style issues before code review) - NO LLM
# =============================================================================

async def lint_and_format(state: DeveloperState, agent=None) -> DeveloperState:
    """Auto-fix lint/format issues before code review.
    
    Uses project_config to determine lint/format commands:
    - Node/Bun: eslint --fix + prettier --write
    - Python: ruff check --fix + ruff format
    
    This reduces trivial style issues in code review, saving tokens and time.
    """
    workspace_path = state.get("workspace_path")
    if not workspace_path:
        logger.warning("[lint_and_format] No workspace_path, skipping")
        return state
    
    project_config = state.get("project_config", {})
    tech_stack = project_config.get("tech_stack", {})
    services = tech_stack.get("service", []) if isinstance(tech_stack, dict) else []
    
    if not services:
        logger.info("[lint_and_format] No tech_stack.service config, skipping")
        return state
    
    fixed_count = 0
    lint_output = ""
    
    for svc in services:
        svc_name = svc.get("name", "app")
        svc_path = str(Path(workspace_path) / svc.get("path", "."))
        runtime = svc.get("runtime", "node")
        
        # Run install first (before lint/format)
        install_cmd = svc.get("install_cmd")
        if install_cmd:
            logger.info(f"[lint_and_format] Installing deps for {svc_name}: {install_cmd}")
            try:
                execute_shell.invoke({
                    "command": install_cmd,
                    "working_directory": svc_path,
                    "timeout": 180
                })
            except Exception as e:
                logger.warning(f"[lint_and_format] Install error: {e}")
        
        # Run db_cmds (prisma generate) if needed
        db_cmds = svc.get("db_cmds", [])
        for db_cmd in db_cmds:
            if "prisma generate" in db_cmd:
                logger.info(f"[lint_and_format] Running: {db_cmd}")
                try:
                    execute_shell.invoke({
                        "command": db_cmd,
                        "working_directory": svc_path,
                        "timeout": 60
                    })
                except Exception as e:
                    logger.warning(f"[lint_and_format] DB cmd error: {e}")
        
        # Use custom commands if provided, otherwise use defaults
        lint_fix_cmd = svc.get("lint_fix_cmd")
        format_cmd = svc.get("format_cmd")
        
        if not lint_fix_cmd and not format_cmd:
            # Default commands based on runtime (no Unix redirects for Windows compatibility)
            if runtime in ["bun"]:
                lint_fix_cmd = "bunx eslint --fix . --ext .ts,.tsx,.js,.jsx"
                format_cmd = "bunx prettier --write ."
            elif runtime in ["pnpm", "node"]:
                lint_fix_cmd = "pnpm exec eslint --fix . --ext .ts,.tsx,.js,.jsx"
                format_cmd = "pnpm exec prettier --write ."
            elif runtime == "python":
                lint_fix_cmd = "ruff check --fix ."
                format_cmd = "ruff format ."
            else:
                continue
        
        logger.info(f"[lint_and_format] Running lint/format for {svc_name} ({runtime})")
        
        # Execute lint fix
        if lint_fix_cmd:
            try:
                result = execute_shell.invoke({"command": lint_fix_cmd, "working_directory": workspace_path, "timeout": 60})
                if isinstance(result, dict):
                    output = result.get("stdout", "") + result.get("stderr", "")
                    lint_output += f"\n=== {svc_name} lint ===\n{output}"
                    # Count fixed files from output (rough estimate)
                    if "fixed" in output.lower() or "formatted" in output.lower():
                        fixed_count += 1
            except Exception as e:
                logger.warning(f"[lint_and_format] Lint fix error: {e}")
        
        # Execute format
        if format_cmd:
            try:
                result = execute_shell.invoke({"command": format_cmd, "working_directory": workspace_path, "timeout": 60})
                if isinstance(result, dict):
                    output = result.get("stdout", "") + result.get("stderr", "")
                    lint_output += f"\n=== {svc_name} format ===\n{output}"
            except Exception as e:
                logger.warning(f"[lint_and_format] Format error: {e}")
    
    logger.info(f"[lint_and_format] Completed, ~{fixed_count} fixes applied")
    
    return {
        **state,
        "lint_output": lint_output,
    }


# =============================================================================
# CODE REVIEW (BATCH - Review ALL files in ONE LLM call)
# =============================================================================

async def code_review(state: DeveloperState, agent=None) -> DeveloperState:
    """Batch review ALL files in ONE LLM call.
    
    Speed optimization: Instead of N separate LLM calls for N files,
    review all files together in a single call.
    
    Expected improvement: 24 calls -> 1 call (~180s -> ~20s)
    """
    print("[NODE] code_review - Reviewing code quality...")
    try:
        code_changes = state.get("code_changes", [])
        k = state.get("code_review_k", 2)
        workspace_path = state.get("workspace_path", "")
        iteration = state.get("code_review_iteration", 0)
        
        if not code_changes:
            logger.info("[code_review] No code changes to review")
            return {**state, "code_review_passed": True}
        
        # Build ALL files into one prompt (batch review)
        all_code_blocks = []
        file_map = {}  # Map file_path to code_change for updates
        
        for change in code_changes:
            file_path = change.get("file_path") or ""
            code = change.get("code_snippet", "")
            
            if not code:
                continue
            
            file_map[file_path] = change
            lang = get_markdown_code_block_type(file_path)
            all_code_blocks.append(f"### {file_path}\n```{lang}\n{code}\n```")
        
        if not all_code_blocks:
            logger.info("[code_review] No code to review")
            return {**state, "code_review_passed": True}
        
        combined_code = "\n\n".join(all_code_blocks)
        logger.info(f"[code_review] Batch reviewing {len(all_code_blocks)} files in ONE call")
        
        # Build batch review prompt
        requirements = state.get("story_content", "") or state.get("task_doc", "")
        acceptance_criteria = "\n".join(f"- {ac}" for ac in state.get("acceptance_criteria", []))
        
        sys_prompt = _build_system_prompt("batch_code_review", agent)
        input_template = _get_prompt("batch_code_review", "input_template")
        user_prompt = input_template.format(
            requirements=f"{requirements}\n\nAcceptance Criteria:\n{acceptance_criteria}",
            all_code_blocks=combined_code
        )
        
        messages = [
            SystemMessage(content=sys_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        # Single LLM call for ALL files
        response = await _code_llm.ainvoke(messages, config=_cfg(state, "batch_code_review"))
        clean_json = _clean_json(response.content)
        
        try:
            import json
            batch_result = json.loads(clean_json)
        except json.JSONDecodeError:
            logger.warning("[code_review] Failed to parse batch result, assuming LGTM")
            batch_result = {"files": {}, "summary": "Parse error"}
        
        # Process batch results - calculate all_passed from actual file results (ignore overall_result)
        files_review = batch_result.get("files", {})
        
        # all_passed = True only if ALL files are LGTM
        all_passed = all(
            "LGTM" in r.get("result", "LGTM")
            for r in files_review.values()
        ) if files_review else True
        
        review_results = []
        error_logs_parts = ["## CODE REVIEW FEEDBACK (MUST FIX):"]
        
        for file_path, review in files_review.items():
            result = review.get("result", "LGTM")
            issues = review.get("issues", [])
            rewritten = review.get("rewritten_code", "")
            
            review_results.append({
                "filename": file_path,
                "result": result,
                "issues": issues,
            })
            
            if "LBTM" in result:
                all_passed = False
                
                # Collect feedback for implement
                if issues:
                    error_logs_parts.append(f"\n### {file_path}:")
                    for issue in issues:
                        error_logs_parts.append(f"  - {issue}")
                
                # If rewritten code provided, update the file using write_file_safe tool
                if rewritten and rewritten.strip() and file_path in file_map:
                    file_map[file_path]["code_snippet"] = rewritten
                    
                    if workspace_path:
                        try:
                            result = write_file_safe.invoke({
                                "file_path": file_path,
                                "content": rewritten,
                                "mode": "w"
                            })
                            logger.info(f"[code_review] {result}")
                        except Exception as e:
                            logger.warning(f"[code_review] Failed to write {file_path}: {e}")
        
        new_iteration = iteration + 1
        logger.info(f"[code_review] Batch result: {'PASSED' if all_passed else 'FAILED'} ({len(files_review)} files)")
        
        # If not all passed and can retry, go back to implement
        if not all_passed and new_iteration < k:
            logger.info(f"[code_review] Iteration {new_iteration}, routing to implement for fixes...")
            
            return {
                **state,
                "code_review_passed": False,
                "code_review_results": review_results,
                "code_review_iteration": new_iteration,
                "error_logs": "\n".join(error_logs_parts),
                "current_step": 0,  # Reset to re-implement from start
            }
        
        return {
            **state,
            "code_review_passed": all_passed,
            "code_review_results": review_results,
            "code_review_iteration": new_iteration,
        }
        
    except Exception as e:
        logger.error(f"[code_review] Error: {e}", exc_info=True)
        return {**state, "code_review_passed": True}  # Pass on error to continue flow


# =============================================================================
# RUN CODE (Execute tests to verify) - Rule-based, NO LLM
# =============================================================================

def _get_langfuse_span(state: DeveloperState, name: str, input_data: dict = None):
    """Get Langfuse span if handler is available."""
    handler = state.get("langfuse_handler")
    if not handler:
        return None
    try:
        from langfuse import get_client
        langfuse = get_client()
        return langfuse.span(name=name, input=input_data or {})
    except Exception:
        return None


async def _run_code_multi_service(state: DeveloperState, workspace_path: str, services: list) -> DeveloperState:
    """Run tests for project services (single or multi-service).
    
    Args:
        state: Current state
        workspace_path: Root workspace path
        services: List of service configs [{"name": "app", "path": ".", ...}, ...]
    
    Returns:
        Updated state with combined test results
    """
    task_id = state.get("task_id") or state.get("story_id", "")
    branch_name = state.get("branch_name") or task_id
    
    # Langfuse tracing
    parent_span = _get_langfuse_span(state, "run_code_multi_service", {
        "services": [s.get("name") for s in services],
        "workspace": workspace_path,
    })
    
    all_stdout = ""
    all_stderr = ""
    all_passed = True
    summaries = []
    
    try:
        for svc_config in services:
            svc_name = svc_config.get("name", "app")
            svc_path = str(Path(workspace_path) / svc_config.get("path", "."))
            test_cmd = svc_config.get("test_cmd", "")
            install_cmd = svc_config.get("install_cmd", "")
            needs_db = svc_config.get("needs_db", False)
            db_cmds = svc_config.get("db_cmds", [])
            
            if not test_cmd:
                logger.info(f"[run_code] Skipping {svc_name}: no test_cmd")
                continue
            
            # Service-level span
            svc_span = parent_span.span(name=f"service:{svc_name}", input={
                "path": svc_config.get("path", "."),
                "needs_db": needs_db,
            }) if parent_span else None
            
            logger.info(f"[run_code] Running tests for {svc_name} at {svc_path}")
            all_stdout += f"\n\n{'='*40}\n SERVICE: {svc_name}\n{'='*40}\n"
            
            # Build commands for this service (no cd, use working_directory instead)
            commands = []
            if install_cmd:
                commands.append(install_cmd)
            for db_cmd in db_cmds:
                commands.append(db_cmd)
            commands.append(test_cmd)
            
            svc_passed = True
            
            # Start DB container if needed (just for database connection)
            if needs_db:
                try:
                    set_container_context(branch_name=branch_name, workspace_path=workspace_path)
                    dev_container_manager.get_or_create(
                        branch_name=branch_name,
                        workspace_path=workspace_path,
                        project_type="node",
                    )
                    logger.info(f"[run_code] DB container started for {svc_name}")
                except Exception as e:
                    logger.warning(f"[run_code] DB container error (continuing anyway): {e}")
            
            # Always run commands directly on host
            # Use working_directory parameter for Windows compatibility
            for base_cmd in commands:
                cmd_span = svc_span.span(name=f"exec:{base_cmd[:40]}...") if svc_span else None
                
                try:
                    timeout = 300 if "install" in base_cmd else 120
                    result = execute_shell.invoke({"command": base_cmd, "working_directory": workspace_path, "timeout": timeout})
                    
                    if isinstance(result, str):
                        import json
                        result = json.loads(result)
                    
                    if isinstance(result, dict):
                        all_stdout += f"\n$ {base_cmd}\n{result.get('stdout', '')}"
                        exit_code = result.get("exit_code", 0)
                        
                        if cmd_span:
                            cmd_span.end(output={"exit_code": exit_code})
                        
                        if exit_code != 0 and "test" in base_cmd.lower():
                            test_output = result.get("stdout", "") + result.get("stderr", "")
                            all_stderr += test_output
                            all_passed = False
                            svc_passed = False
                            summaries.append(f"{svc_name}: FAIL")
                            logger.error(f"[run_code] TEST FAILED for {svc_name}:\n{test_output[:2000]}")
                            task_id = state.get("task_id") or state.get("story_id", "unknown")
                            _write_test_log(task_id, test_output, "FAIL")
                            break
                except Exception as e:
                    if cmd_span:
                        cmd_span.end(output={"error": str(e)})
                    all_stderr += f"\nError: {e}"
                    all_passed = False
                    svc_passed = False
                    summaries.append(f"{svc_name}: ERROR")
                    break
            else:
                summaries.append(f"{svc_name}: PASS")
            
            # End service span
            if svc_span:
                svc_span.end(output={"status": "PASS" if svc_passed else "FAIL"})
        
        run_status = "PASS" if all_passed else "FAIL"
        summary = ", ".join(summaries)
        
        logger.info(f"[run_code] Multi-service result: {run_status} ({summary})")
        
        # Start dev server if tests pass
        app_url = None
        if run_status == "PASS":
           print('completed tests successfully')
        
        # End parent span
        if parent_span:
            parent_span.end(output={"status": run_status, "summary": summary, "services": summaries})
        
        return {
            **state,
            "run_status": run_status,
            "run_stdout": all_stdout,
            "run_stderr": all_stderr,
            "run_result": {
                "status": run_status,
                "summary": f"Multi-service: {summary}",
                "services": summaries,
            },
            "app_url": app_url,
            "message": f"âœ… Tests passed! Dev server running at {app_url}" if app_url else None,
        }
    except Exception as e:
        if parent_span:
            parent_span.end(output={"error": str(e)})
        raise


async def run_code(state: DeveloperState, agent=None) -> DeveloperState:
    """Execute tests using project_config (required).
    
    Requires project_config with tech_stack and commands.
    """
    print("[NODE] run_code - Running tests...")
    
    workspace_path = state.get("workspace_path", "")
    project_id = state.get("project_id", "default")
    task_id = state.get("task_id") or state.get("story_id", "")
    
    # Langfuse tracing for run_code
    run_code_span = _get_langfuse_span(state, "run_code", {
        "workspace": workspace_path,
        "task_id": task_id,
    })
    
    try:
        if not workspace_path or not Path(workspace_path).exists():
            logger.warning("[run_code] No workspace path, skipping tests")
            if run_code_span:
                run_code_span.end(output={"status": "PASS", "reason": "No workspace"})
            return {
                **state,
                "run_status": "PASS",
                "run_result": {"status": "PASS", "summary": "No workspace to test"},
            }
        
        # Setup tool context for shell tools
        _setup_tool_context(workspace_path, project_id, task_id)
        
        # Require project_config.tech_stack.service (array of service configs)
        project_config = state.get("project_config", {})
        tech_stack = project_config.get("tech_stack", {})
        services = tech_stack.get("service", []) if isinstance(tech_stack, dict) else []
        
        if not services:
            logger.error("[run_code] Missing project_config.tech_stack.service")
            if run_code_span:
                run_code_span.end(output={"status": "ERROR", "reason": "Missing tech_stack.service"})
            return {
                **state,
                "run_status": "ERROR",
                "run_result": {"status": "ERROR", "summary": "Missing project_config.tech_stack.service"},
            }
        
        # tech_stack.service is array of service configs
        svc_names = [s.get("name", "app") for s in services]
        logger.info(f"[run_code] tech_stack services: {svc_names}")
        if run_code_span:
            run_code_span.end(output={"mode": "tech_stack", "services": svc_names})
        return await _run_code_multi_service(state, workspace_path, services)
        
    except Exception as e:
        logger.error(f"[run_code] Error: {e}", exc_info=True)
        if run_code_span:
            run_code_span.end(output={"error": str(e)})
        return {
            **state,
            "run_status": "PASS",  # Pass on error to continue flow
            "run_result": {"status": "PASS", "summary": f"Test execution error: {str(e)}"},
        }


# =============================================================================
# DEBUG ERROR (Fix bugs based on test output)
# =============================================================================

async def debug_error(state: DeveloperState, agent=None) -> DeveloperState:
    """Debug and fix errors.
    
    Refactored: Tools for exploration + with_structured_output for response.
    """
    print("[NODE] debug_error - Fixing errors...")
    try:
        run_result = state.get("run_result", {})
        workspace_path = state.get("workspace_path", "")
        project_id = state.get("project_id", "default")
        task_id = state.get("task_id") or state.get("story_id", "")
        debug_count = state.get("debug_count", 0)
        max_debug = state.get("max_debug", 5)  # MetaGPT pattern
        
        if run_result.get("status") == "PASS":
            logger.info("[debug_error] No errors to debug")
            return state
        
        # MetaGPT DebugError pattern: Check if tests already pass via "OK" pattern
        stderr = state.get("run_stderr", "") or run_result.get("stderr", "")
        ok_pattern = r"Ran (\d+) tests? in ([\d.]+)s\s*\n\s*OK"
        if re.search(ok_pattern, stderr):
            logger.info("[debug_error] Tests already pass (OK pattern detected), skipping")
            return {**state, "run_result": {"status": "PASS", "summary": "All tests passed"}}
        
        if debug_count >= max_debug:
            logger.warning(f"[debug_error] Max debug attempts ({max_debug}) reached")
            if agent:
                pass
            return state
        
        file_to_fix = run_result.get("file_to_fix", "")
        if not file_to_fix:
            # Try to get from modified files
            files_modified = state.get("files_modified", [])
            file_to_fix = files_modified[0] if files_modified else ""
        
        if not file_to_fix:
            logger.warning("[debug_error] No file identified to fix")
            return {**state, "debug_count": debug_count + 1}
        
        if agent:
            pass
        
        # Read the file to fix using read_file_safe tool
        code_content = ""
        if workspace_path:
            try:
                result = read_file_safe.invoke({"file_path": file_to_fix})
                if result and not result.startswith("Error:"):
                    if "\n\n" in result:
                        code_content = result.split("\n\n", 1)[1]
                    else:
                        code_content = result
            except Exception:
                pass
        
        # Find and read test file using read_file_safe tool
        test_filename = find_test_file(workspace_path, file_to_fix) if workspace_path else ""
        test_content = ""
        if test_filename and workspace_path:
            try:
                result = read_file_safe.invoke({"file_path": test_filename})
                if result and not result.startswith("Error:"):
                    if "\n\n" in result:
                        test_content = result.split("\n\n", 1)[1]
                    else:
                        test_content = result
            except Exception:
                pass
        
        language = get_markdown_code_block_type(file_to_fix)
        
        # Setup tool context
        _setup_tool_context(workspace_path, project_id, task_id)
        
        # Get tech_stack info for install commands
        project_config = state.get("project_config", {})
        tech_stack = project_config.get("tech_stack", {})
        services = tech_stack.get("service", []) if isinstance(tech_stack, dict) else []
        
        # Build tech_stack context for prompt
        tech_stack_info = ""
        if services:
            for svc in services:
                runtime = svc.get("runtime", "node")
                install_cmd = svc.get("install_cmd", "npm install")
                tech_stack_info += f"- Runtime: {runtime}, Install command pattern: {install_cmd}\n"
        
        # Build debug prompt
        sys_prompt = _build_system_prompt("debug_error", agent)
        user_prompt = _get_prompt("debug_error", "user_prompt").format(
            code_filename=file_to_fix,
            language=language,
            code=code_content or "No code available",
            test_filename=test_filename or "No test file",
            test_code=test_content or "No test code available",
            error_logs=state.get("run_stderr", "")[:8000],
            error_summary=run_result.get("summary", ""),
            file_to_fix=file_to_fix,
            tech_stack_info=tech_stack_info or "Not specified",
        )
        
        # Tools for debugging exploration (including Tavily for web search)
        tavily_tool = TavilySearch(max_results=3)
        tools = [read_file_safe, list_directory_safe, semantic_code_search, execute_shell, tavily_tool]
        
        # Step 1: Explore with tools
        messages = [
            SystemMessage(content=sys_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        exploration = await _llm_with_tools(
            llm=_code_llm,
            tools=tools,
            messages=messages,
            state=state,
            name="debug_explore",
            max_iterations=2
        )
        
        # Step 2: Get structured response (exploration already filtered by semantic search)
        messages.append(HumanMessage(content=f"Context gathered:\n{exploration[:5000]}\n\nNow provide your debug analysis and fixed code."))
        structured_llm = _code_llm.with_structured_output(DebugResult)
        debug_result = await structured_llm.ainvoke(messages, config=_cfg(state, "debug_error"))
        
        if debug_result.fixed_code and workspace_path:
            # Write fixed code using write_file_safe tool
            try:
                result = write_file_safe.invoke({
                    "file_path": file_to_fix,
                    "content": debug_result.fixed_code,
                    "mode": "w"
                })
                logger.info(f"[debug_error] {result}")
                # Note: Removed incremental_update_index - not needed for debug fixes
            except Exception as e:
                logger.error(f"[debug_error] Failed to write fixed code: {e}")
        
        # Update debug history
        debug_history = state.get("debug_history", []) or []
        debug_history.append({
            "iteration": debug_count + 1,
            "file": file_to_fix,
            "analysis": debug_result.analysis,
            "root_cause": debug_result.root_cause,
            "fix_description": debug_result.fix_description,
        })
        
        return {
            **state,
            "debug_count": debug_count + 1,
            "last_debug_file": file_to_fix,
            "debug_history": debug_history,
        }
        
    except Exception as e:
        logger.error(f"[debug_error] Error: {e}", exc_info=True)
        return {**state, "debug_count": state.get("debug_count", 0) + 1}
