"""
Tavily Search Tool

Wrapper cho Tavily Search API ƒë·ªÉ t√¨m ki·∫øm web khi c·∫ßn th√¥ng tin b·ªï sung
cho vi·ªác t·∫°o implementation plan.
"""

import json
import os
import time
from typing import Any

from langchain_core.tools import tool

try:
    from tavily import TavilyClient
except ImportError:
    TavilyClient = None

from pydantic import BaseModel


def create_tavily_client(api_key: str | None = None) -> TavilyClient | None:
    """
    T·∫°o Tavily Client ƒë·ªÉ s·ª≠ d·ª•ng Python SDK.

    Args:
        api_key: Tavily API key (optional, s·∫Ω l·∫•y t·ª´ env n·∫øu kh√¥ng c√≥)

    Returns:
        TavilyClient instance ho·∫∑c None n·∫øu kh√¥ng available
    """
    if TavilyClient is None:
        return None

    try:
        if api_key is None:
            api_key = os.getenv("TAVILY_API_KEY")

        if not api_key:
            return None

        return TavilyClient(api_key=api_key)
    except Exception:
        return None


class SearchResult(BaseModel):
    """Model cho k·∫øt qu·∫£ t√¨m ki·∫øm t·ª´ Tavily."""

    title: str = ""
    url: str = ""
    content: str = ""
    score: float = 0.0
    published_date: str | None = None
    raw_content: str | None = None  # Th√™m raw_content t·ª´ crawl


class CrawlResult(BaseModel):
    """Model cho k·∫øt qu·∫£ crawl t·ª´ Tavily."""

    url: str = ""
    raw_content: str = ""
    favicon: str | None = None


class WebSearchResults(BaseModel):
    """Model cho t·ªïng h·ª£p k·∫øt qu·∫£ web search."""

    query: str = ""
    results: list[SearchResult] = []
    crawl_result: CrawlResult | None = None  # Th√™m crawl result
    total_results: int = 0
    search_time: float = 0.0
    crawl_time: float = 0.0  # Th√™m crawl time
    summary: str = ""


@tool
def tavily_search_tool(
    query: str,
    max_results: int = 5,
    search_depth: str = "basic",
    include_answer: bool = True,
    include_raw_content: bool = True,
    include_images: bool = False,
) -> str:
    """
    T√¨m ki·∫øm web s·ª≠ d·ª•ng Tavily Python SDK v·ªõi workflow search + crawl.

    Workflow:
    1. Search ƒë·ªÉ l·∫•y top URLs v·ªõi summary content
    2. Crawl URL ƒë·∫ßu ti√™n (highest score) ƒë·ªÉ l·∫•y raw_content chi ti·∫øt
    3. K·∫øt h·ª£p c·∫£ summary v√† raw_content ƒë·ªÉ c√≥ th√¥ng tin ƒë·∫ßy ƒë·ªß

    Args:
        query: Search query string
        max_results: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ t·ªëi ƒëa (default: 5)
        search_depth: ƒê·ªô s√¢u t√¨m ki·∫øm - "basic" ho·∫∑c "advanced" (default: "basic")
        include_answer: C√≥ bao g·ªìm AI-generated answer kh√¥ng (default: True)
        include_raw_content: C√≥ crawl ƒë·ªÉ l·∫•y raw content hay kh√¥ng (default: True)
        include_images: C√≥ bao g·ªìm images kh√¥ng (default: False)

    Returns:
        JSON string v·ªõi k·∫øt qu·∫£ t√¨m ki·∫øm v√† crawl
    """
    try:
        print(f"üîç Searching web for: {query}")

        # Ki·ªÉm tra API key
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            return json.dumps(
                {
                    "status": "error",
                    "message": "TAVILY_API_KEY not found in environment variables",
                    "query": query,
                    "results": [],
                },
                indent=2,
            )

        # T·∫°o Tavily client
        client = create_tavily_client()
        if client is None:
            return json.dumps(
                {
                    "status": "error",
                    "message": "Tavily client not available. Please install tavily-python package and set TAVILY_API_KEY.",
                    "query": query,
                    "results": [],
                },
                indent=2,
            )

        # B∆∞·ªõc 1: Search ƒë·ªÉ l·∫•y URLs v√† summary content
        start_time = time.time()

        search_response = client.search(
            query=query,
            search_depth=search_depth,
            max_results=max_results,
            include_answer=include_answer,
            include_images=include_images,
            include_raw_content=False,  # Kh√¥ng l·∫•y raw_content trong search
        )

        search_time = time.time() - start_time

        print(
            f"üìä Found {len(search_response.get('results', []))} results in {search_time:.2f}s"
        )

        # Parse search results
        search_results = []
        crawl_result = None
        crawl_time = 0.0

        if search_response.get("results"):
            for result in search_response["results"]:
                search_result = SearchResult(
                    title=result.get("title", ""),
                    url=result.get("url", ""),
                    content=result.get("content", ""),
                    score=result.get("score", 0.0),
                    published_date=result.get("published_date"),
                )
                search_results.append(search_result)

            # B∆∞·ªõc 2: Crawl URL ƒë·∫ßu ti√™n (highest score) ƒë·ªÉ l·∫•y raw_content
            if include_raw_content and search_results:
                try:
                    top_result = search_results[0]  # URL c√≥ score cao nh·∫•t
                    print(f"üï∑Ô∏è Crawling top result: {top_result.url}")

                    crawl_start_time = time.time()
                    crawl_response = client.crawl(
                        url=top_result.url,
                        instructions=f"Extract detailed information about: {query}",
                        max_depth=2,
                        max_breadth=10,
                        extract_depth="advanced",
                    )
                    crawl_time = time.time() - crawl_start_time

                    print(f"üï∑Ô∏è Crawl completed in {crawl_time:.2f}s")

                    if crawl_response.get("results"):
                        crawl_data = crawl_response["results"][0]
                        crawl_result = CrawlResult(
                            url=crawl_data.get("url", top_result.url),
                            raw_content=crawl_data.get("raw_content", ""),
                            favicon=crawl_data.get("favicon"),
                        )

                        # C·∫≠p nh·∫≠t search result ƒë·∫ßu ti√™n v·ªõi raw_content
                        search_results[0].raw_content = crawl_result.raw_content

                except Exception as crawl_error:
                    print(f"‚ö†Ô∏è Crawl failed: {crawl_error}")
                    # Ti·∫øp t·ª•c v·ªõi search results, kh√¥ng fail to√†n b·ªô

        # T·∫°o summary t·ª´ search results v√† crawl content
        summary = _generate_enhanced_search_summary(query, search_results, crawl_result)

        # T·∫°o response dict
        result_dict = {
            "status": "success",
            "query": query,
            "total_results": len(search_results),
            "search_time": search_time,
            "crawl_time": crawl_time,
            "has_crawl_content": crawl_result is not None,
            "summary": summary,
            "results": [result.model_dump() for result in search_results],
            "crawl_result": crawl_result.model_dump() if crawl_result else None,
        }

        print("‚úÖ Web search completed successfully")
        return json.dumps(result_dict, indent=2)

    except Exception as e:
        print(f"‚ùå Web search failed: {e}")
        return json.dumps(
            {
                "status": "error",
                "message": f"Search failed: {str(e)}",
                "query": query,
                "results": [],
            },
            indent=2,
        )


def _generate_enhanced_search_summary(
    query: str, results: list[SearchResult], crawl_result: CrawlResult | None = None
) -> str:
    """
    T·∫°o enhanced summary t·ª´ k·∫øt qu·∫£ search v√† crawl.

    Args:
        query: Original search query
        results: List of search results
        crawl_result: Crawl result v·ªõi raw content (optional)

    Returns:
        Enhanced summary string
    """
    if not results:
        return f"No search results found for query: '{query}'"

    summary_parts = []

    # Ph·∫ßn 1: T·ªïng quan t·ª´ search results
    summary_parts.append(f"Search Results for '{query}':")
    summary_parts.append(f"Found {len(results)} relevant sources:")

    for i, result in enumerate(results[:3], 1):  # Top 3 results
        summary_parts.append(f"{i}. {result.title}")
        if result.content:
            content_preview = (
                result.content[:200] + "..."
                if len(result.content) > 200
                else result.content
            )
            summary_parts.append(f"   Summary: {content_preview}")
        summary_parts.append(f"   Source: {result.url}")
        if result.score > 0:
            summary_parts.append(f"   Relevance: {result.score:.2f}")
        summary_parts.append("")

    # Ph·∫ßn 2: Detailed content t·ª´ crawl (n·∫øu c√≥)
    if crawl_result and crawl_result.raw_content:
        summary_parts.append("--- Detailed Content (from crawl) ---")
        raw_content = crawl_result.raw_content

        # Truncate raw content n·∫øu qu√° d√†i
        if len(raw_content) > 2000:
            raw_content = raw_content[:2000] + "\n... (content truncated)"

        summary_parts.append(raw_content)
        summary_parts.append("")
        summary_parts.append(f"Full content source: {crawl_result.url}")

    # Ph·∫ßn 3: Key insights
    summary_parts.append("--- Key Insights ---")
    if crawl_result and crawl_result.raw_content:
        summary_parts.append(
            "‚úÖ Detailed implementation information available from crawled content"
        )
    else:
        summary_parts.append("‚ÑπÔ∏è Summary information available from search results")

    summary_parts.append(f"üìä Total sources analyzed: {len(results)}")

    return "\n".join(summary_parts)


def _generate_search_summary(query: str, results: list[SearchResult]) -> str:
    """
    T·∫°o summary t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm.

    Args:
        query: Original search query
        results: List of search results

    Returns:
        Summary string
    """
    if not results:
        return f"No relevant results found for query: {query}"

    # L·∫•y top 3 results ƒë·ªÉ t·∫°o summary
    top_results = results[:3]

    summary_parts = [f"Found {len(results)} results for '{query}':"]

    for i, result in enumerate(top_results, 1):
        content_preview = (
            result.content[:200] + "..."
            if len(result.content) > 200
            else result.content
        )
        summary_parts.append(f"{i}. {result.title}: {content_preview}")

    return "\n".join(summary_parts)


def should_perform_websearch(
    task_description: str, task_requirements: dict[str, Any], codebase_context: str = ""
) -> tuple[bool, str]:
    """
    Quy·∫øt ƒë·ªãnh c√≥ c·∫ßn th·ª±c hi·ªán web search hay kh√¥ng d·ª±a tr√™n task analysis.

    Args:
        task_description: M√¥ t·∫£ task g·ªëc
        task_requirements: Parsed task requirements
        codebase_context: Context v·ªÅ codebase hi·ªán t·∫°i

    Returns:
        Tuple of (should_search: bool, reason: str)
    """
    # C√°c keywords cho th·∫•y c·∫ßn t√¨m ki·∫øm th√™m th√¥ng tin
    search_indicators = [
        "best practices",
        "how to implement",
        "integration with",
        "latest version",
        "documentation",
        "tutorial",
        "example",
        "guide",
        "API reference",
        "configuration",
        "setup",
        "install",
        "deploy",
        "security",
        "performance",
        "optimization",
        "third-party",
        "external service",
        "library",
        "framework",
        "tool",
        "service",
        "platform",
    ]

    # Ki·ªÉm tra task description
    task_lower = task_description.lower()
    found_indicators = [
        indicator for indicator in search_indicators if indicator in task_lower
    ]

    # Ki·ªÉm tra requirements
    requirements = task_requirements.get("requirements", [])
    technical_specs = task_requirements.get("technical_specs", {})

    # N·∫øu c√≥ √≠t th√¥ng tin technical specs
    has_limited_tech_info = len(technical_specs) < 2

    # N·∫øu c√≥ nhi·ªÅu requirements ph·ª©c t·∫°p
    has_complex_requirements = len(requirements) > 5

    # Quy·∫øt ƒë·ªãnh
    if found_indicators:
        return True, f"Found search indicators: {', '.join(found_indicators[:3])}"
    elif has_limited_tech_info and has_complex_requirements:
        return True, "Limited technical specifications for complex requirements"
    elif not codebase_context.strip():
        return True, "No codebase context provided, need external information"
    else:
        return False, "Sufficient information available for implementation planning"


def generate_search_queries(
    task_description: str, task_requirements: dict[str, Any]
) -> list[str]:
    """
    T·∫°o danh s√°ch search queries d·ª±a tr√™n task analysis.

    Args:
        task_description: M√¥ t·∫£ task g·ªëc
        task_requirements: Parsed task requirements

    Returns:
        List of search query strings
    """
    queries = []

    # Base query t·ª´ task description
    base_query = task_description[:100]  # Limit length
    queries.append(f"{base_query} implementation guide")

    # Queries t·ª´ technical specs
    technical_specs = task_requirements.get("technical_specs", {})
    for tech, spec in technical_specs.items():
        if isinstance(spec, str) and spec:
            queries.append(f"{tech} {spec} best practices")

    # Queries t·ª´ requirements
    requirements = task_requirements.get("requirements", [])
    for req in requirements[:2]:  # Limit to top 2 requirements
        if len(req) > 10:  # Only meaningful requirements
            queries.append(f"{req} implementation example")

    # Limit total queries
    return queries[:3]
