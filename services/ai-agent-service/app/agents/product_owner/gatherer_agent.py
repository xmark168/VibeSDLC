"""L·ªõp n√†y ch·ª©a LangGraph Agent/workflow v√† c√°c t∆∞∆°ng t√°c v·ªõi LLM cho gatherer agent."""

import json
import os
import re
from typing import Any, Literal

from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langfuse.langchain import CallbackHandler
from langgraph.graph import END, START, StateGraph
from pydantic import BaseModel, Field

from templates.prompts.product_owner.gatherer import (
    EVALUATE_PROMPT,
    CLARIFY_PROMPT,
    SUGGEST_PROMPT,
    ASK_USER_PROMPT,
    GENERATE_PROMPT,
    VALIDATE_PROMPT,
    FINALIZE_PROMPT,
    EDIT_MODE_PROMPT,
    FORCE_GENERATE_PROMPT,
)
from langgraph.checkpoint.memory import MemorySaver

load_dotenv()


class EvaluateOutput(BaseModel):
    gaps: list[str] = Field(description="Danh s√°ch c√°c th√¥ng tin c√≤n thi·∫øu")
    score: float = Field(description="ƒêi·ªÉm ƒë√°nh gi√° ƒë·ªô ƒë·∫ßy ƒë·ªß", ge=0.0, le=1.0)
    status: Literal["incomplete", "done"] = Field(
        description="Tr·∫°ng th√°i: 'incomplete' n·∫øu score < 0.8, 'done' n·∫øu score >= 0.8"
    )
    confidence: float = Field(description="ƒê·ªô tin c·∫≠y ƒë√°nh gi√°", ge=0.0, le=1.0)
    message: str = Field(description="L√Ω do")


class EvaluateMessageOutput(BaseModel):
    is_unclear: bool = Field(
        description="True n·∫øu message m∆° h·ªì/kh√¥ng r√µ r√†ng, False n·∫øu r√µ r√†ng"
    )
    reason: str = Field(description="L√Ω do ƒë√°nh gi√° message l√† unclear ho·∫∑c clear")


class ClarifyOutput(BaseModel):
    summary: str = Field(description="T√≥m t·∫Øt nh·ªØng g√¨ ƒë√£ hi·ªÉu t·ª´ cu·ªôc h·ªôi tho·∫°i")
    unclear_points: list[str] = Field(
        description="Danh s√°ch c√°c ƒëi·ªÉm c√≤n m∆° h·ªì ho·∫∑c c·∫ßn l√†m r√µ"
    )
    clarified_gaps: list[str] = Field(
        description="Danh s√°ch gaps ƒë√£ ƒë∆∞·ª£c l√†m r√µ v√† c·∫ßn ∆∞u ti√™n thu th·∫≠p"
    )
    message_to_user: str = Field(
        description="Th√¥ng ƒëi·ªáp g·ª≠i ƒë·∫øn user ƒë·ªÉ x√°c nh·∫≠n hi·ªÉu bi·∫øt v√† y√™u c·∫ßu l√†m r√µ"
    )


class FilledGap(BaseModel):
    gap_name: str = Field(description="T√™n c·ªßa gap")
    suggested_value: str = Field(description="Gi√° tr·ªã g·ª£i √Ω ƒë·ªÉ fill gap")
    reason: str = Field(description="L√Ω do ng·∫Øn g·ªçn t·∫°i sao g·ª£i √Ω gi√° tr·ªã n√†y")


class SuggestOutput(BaseModel):
    prioritized_gaps: list[str] = Field(
        description="Danh s√°ch gaps c√≤n l·∫°i ch∆∞a fill ƒë∆∞·ª£c, s·∫Øp x·∫øp theo ƒë·ªô ∆∞u ti√™n"
    )
    filled_gaps: list[FilledGap] = Field(
        description="Danh s√°ch c√°c gaps ƒë√£ g·ª£i √Ω fill v·ªõi gi√° tr·ªã v√† l√Ω do"
    )


class CollectInputsOutput(BaseModel):
    total_messages: int = Field(description="T·ªïng s·ªë messages trong context")
    new_input_received: bool = Field(description="True n·∫øu c√≥ input m·ªõi t·ª´ user")
    last_message_type: str = Field(description="Lo·∫°i message cu·ªëi: human ho·∫∑c ai")
    last_message_preview: str = Field(
        description="Preview 200 k√Ω t·ª± ƒë·∫ßu c·ªßa message cu·ªëi"
    )
    context_summary: str = Field(description="T√≥m t·∫Øt ng·∫Øn g·ªçn context hi·ªán t·∫°i")


class AskUserOutput(BaseModel):
    questions: list[str] = Field(
        description="Danh s√°ch t·ªëi ƒëa 3 c√¢u h·ªèi ƒë·ªÉ thu th·∫≠p th√¥ng tin cho c√°c gaps"
    )


class WaitForUserOutput(BaseModel):
    has_responses: bool = Field(
        description="True n·∫øu user ƒë√£ tr·∫£ l·ªùi √≠t nh·∫•t 1 c√¢u h·ªèi"
    )
    answered_count: int = Field(description="S·ªë c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c tr·∫£ l·ªùi")
    skipped_count: int = Field(description="S·ªë c√¢u h·ªèi b·ªã b·ªè qua")
    skip_all: bool = Field(description="True n·∫øu user ch·ªçn skip_all")
    user_responses: list[dict[str, str]] = Field(
        description="Danh s√°ch {question, answer} c·ªßa c√°c c√¢u ƒë√£ tr·∫£ l·ªùi"
    )
    status: str = Field(
        description="Tr·∫°ng th√°i: user_responded, skipped_all, no_responses, ho·∫∑c error"
    )
    message: str = Field(description="Th√¥ng ƒëi·ªáp t√≥m t·∫Øt k·∫øt qu·∫£ thu th·∫≠p")


class GenerateOutput(BaseModel):
    product_name: str = Field(description="T√™n s·∫£n ph·∫©m")
    description: str = Field(description="M√¥ t·∫£ chi ti·∫øt s·∫£n ph·∫©m")
    target_audience: list[str] = Field(description="Danh s√°ch ƒë·ªëi t∆∞·ª£ng m·ª•c ti√™u")
    key_features: list[str] = Field(description="Danh s√°ch t√≠nh nƒÉng ch√≠nh")
    benefits: list[str] = Field(description="Danh s√°ch l·ª£i √≠ch")
    competitors: list[str] = Field(
        default_factory=list, description="Danh s√°ch ƒë·ªëi th·ªß c·∫°nh tranh"
    )
    completeness_note: str = Field(description="Ghi ch√∫ v·ªÅ m·ª©c ƒë·ªô ho√†n thi·ªán")


class EditModeOutput(BaseModel):
    product_name: str = Field(description="T√™n s·∫£n ph·∫©m")
    description: str = Field(description="M√¥ t·∫£ chi ti·∫øt s·∫£n ph·∫©m")
    target_audience: list[str] = Field(description="Danh s√°ch ƒë·ªëi t∆∞·ª£ng m·ª•c ti√™u")
    key_features: list[str] = Field(description="Danh s√°ch t√≠nh nƒÉng ch√≠nh")
    benefits: list[str] = Field(description="Danh s√°ch l·ª£i √≠ch")
    competitors: list[str] = Field(
        default_factory=list, description="Danh s√°ch ƒë·ªëi th·ªß c·∫°nh tranh"
    )
    completeness_note: str = Field(description="Ghi ch√∫ v·ªÅ thay ƒë·ªïi ƒë√£ √°p d·ª•ng")


class ForceGenerateOutput(BaseModel):
    product_name: str = Field(description="T√™n s·∫£n ph·∫©m")
    description: str = Field(description="M√¥ t·∫£ chi ti·∫øt s·∫£n ph·∫©m")
    target_audience: list[str] = Field(description="Danh s√°ch ƒë·ªëi t∆∞·ª£ng m·ª•c ti√™u")
    key_features: list[str] = Field(description="Danh s√°ch t√≠nh nƒÉng ch√≠nh")
    benefits: list[str] = Field(description="Danh s√°ch l·ª£i √≠ch")
    competitors: list[str] = Field(
        default_factory=list, description="Danh s√°ch ƒë·ªëi th·ªß c·∫°nh tranh"
    )
    completeness_note: str = Field(
        description="Ghi ch√∫ v·ªÅ m·ª©c ƒë·ªô ho√†n thi·ªán v√† c√°c ph·∫ßn suy lu·∫≠n"
    )
    incomplete_flag: bool = Field(
        description="Flag ƒë√°nh d·∫•u brief ch∆∞a ho√†n ch·ªânh", default=True
    )


class ValidateOutput(BaseModel):
    is_valid: bool = Field(description="True n·∫øu brief ƒë·∫°t y√™u c·∫ßu t·ªëi thi·ªÉu")
    confidence_score: float = Field(description="ƒê·ªô tin c·∫≠y c·ªßa brief", ge=0.0, le=1.0)
    completeness_score: float = Field(
        description="ƒêi·ªÉm ƒë√°nh gi√° ƒë·ªô ƒë·∫ßy ƒë·ªß", ge=0.0, le=1.0
    )
    missing_fields: list[str] = Field(
        description="Danh s√°ch fields c√≤n thi·∫øu ho·∫∑c ch∆∞a ƒë·∫ßy ƒë·ªß"
    )
    validation_message: str = Field(
        description="Gi·∫£i th√≠ch ng·∫Øn g·ªçn k·∫øt qu·∫£ validation"
    )


class FinalizeOutput(BaseModel):
    product_name: str = Field(description="T√™n s·∫£n ph·∫©m")
    executive_summary: str = Field(description="T√≥m t·∫Øt ƒëi·ªÅu h√†nh 2-3 c√¢u")
    target_users: str = Field(description="ƒê·ªëi t∆∞·ª£ng m·ª•c ti√™u ch√≠nh (1 c√¢u ng·∫Øn)")
    top_features: list[str] = Field(description="Danh s√°ch 3-5 t√≠nh nƒÉng n·ªïi b·∫≠t")
    core_value: str = Field(description="Gi√° tr·ªã c·ªët l√µi mang l·∫°i (1-2 c√¢u)")
    summary_markdown: str = Field(description="T√≥m t·∫Øt ƒë·∫ßy ƒë·ªß theo format markdown")


class State(BaseModel):
    """Tr·∫°ng th√°i cho quy tr√¨nh l√†m vi·ªác c·ªßa gatherer agent."""

    messages: list[BaseMessage] = Field(default_factory=list)
    iteration_count: int = 0
    max_iterations: int = 3
    retry_count: int = 0
    gaps: list[str] = Field(default_factory=list)
    score: float = 0.0
    status: str = "initial"
    confidence: float = 0.0
    message: str = ""
    brief: dict = Field(default_factory=dict)
    incomplete_flag: bool = False
    questions: list[str] = Field(default_factory=list)
    unclear_input: list[str] = Field(default_factory=list)
    user_choice: Literal["approve", "edit", "regenerate", "skip", ""] = ""
    edit_changes: str = ""
    user_skipped: bool = False


class GathererAgent:
    """Gatherer Agent ƒë·ªÉ thu th·∫≠p th√¥ng tin s·∫£n ph·∫©m gi√∫p t·∫°o backlog trong t∆∞∆°ng lai."""

    def __init__(self, session_id: str | None = None, user_id: str | None = None):
        """Kh·ªüi t·∫°o gatherer agent.

        Args:
            session_id: Session ID t√πy ch·ªçn ƒë·ªÉ theo d√µi
            user_id: User ID t√πy ch·ªçn ƒë·ªÉ theo d√µi
        """
        self.session_id = session_id
        self.user_id = user_id

        self.langfuse_handler = CallbackHandler()

        self.graph = self._build_graph()

    def _llm(self, model: str, temperature: str) -> ChatOpenAI:
        try:
            llm = ChatOpenAI(
                model=model,
                temperature=temperature,
                api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_BASE_URL"),
            )
            return llm
        except Exception:
            return ""

    def _build_graph(self) -> StateGraph:
        """X√¢y d·ª±ng quy tr√¨nh l√†m vi·ªác LangGraph."""
        graph_builder = StateGraph(State)

        graph_builder.add_node("initialize", self._initialize)
        graph_builder.add_node("evaluate", self.evaluate)
        graph_builder.add_node("clarify", self.clarify)
        graph_builder.add_node("suggest", self.suggest)
        graph_builder.add_node("ask_user", self.ask_user)
        graph_builder.add_node("increment_iteration", self.increment_iteration)
        graph_builder.add_node("wait_for_user", self.wait_for_user)
        graph_builder.add_node("generate", self.generate)
        graph_builder.add_node("force_generate", self.force_generate)
        graph_builder.add_node("validate", self.validate)
        graph_builder.add_node("retry_decision", self.retry_decision)
        graph_builder.add_node("preview", self.preview)
        graph_builder.add_node("edit_mode", self.edit_mode)
        graph_builder.add_node("finalize", self.finalize)

        graph_builder.add_edge(START, "initialize")
        graph_builder.add_edge("initialize", "evaluate")
        graph_builder.add_conditional_edges("evaluate", self.evaluate_branch)
        graph_builder.add_edge("clarify", "suggest")
        graph_builder.add_edge("suggest", "ask_user")
        graph_builder.add_edge("ask_user", "increment_iteration")
        graph_builder.add_edge("increment_iteration", "wait_for_user")
        graph_builder.add_conditional_edges("wait_for_user", self.wait_for_user_branch)
        graph_builder.add_edge("generate", "validate")
        graph_builder.add_edge("force_generate", "validate")
        graph_builder.add_conditional_edges("validate", self.validate_branch)
        graph_builder.add_conditional_edges("preview", self.preview_branch)
        graph_builder.add_conditional_edges(
            "retry_decision", self.retry_decision_branch
        )
        graph_builder.add_edge("edit_mode", "validate")
        graph_builder.add_edge("finalize", END)
        checkpointer = MemorySaver()
        return graph_builder.compile(checkpointer=checkpointer)

    def _initialize(self, state: State) -> State:
        print(state)
        """Kh·ªüi t·∫°o tr·∫°ng th√°i."""
        return state

    def evaluate_message(self, message: str) -> bool:
        """ƒê√°nh gi√° xem message cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng c√≥ unclear hay kh√¥ng b·∫±ng regex patterns."""
        message_lower = message.lower().strip()

        unclear_patterns = [
            r"\b(n√≥|c√°i ƒë√≥|c√°i n√†y|th·ª© ƒë√≥|th·ª© n√†y|ch·ªó ƒë√≥|ch·ªó n√†y|c√°i kia|th·∫±ng ƒë√≥)\b",
            r"\b(nh∆∞ tr√™n|nh∆∞ v·∫≠y|nh∆∞ th·∫ø|y nh∆∞|y chang|t∆∞∆°ng t·ª±|gi·ªëng v·∫≠y|nh∆∞ kia)\b",
            r"^.{1,10}$",
            r"^\s*(sao|th·∫ø n√†o|nh∆∞ n√†o|ra sao|g√¨|√†|h·∫£|·ª´|uh|uhm)\s*\??\s*$",
            r"^\s*(c√≥|kh√¥ng|ok|ƒë∆∞·ª£c|r·ªìi|·ª´|uh|yes|no|yeah|nope)\s*$",
        ]

        for pattern in unclear_patterns:
            if re.search(pattern, message_lower):
                return True

        return False

    def evaluate(self, state: State) -> State:
        """ƒê√°nh gi√° ƒë·ªô ƒë·∫ßy ƒë·ªß c·ªßa cu·ªôc h·ªôi tho·∫°i ƒë·ªÉ t·∫°o b·∫£n t√≥m t·∫Øt s·ª≠ d·ª•ng structured output."""
        if state.messages:
            last_message = state.messages[-1]
            if last_message.type == "human":
                is_unclear = self.evaluate_message(last_message.content)
                if is_unclear:
                    state.unclear_input.append(last_message.content)

        formatted_messages = (
            "\n".join(
                [
                    f"{i}. [{'User' if msg.type=='human' else 'Assistant'}]: "
                    f"{msg.content if hasattr(msg, 'content') else str(msg)}"
                    for i, msg in enumerate(state.messages, 1)
                ]
            )
            if state.messages
            else "Ch∆∞a c√≥ th√¥ng tin n√†o ƒë∆∞·ª£c thu th·∫≠p."
        )

        prompt = EVALUATE_PROMPT.format(messages=formatted_messages)

        structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(
            EvaluateOutput
        )
        evaluation = structured_llm.invoke([HumanMessage(content=prompt)])

        state.gaps = evaluation.gaps
        state.score = evaluation.score
        state.confidence = evaluation.confidence
        state.message = evaluation.message

        state.status = "done" if evaluation.score >= 0.8 else "incomplete"

        return state

    def force_generate(self, state: State) -> State:
        """T·∫°o brief v·ªõi th√¥ng tin hi·ªán c√≥ d√π ch∆∞a ƒë·∫ßy ƒë·ªß khi ƒë·∫°t max_iterations.

        Theo s∆° ƒë·ªì:
        - Input: cu·ªôc h·ªôi tho·∫°i khi iteration_count >= max_iterations
        - Generate brief v·ªõi available info, flag: incomplete
        - Output: ForceGenerateOutput structured JSON
        """
        state.user_choice = ""

        formatted_messages = "\n".join(
            [
                f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content}"
                for msg in state.messages
            ]
        )

        prompt = FORCE_GENERATE_PROMPT.format(messages=formatted_messages)

        try:
            structured_llm = self._llm("gpt-4.1", 0.3).with_structured_output(
                ForceGenerateOutput
            )
            brief_output = structured_llm.invoke([HumanMessage(content=prompt)])

            state.brief = brief_output.model_dump()
            state.incomplete_flag = brief_output.incomplete_flag

            print("\n" + "=" * 80)
            print("‚ö†Ô∏è  FORCE GENERATE - T·∫†O BRIEF V·ªöI TH√îNG TIN CH∆ØA ƒê·∫¶Y ƒê·ª¶")
            print("=" * 80)
            print(f"‚ö†Ô∏è  ƒê√£ ƒë·∫°t s·ªë l·∫ßn l·∫∑p t·ªëi ƒëa ({state.max_iterations})")
            print(
                f"‚ö†Ô∏è  Brief ƒë∆∞·ª£c t·∫°o v·ªõi th√¥ng tin hi·ªán c√≥, m·ªôt s·ªë ph·∫ßn c√≥ th·ªÉ ƒë∆∞·ª£c suy lu·∫≠n"
            )
            print("-" * 80)
            print(json.dumps(state.brief, ensure_ascii=False, indent=2))
            print("=" * 80 + "\n")

            print("\nüìä Structured Output t·ª´ force_generate:")
            print(json.dumps(brief_output.model_dump(), ensure_ascii=False, indent=2))
            print()

        except Exception as e:
            print(f"‚ùå L·ªói khi force generate brief: {e}")
            state.brief = {
                "product_name": "Ch∆∞a x√°c ƒë·ªãnh",
                "description": f"Brief ƒë∆∞·ª£c t·∫°o t·ª´ {len(state.messages)} messages v·ªõi th√¥ng tin ch∆∞a ƒë·∫ßy ƒë·ªß",
                "target_audience": ["Ch∆∞a x√°c ƒë·ªãnh"],
                "key_features": ["Ch∆∞a c√≥ th√¥ng tin ƒë·∫ßy ƒë·ªß"],
                "benefits": ["Ch∆∞a c√≥ th√¥ng tin ƒë·∫ßy ƒë·ªß"],
                "competitors": [],
                "completeness_note": f"L·ªói khi force generate: {str(e)}",
                "incomplete_flag": True,
            }
            state.incomplete_flag = True

        return state

    def clarify(self, state: State) -> State:
        """L√†m r√µ c√°c th√¥ng tin m∆° h·ªì ho·∫∑c kh√¥ng r√µ r√†ng trong cu·ªôc h·ªôi tho·∫°i."""
        formatted_messages = "\n".join(
            [
                f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content}"
                for msg in state.messages
            ]
        )

        unclear_inputs = (
            "\n".join([f"- {unclear}" for unclear in state.unclear_input])
            if state.unclear_input
            else "Kh√¥ng c√≥"
        )

        formatted_gaps = (
            "\n".join([f"- {gap}" for gap in state.gaps]) if state.gaps else "Kh√¥ng c√≥"
        )

        prompt = CLARIFY_PROMPT.format(
            messages=formatted_messages,
            unclear_inputs=unclear_inputs,
            gaps=formatted_gaps,
        )

        structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(ClarifyOutput)
        clarify_result = structured_llm.invoke([HumanMessage(content=prompt)])

        state.gaps = clarify_result.clarified_gaps

        state.messages.append(AIMessage(content=clarify_result.message_to_user))

        return state

    def suggest(self, state: State) -> State:
        """G·ª£i √Ω n·ªôi dung ƒë·ªÉ t·ª± ƒë·ªông fill c√°c gaps quan tr·ªçng, gi√∫p thu th·∫≠p th√¥ng tin nhanh h∆°n m√† kh√¥ng b·∫Øt user nghƒ© t·∫•t c·∫£."""
        formatted_gaps = (
            "\n".join([f"- {gap}" for gap in state.gaps])
            if state.gaps
            else "Kh√¥ng c√≥ gaps"
        )

        formatted_messages = "\n".join(
            [
                f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content[:500]}"
                for msg in state.messages[-10:]
            ]
        )

        prompt = SUGGEST_PROMPT.format(gaps=formatted_gaps, messages=formatted_messages)

        try:
            structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(
                SuggestOutput
            )
            suggest_result = structured_llm.invoke([HumanMessage(content=prompt)])

            state.gaps = suggest_result.prioritized_gaps

            if suggest_result.filled_gaps:
                filled_msg = (
                    "C√°c th√¥ng tin ƒë∆∞·ª£c g·ª£i √Ω t·ª± ƒë·ªông fill d·ª±a tr√™n ng·ªØ c·∫£nh:\n\n"
                    + "\n\n".join(
                        [
                            f"**{fg.gap_name}**\n‚Ä¢ Gi√° tr·ªã: {fg.suggested_value}\n‚Ä¢ L√Ω do: {fg.reason}"
                            for fg in suggest_result.filled_gaps
                        ]
                    )
                    + "\n\nN·∫øu kh√¥ng ch√≠nh x√°c, vui l√≤ng ch·ªânh s·ª≠a."
                )
                state.messages.append(AIMessage(content=filled_msg))
        except Exception as e:
            print(f"Error in suggest: {e}")
            pass

        return state

    def ask_user(self, state: State) -> State:
        """T·∫°o c√¢u h·ªèi ƒë·ªÉ thu th·∫≠p th√¥ng tin cho c√°c gaps c√≤n thi·∫øu."""
        formatted_gaps = (
            "\n".join([f"- {gap}" for gap in state.gaps])
            if state.gaps
            else "Kh√¥ng c√≥ gaps"
        )

        formatted_messages = "\n".join(
            [
                f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content[:500]}"
                for msg in state.messages[-10:]
            ]
        )

        prompt = ASK_USER_PROMPT.format(
            gaps=formatted_gaps, messages=formatted_messages
        )

        try:
            structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(
                AskUserOutput
            )
            ask_result = structured_llm.invoke([HumanMessage(content=prompt)])
            state.questions = ask_result.questions
            state.status = "awaiting_user"
        except Exception as e:
            print(f"Error in ask_user: {e}")
            state.questions = []
            state.status = "error_generating_questions"

        print(f"\nüìù ƒê√£ t·∫°o {len(state.questions)} c√¢u h·ªèi ƒë·ªÉ thu th·∫≠p th√¥ng tin.")

        return state

    def increment_iteration(self, state: State) -> State:
        """TƒÉng iteration count v√† checkpoint state ƒë·ªÉ c√≥ th·ªÉ resume sau n√†y."""
        state.iteration_count += 1
        print(
            f"\n=== Iteration {state.iteration_count}/{state.max_iterations} completed ==="
        )
        print(f"Current gaps: {len(state.gaps)}")
        print(
            f"Score: {state.score}, Confidence: {state.confidence}, Status: {state.status}"
        )

        return state

    def wait_for_user(self, state: State) -> State:
        """H·ªèi user t·ª´ng c√¢u m·ªôt v√† ƒë·ª£i response v·ªõi timeout 10 ph√∫t cho m·ªói c√¢u. Tr·∫£ v·ªÅ structured output."""
        import signal

        print("\n" + "=" * 60)
        print("üí¨ PH·∫¶N H·ªéI ƒê√ÅP - Thu th·∫≠p th√¥ng tin")
        print("=" * 60)
        print("üí° B·∫°n c√≥ th·ªÉ:")
        print("  - Tr·∫£ l·ªùi t·ª´ng c√¢u h·ªèi")
        print("  - G√µ 'skip' ƒë·ªÉ b·ªè qua c√¢u hi·ªán t·∫°i")
        print("  - G√µ 'skip_all' ƒë·ªÉ b·ªè qua t·∫•t c·∫£ v√† t·∫°o brief v·ªõi th√¥ng tin hi·ªán c√≥")
        print("  - Timeout: 10 ph√∫t cho m·ªói c√¢u h·ªèi")
        print("=" * 60 + "\n")

        def timeout_handler(signum, frame):
            raise TimeoutError("Timeout")

        has_responses = False
        skip_all = False
        answered_count = 0
        skipped_count = 0
        user_responses = []

        for idx, question in enumerate(state.questions, 1):
            if skip_all:
                break

            state.messages.append(AIMessage(content=question))

            print(f"\n[C√¢u h·ªèi {idx}/{len(state.questions)}]")
            print(f"‚ùì {question}\n")

            try:
                if hasattr(signal, "SIGALRM"):
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(600)

                user_input = input("üë§ C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n: ").strip()

                if hasattr(signal, "SIGALRM"):
                    signal.alarm(0)

                if user_input.lower() == "skip_all":
                    skip_all = True
                    print("\n‚äò B·∫°n ƒë√£ ch·ªçn b·ªè qua t·∫•t c·∫£ c√¢u h·ªèi c√≤n l·∫°i.")
                    break
                elif user_input.lower() == "skip":
                    print("‚äò B·ªè qua c√¢u n√†y.\n")
                    skipped_count += 1
                    continue
                elif user_input:
                    state.messages.append(HumanMessage(content=user_input))
                    user_responses.append({"question": question, "answer": user_input})
                    has_responses = True
                    answered_count += 1
                    print("‚úì ƒê√£ ghi nh·∫≠n c√¢u tr·∫£ l·ªùi.\n")
                else:
                    print("‚ö† C√¢u tr·∫£ l·ªùi tr·ªëng, b·ªè qua.\n")
                    skipped_count += 1

            except TimeoutError:
                print(f"\n‚è∞ Timeout cho c√¢u h·ªèi {idx}. B·ªè qua c√¢u n√†y.")
                skipped_count += 1
                continue
            except Exception as e:
                print(f"\n‚ùå L·ªói: {e}. B·ªè qua c√¢u n√†y.")
                skipped_count += 1
                continue

        if skip_all:
            output = WaitForUserOutput(
                has_responses=has_responses,
                answered_count=answered_count,
                skipped_count=skipped_count,
                skip_all=True,
                user_responses=user_responses,
                status="skipped_all",
                message=f"ƒê√£ b·ªè qua t·∫•t c·∫£ c√¢u h·ªèi. Tr·∫£ l·ªùi: {answered_count}, B·ªè qua: {skipped_count}",
            )
            state.user_skipped = True
            state.status = "skipped_all"
            print("\n" + "=" * 60)
            print("‚äò ƒê√£ b·ªè qua t·∫•t c·∫£ c√¢u h·ªèi. S·∫Ω t·∫°o brief v·ªõi th√¥ng tin hi·ªán c√≥.")
            print("=" * 60 + "\n")
        elif has_responses:
            output = WaitForUserOutput(
                has_responses=True,
                answered_count=answered_count,
                skipped_count=skipped_count,
                skip_all=False,
                user_responses=user_responses,
                status="user_responded",
                message=f"Thu th·∫≠p th√†nh c√¥ng {answered_count} c√¢u tr·∫£ l·ªùi, b·ªè qua {skipped_count} c√¢u",
            )
            state.user_skipped = False
            state.status = "user_responded"
            print("\n" + "=" * 60)
            print("‚úì ƒê√£ ho√†n th√†nh ph·∫ßn h·ªèi ƒë√°p. Ti·∫øp t·ª•c thu th·∫≠p th√¥ng tin...")
            print("=" * 60 + "\n")
        else:
            output = WaitForUserOutput(
                has_responses=False,
                answered_count=0,
                skipped_count=skipped_count,
                skip_all=False,
                user_responses=[],
                status="no_responses",
                message=f"Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi n√†o. B·ªè qua: {skipped_count} c√¢u",
            )
            state.user_skipped = True
            state.status = "no_responses"
            print("\n" + "=" * 60)
            print("‚ö† Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi n√†o. S·∫Ω t·∫°o brief v·ªõi th√¥ng tin hi·ªán c√≥.")
            print("=" * 60 + "\n")

        print("\nüìä Structured Output:")
        print(json.dumps(output.model_dump(), ensure_ascii=False, indent=2))
        print()

        return state

    def generate(self, state: State) -> State:
        """T·∫°o Product Brief ho√†n ch·ªânh t·ª´ th√¥ng tin ƒë√£ thu th·∫≠p, output structured JSON."""
        state.user_choice = ""

        formatted_messages = "\n".join(
            [
                f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content}"
                for msg in state.messages
            ]
        )

        prompt = GENERATE_PROMPT.format(messages=formatted_messages)

        try:
            structured_llm = self._llm("gpt-4.1", 0.3).with_structured_output(
                GenerateOutput
            )
            brief_output = structured_llm.invoke([HumanMessage(content=prompt)])

            state.brief = brief_output.model_dump()

            print("\n" + "=" * 80)
            print("üìÑ PRODUCT BRIEF ƒê√É T·∫†O")
            print("=" * 80)
            print(json.dumps(state.brief, ensure_ascii=False, indent=2))
            print("=" * 80 + "\n")

        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o brief: {e}")
            state.brief = {
                "product_name": "Ch∆∞a x√°c ƒë·ªãnh",
                "description": f"Brief ƒë∆∞·ª£c t·∫°o t·ª´ {len(state.messages)} messages",
                "target_audience": [],
                "key_features": [],
                "benefits": [],
                "competitors": [],
                "completeness_note": f"L·ªói khi generate: {str(e)}",
            }

        return state

    def validate(self, state: State) -> State:
        """Validate brief ƒë√£ t·∫°o b·∫±ng llm_reflect - ki·ªÉm tra completeness v√† calculate confidence_score.

        Theo s∆° ƒë·ªì:
        - Input: brief t·ª´ force_generate
        - S·ª≠ d·ª•ng llm_reflect ƒë·ªÉ ph√¢n t√≠ch brief
        - Output: ValidateOutput structured JSON
        - Branch logic s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü validate_branch
        """
        brief_text = (
            json.dumps(state.brief, ensure_ascii=False, indent=2)
            if state.brief
            else "Ch∆∞a c√≥ brief"
        )

        formatted_messages = "\n".join(
            [
                f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content[:300]}"
                for msg in state.messages[-10:]
            ]
        )

        prompt = VALIDATE_PROMPT.format(brief=brief_text, messages=formatted_messages)

        try:
            structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(
                ValidateOutput
            )
            validation_result = structured_llm.invoke([HumanMessage(content=prompt)])

            state.confidence = validation_result.confidence_score
            state.score = validation_result.completeness_score

            print("\n" + "=" * 60)
            print("‚úì Validation Result:")
            print(f"  - Valid: {validation_result.is_valid}")
            print(f"  - Confidence: {state.confidence:.2f}")
            print(f"  - Completeness: {state.score:.2f}")
            print(f"  - Message: {validation_result.validation_message}")

            if validation_result.missing_fields:
                print(f"  - Missing: {', '.join(validation_result.missing_fields)}")

            print("=" * 60 + "\n")

            print("\nüìä Structured Output t·ª´ validate:")
            print(
                json.dumps(validation_result.model_dump(), ensure_ascii=False, indent=2)
            )
            print()

        except Exception as e:
            print(f"‚ùå Error in validate: {e}")
            state.confidence = 0.3
            state.score = 0.3

        return state

    def retry_decision(self, state: State) -> State:
        """TƒÉng retry_count ƒë·ªÉ theo d√µi s·ªë l·∫ßn retry validate.

        Theo s∆° ƒë·ªì:
        - Input: brief kh√¥ng h·ª£p l·ªá ho·∫∑c confidence ‚â§ 0.7
        - TƒÉng retry_count
        - Branch logic s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü retry_decision_branch
        """
        state.retry_count += 1

        print("\n" + "=" * 60)
        print(f"üîÑ RETRY DECISION - L·∫ßn th·ª≠ {state.retry_count}")
        print(f"  - Confidence: {state.confidence:.2f}")
        print(f"  - Completeness: {state.score:.2f}")
        print("=" * 60 + "\n")

        return state

    def preview(self, state: State) -> State:
        """Hi·ªÉn th·ªã brief cho user v√† h·ªèi: Approve/Edit/Regenerate.

        Theo s∆° ƒë·ªì:
        - Format brief v√† show to user
        - H·ªèi user l·ª±a ch·ªçn: Approve/Edit/Regenerate (v·ªõi timeout option)
        - C·∫≠p nh·∫≠t user_choice v√†o state
        - N·∫øu ch·ªçn Edit, thu th·∫≠p edit_changes
        """
        print("\n" + "=" * 80)
        print("üìã PREVIEW - XEM TR∆Ø·ªöC PRODUCT BRIEF")
        print("=" * 80)
        print(f"üö© C·ªù ch∆∞a ho√†n ch·ªânh: {state.incomplete_flag}")
        print("\nN·ªôi dung Brief:")
        print("-" * 80)
        print(json.dumps(state.brief, ensure_ascii=False, indent=2))
        print("=" * 80 + "\n")

        print("üí° B·∫°n c√≥ th·ªÉ:")
        print("  1. G√µ 'approve' ƒë·ªÉ ph√™ duy·ªát brief")
        print("  2. G√µ 'edit' ƒë·ªÉ ch·ªânh s·ª≠a brief")
        print("  3. G√µ 'regenerate' ƒë·ªÉ t·∫°o l·∫°i brief")
        print()

        try:
            user_choice = (
                input("üë§ L·ª±a ch·ªçn c·ªßa b·∫°n (approve/edit/regenerate): ").strip().lower()
            )

            if user_choice not in ["approve", "edit", "regenerate"]:
                print(
                    f"‚ö† L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá: '{user_choice}'. M·∫∑c ƒë·ªãnh ch·ªçn 'approve'."
                )
                user_choice = "approve"

            state.user_choice = user_choice

            if user_choice == "edit":
                print("\nüìù Nh·∫≠p c√°c thay ƒë·ªïi b·∫°n mu·ªën √°p d·ª•ng v√†o brief:")
                edit_changes = input("üë§ Thay ƒë·ªïi: ").strip()
                state.edit_changes = edit_changes
                print(f"‚úì ƒê√£ ghi nh·∫≠n thay ƒë·ªïi: {edit_changes[:100]}...")
            elif user_choice == "approve":
                print("‚úì B·∫°n ƒë√£ ph√™ duy·ªát brief.")
            elif user_choice == "regenerate":
                print("üîÑ S·∫Ω t·∫°o l·∫°i brief.")
                state.retry_count = 0

        except Exception as e:
            print(f"‚ùå L·ªói khi nh·∫≠n input: {e}. M·∫∑c ƒë·ªãnh ch·ªçn 'approve'.")
            state.user_choice = "approve"

        return state

    def edit_mode(self, state: State) -> State:
        """√Åp d·ª•ng c√°c thay ƒë·ªïi t·ª´ user v√†o brief v√† re-validate.

        Theo s∆° ƒë·ªì:
        - Input: edit_changes t·ª´ preview node
        - Apply user changes v√†o brief
        - Re-validate ƒë·ªÉ ƒë·∫£m b·∫£o brief v·∫´n h·ª£p l·ªá
        """
        state.user_choice = ""

        brief_text = json.dumps(state.brief, ensure_ascii=False, indent=2)

        prompt = EDIT_MODE_PROMPT.format(
            brief=brief_text, edit_changes=state.edit_changes
        )

        try:
            structured_llm = self._llm("gpt-4.1", 0.3).with_structured_output(
                EditModeOutput
            )
            edited_brief = structured_llm.invoke([HumanMessage(content=prompt)])

            state.brief = edited_brief.model_dump()

            state.edit_changes = ""

            print("\n" + "=" * 60)
            print("‚úèÔ∏è ƒê√É √ÅP D·ª§NG THAY ƒê·ªîI V√ÄO BRIEF")
            print("=" * 60)
            print(json.dumps(state.brief, ensure_ascii=False, indent=2))
            print("=" * 60 + "\n")

            print("\nüìä Structured Output t·ª´ edit_mode:")
            print(json.dumps(edited_brief.model_dump(), ensure_ascii=False, indent=2))
            print()

        except Exception as e:
            print(f"‚ùå L·ªói khi √°p d·ª•ng thay ƒë·ªïi: {e}")
            state.edit_changes = ""

        return state

    def finalize(self, state: State) -> State:
        """L∆∞u brief v√† t·∫°o summary cu·ªëi c√πng v·ªõi structured output.

        Theo s∆° ƒë·ªì:
        - Input: brief ƒë√£ ƒë∆∞·ª£c approve
        - Generate summary t·ª´ brief v·ªõi structured JSON output
        - C·∫≠p nh·∫≠t status = "completed"
        - Output: FinalizeOutput structured JSON
        """
        brief_text = json.dumps(state.brief, ensure_ascii=False, indent=2)

        prompt = FINALIZE_PROMPT.format(brief=brief_text)

        try:
            structured_llm = self._llm("gpt-4.1", 0.3).with_structured_output(
                FinalizeOutput
            )
            finalize_result = structured_llm.invoke([HumanMessage(content=prompt)])

            state.status = "completed"

            print("\n" + "=" * 80)
            print("‚úÖ HO√ÄN T·∫§T - PRODUCT BRIEF ƒê√É ƒê∆Ø·ª¢C PH√ä DUY·ªÜT")
            print("=" * 80)
            print("\nüìä T√ìM T·∫ÆT CU·ªêI C√ôNG:\n")
            print(finalize_result.summary_markdown)
            print("\n" + "=" * 80)
            print(f"üìà Th·ªëng k√™:")
            print(f"  - S·ªë l·∫ßn l·∫∑p: {state.iteration_count}/{state.max_iterations}")
            print(f"  - S·ªë l·∫ßn retry: {state.retry_count}")
            print(f"  - Confidence score: {state.confidence:.2f}")
            print(f"  - Completeness score: {state.score:.2f}")
            print(f"  - T·ªïng s·ªë messages: {len(state.messages)}")
            print("=" * 80 + "\n")

            print("\nüìÑ Structured Output t·ª´ finalize:")
            print(
                json.dumps(finalize_result.model_dump(), ensure_ascii=False, indent=2)
            )
            print()

        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o t√≥m t·∫Øt: {e}")
            state.status = "completed_with_errors"
            print("\n" + "=" * 80)
            print("‚ö† HO√ÄN T·∫§T V·ªöI L·ªñI")
            print("=" * 80)
            print(f"Brief ƒë√£ ƒë∆∞·ª£c l∆∞u nh∆∞ng kh√¥ng th·ªÉ t·∫°o t√≥m t·∫Øt: {str(e)}")
            print("=" * 80 + "\n")

        return state

    # Conditional branches
    def evaluate_branch(self, state: State) -> str:
        """Quy·∫øt ƒë·ªãnh lu·ªìng sau evaluate node theo s∆° ƒë·ªì.

        Theo s∆° ƒë·ªì:
        - N·∫øu iteration_count >= max_iterations ‚Üí force_generate
        - N·∫øu status == done OR score >= 0.8 ‚Üí generate
        - N·∫øu confidence <= 0.6 ‚Üí clarify (low confidence)
        - N·∫øu c√≥ gaps AND confidence > 0.6 ‚Üí suggest
        """
        if state.iteration_count >= state.max_iterations:
            return "force_generate"
        elif state.status == "done" or state.score >= 0.8:
            return "generate"
        elif state.confidence <= 0.6:
            return "clarify"
        elif len(state.gaps) > 0 and state.confidence > 0.6:
            return "suggest"
        else:
            return "generate"

    def wait_for_user_branch(self, state: State) -> str:
        """Quy·∫øt ƒë·ªãnh next node sau wait_for_user."""
        if state.user_skipped or state.status in [
            "skipped_all",
            "no_responses",
            "error_generating_questions",
        ]:
            return "generate"
        elif state.status == "user_responded":
            return "evaluate"
        else:
            return "generate"

    def validate_branch(self, state: State) -> str:
        """Quy·∫øt ƒë·ªãnh lu·ªìng sau validate node theo s∆° ƒë·ªì.

        Theo s∆° ƒë·ªì:
        - N·∫øu valid AND confidence > 0.7 ‚Üí preview
        - N·∫øu invalid OR confidence ‚â§ 0.7 ‚Üí retry_decision
        """
        if state.confidence > 0.7:
            return "preview"
        else:
            return "retry_decision"

    def retry_decision_branch(self, state: State) -> str:
        """Quy·∫øt ƒë·ªãnh lu·ªìng sau retry_decision node theo s∆° ƒë·ªì.

        Theo s∆° ƒë·ªì:
        - N·∫øu retry_count >= 2 ‚Üí preview (d√π confidence th·∫•p, v·∫´n cho user xem)
        - N·∫øu retry_count < 2 ‚Üí generate (regenerate l·∫°i)
        """
        if state.retry_count >= 2:
            print(
                f"\n‚ö†Ô∏è  ƒê√£ retry {state.retry_count} l·∫ßn, chuy·ªÉn sang preview ƒë·ªÉ user quy·∫øt ƒë·ªãnh.\n"
            )
            return "preview"
        else:
            print(f"\nüîÑ Retry l·∫ßn {state.retry_count + 1}, regenerate brief...\n")
            return "suggest"

    def preview_branch(self, state: State) -> str:
        """Quy·∫øt ƒë·ªãnh lu·ªìng sau preview node theo s∆° ƒë·ªì.

        Theo s∆° ƒë·ªì:
        - N·∫øu user ch·ªçn "approve" ‚Üí finalize
        - N·∫øu user ch·ªçn "edit" ‚Üí edit_mode
        - N·∫øu user ch·ªçn "regenerate" ‚Üí generate
        """
        if state.user_choice == "approve":
            return "finalize"
        elif state.user_choice == "edit":
            return "edit_mode"
        elif state.user_choice == "regenerate":
            return "generate"
        else:
            return "finalize"

    def run(
        self, initial_context: str = "", thread_id: str | None = None
    ) -> dict[str, Any]:
        """Ch·∫°y quy tr√¨nh l√†m vi·ªác c·ªßa gatherer agent.

        Args:
            initial_context: Ng·ªØ c·∫£nh ban ƒë·∫ßu ho·∫∑c y√™u c·∫ßu cho b·∫£n t√≥m t·∫Øt s·∫£n ph·∫©m
            thread_id: ID ƒë·ªÉ resume state (n·∫øu None, d√πng session_id ho·∫∑c default)

        Returns:
            dict: Tr·∫°ng th√°i cu·ªëi c√πng ch·ª©a b·∫£n t√≥m t·∫Øt ƒë√£ t·∫°o v√† c√°c ch·ªâ s·ªë ƒë√°nh gi√°
        """
        if thread_id is None:
            thread_id = self.session_id or "default_thread"

        initial_state = State(
            messages=[HumanMessage(content=initial_context)] if initial_context else []
        )

        metadata = {}
        if self.session_id:
            metadata["langfuse_session_id"] = self.session_id
        if self.user_id:
            metadata["langfuse_user_id"] = self.user_id
        metadata["langfuse_tags"] = ["gatherer_agent"]

        config = {
            "configurable": {"thread_id": thread_id},
            "callbacks": [self.langfuse_handler],
            "metadata": metadata,
            "recursion_limit": 50,
        }

        final_state = None
        for output in self.graph.stream(
            initial_state.model_dump() if initial_state else None,
            config=config,
        ):
            final_state = output

        return final_state or {}
