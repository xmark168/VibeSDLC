"""Lá»›p nÃ y chá»©a LangGraph Agent/workflow vÃ  cÃ¡c tÆ°Æ¡ng tÃ¡c vá»›i LLM cho gatherer agent."""

import json
import os
import re
from typing import Any, Literal

from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, HumanMessage,AIMessage
from langchain_openai import ChatOpenAI
from langfuse.langchain import CallbackHandler
from langgraph.graph import END, START, StateGraph
from pydantic import BaseModel, Field

from templates.prompts.product_owner.gatherer import EVALUATE_PROMPT, CLARIFY_PROMPT, SUGGEST_PROMPT, ASK_USER_PROMPT
from langgraph.checkpoint.memory import MemorySaver

load_dotenv()


class EvaluateOutput(BaseModel):
    gaps: list[str] = Field(description="Danh sÃ¡ch cÃ¡c thÃ´ng tin cÃ²n thiáº¿u")
    score: float = Field(description="Äiá»ƒm Ä‘Ã¡nh giÃ¡ Ä‘á»™ Ä‘áº§y Ä‘á»§", ge=0.0, le=1.0)
    status: str = Field(description="Tráº¡ng thÃ¡i: incomplete hoáº·c done")
    confidence: float = Field(description="Äá»™ tin cáº­y Ä‘Ã¡nh giÃ¡", ge=0.0, le=1.0)
    message: str = Field(description="LÃ½ do")


class EvaluateMessageOutput(BaseModel):
    is_unclear: bool = Field(description="True náº¿u message mÆ¡ há»“/khÃ´ng rÃµ rÃ ng, False náº¿u rÃµ rÃ ng")
    reason: str = Field(description="LÃ½ do Ä‘Ã¡nh giÃ¡ message lÃ  unclear hoáº·c clear")


class ClarifyOutput(BaseModel):
    summary: str = Field(description="TÃ³m táº¯t nhá»¯ng gÃ¬ Ä‘Ã£ hiá»ƒu tá»« cuá»™c há»™i thoáº¡i")
    unclear_points: list[str] = Field(description="Danh sÃ¡ch cÃ¡c Ä‘iá»ƒm cÃ²n mÆ¡ há»“ hoáº·c cáº§n lÃ m rÃµ")
    clarified_gaps: list[str] = Field(description="Danh sÃ¡ch gaps Ä‘Ã£ Ä‘Æ°á»£c lÃ m rÃµ vÃ  cáº§n Æ°u tiÃªn thu tháº­p")
    message_to_user: str = Field(description="ThÃ´ng Ä‘iá»‡p gá»­i Ä‘áº¿n user Ä‘á»ƒ xÃ¡c nháº­n hiá»ƒu biáº¿t vÃ  yÃªu cáº§u lÃ m rÃµ")


class FilledGap(BaseModel):
    gap_name: str = Field(description="TÃªn cá»§a gap")
    suggested_value: str = Field(description="GiÃ¡ trá»‹ gá»£i Ã½ Ä‘á»ƒ fill gap")
    reason: str = Field(description="LÃ½ do ngáº¯n gá»n táº¡i sao gá»£i Ã½ giÃ¡ trá»‹ nÃ y")


class SuggestOutput(BaseModel):
    prioritized_gaps: list[str] = Field(description="Danh sÃ¡ch gaps cÃ²n láº¡i chÆ°a fill Ä‘Æ°á»£c, sáº¯p xáº¿p theo Ä‘á»™ Æ°u tiÃªn")
    filled_gaps: list[FilledGap] = Field(description="Danh sÃ¡ch cÃ¡c gaps Ä‘Ã£ gá»£i Ã½ fill vá»›i giÃ¡ trá»‹ vÃ  lÃ½ do")


class AskUserOutput(BaseModel):
    questions: list[str] = Field(description="Danh sÃ¡ch tá»‘i Ä‘a 3 cÃ¢u há»i Ä‘á»ƒ thu tháº­p thÃ´ng tin cho cÃ¡c gaps")


class State(BaseModel):
    """Tráº¡ng thÃ¡i cho quy trÃ¬nh lÃ m viá»‡c cá»§a gatherer agent."""

    messages: list[BaseMessage] = Field(default_factory=list)
    iteration_count: int = 0
    max_iterations: int = 5
    retry_count: int = 0
    gaps: list[str] = Field(default_factory=list)
    score: float = 0.0
    status: str = "initial"
    confidence: float = 0.0
    message: str = ""
    brief: dict =  Field(default_factory=dict)
    incomplete_flag: bool = False
    questions: list[str] = Field(default_factory=list)
    unclear_input: list[str] = Field(default_factory=list)
    user_choice: Literal["approve", "edit", "regenerate", "skip", ""] = ""
    edit_changes: str = ""
    user_skipped: bool = False


class GathererAgent:
    """Gatherer Agent Ä‘á»ƒ thu tháº­p thÃ´ng tin sáº£n pháº©m giÃºp táº¡o backlog trong tÆ°Æ¡ng lai."""

    def __init__(self, session_id: str | None = None, user_id: str | None = None):
        """Khá»Ÿi táº¡o gatherer agent.

        Args:
            session_id: Session ID tÃ¹y chá»n Ä‘á»ƒ theo dÃµi
            user_id: User ID tÃ¹y chá»n Ä‘á»ƒ theo dÃµi
        """
        self.session_id = session_id
        self.user_id = user_id

        # Initialize Langfuse callback handler
        self.langfuse_handler = CallbackHandler()

        self.graph = self._build_graph()

    def _llm(self, model: str, temperature: str) -> ChatOpenAI:
        try:
            llm = ChatOpenAI(
                model= model,
                temperature= temperature,
                api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_BASE_URL"),
            )
            return llm
        except Exception:
            # Avoid noisy context warnings when no active span
            return ""

    def _build_graph(self) -> StateGraph:
        """XÃ¢y dá»±ng quy trÃ¬nh lÃ m viá»‡c LangGraph."""
        graph_builder = StateGraph(State)

        # Add nodes
        graph_builder.add_node("initialize", self._initialize)
        graph_builder.add_node("collect_inputs", self.collect_inputs)
        graph_builder.add_node("evaluate", self.evaluate)
        # graph_builder.add_node("force_generate", self.force_generate)
        # graph_builder.add_node("validate", self.validate)    
        # graph_builder.add_node("retry_decision", self.retry_decision)
        graph_builder.add_node("clarify", self.clarify)
        graph_builder.add_node("suggest", self.suggest)
        graph_builder.add_node("ask_user", self.ask_user)
        graph_builder.add_node("increment_iteration", self.increment_iteration)
        graph_builder.add_node("wait_for_user", self.wait_for_user)
        # graph_builder.add_node("preview", self.preview)
        # graph_builder.add_node("edit_mode", self.edit_mode)
        # graph_builder.add_node("finalize", self.finalize)
        # graph_builder.add_node("generate", self.generate)
        # # Add edges
        graph_builder.add_edge(START, "initialize")
        # graph_builder.add_edge("initialize", "collect_inputs")
        # graph_builder.add_edge("initialize", END)
        graph_builder.add_edge("collect_inputs", "evaluate")
        graph_builder.add_conditional_edges("initialize", self.initialize_branch)
        graph_builder.add_conditional_edges("evaluate", self.evaluate_branch)
        graph_builder.add_edge("clarify", "suggest")
        graph_builder.add_edge("suggest", "ask_user")
        graph_builder.add_edge("ask_user", "increment_iteration")
        graph_builder.add_edge("increment_iteration", "wait_for_user")
        graph_builder.add_conditional_edges("wait_for_user", self.wait_for_user_branch)
        checkpointer = MemorySaver()  # Khá»Ÿi táº¡o MemorySaver
        return graph_builder.compile(
            checkpointer=checkpointer,
            interrupt_before=["collect_inputs"]  # Pause trÆ°á»›c node collect_inputs
        )

    def _initialize(self, state: State) -> State:
        print(state)
        """Khá»Ÿi táº¡o tráº¡ng thÃ¡i."""
        return state
    def collect_inputs(self, state: State) -> State:
        """Thu tháº­p thÃ´ng tin bá»• sung tá»« ngÆ°á»i dÃ¹ng Ä‘á»ƒ Ä‘iá»n vÃ o cÃ¡c khoáº£ng trá»‘ng thÃ´ng tin."""
        

        return state
    
    def evaluate_message(self, message: str) -> bool:
        """ÄÃ¡nh giÃ¡ xem message cuá»‘i cÃ¹ng cá»§a ngÆ°á»i dÃ¹ng cÃ³ unclear hay khÃ´ng báº±ng regex patterns."""
        message_lower = message.lower().strip()

        # Patterns cho unclear messages (Vietnamese best practices)
        unclear_patterns = [
            # Äáº¡i tá»« mÆ¡ há»“
            r'\b(nÃ³|cÃ¡i Ä‘Ã³|cÃ¡i nÃ y|thá»© Ä‘Ã³|thá»© nÃ y|chá»— Ä‘Ã³|chá»— nÃ y|cÃ¡i kia|tháº±ng Ä‘Ã³)\b',
            # Tham chiáº¿u mÆ¡ há»“
            r'\b(nhÆ° trÃªn|nhÆ° váº­y|nhÆ° tháº¿|y nhÆ°|y chang|tÆ°Æ¡ng tá»±|giá»‘ng váº­y|nhÆ° kia)\b',
            # Thiáº¿u thÃ´ng tin cá»¥ thá»ƒ (cÃ¢u quÃ¡ ngáº¯n < 10 kÃ½ tá»±)
            r'^.{1,10}$',
            # CÃ¢u há»i mÆ¡ há»“ khÃ´ng cÃ³ ngá»¯ cáº£nh
            r'^\s*(sao|tháº¿ nÃ o|nhÆ° nÃ o|ra sao|gÃ¬|Ã |háº£|á»«|uh|uhm)\s*\??\s*$',
            # Chá»‰ cÃ³ yes/no khÃ´ng cÃ³ ngá»¯ cáº£nh
            r'^\s*(cÃ³|khÃ´ng|ok|Ä‘Æ°á»£c|rá»“i|á»«|uh|yes|no|yeah|nope)\s*$',
        ]

        # Check náº¿u match báº¥t ká»³ pattern nÃ o
        for pattern in unclear_patterns:
            if re.search(pattern, message_lower):
                return True 

        # Clear náº¿u khÃ´ng match pattern nÃ o
        return False

    def evaluate(self, state: State) -> State:
        """ÄÃ¡nh giÃ¡ Ä‘á»™ Ä‘áº§y Ä‘á»§ cá»§a cuá»™c há»™i thoáº¡i Ä‘á»ƒ táº¡o báº£n tÃ³m táº¯t sá»­ dá»¥ng structured output."""
        # Evaluate last message if it exists and is from user
        if state.messages:
            last_message = state.messages[-1]
            if last_message.type == "human":
                is_unclear = self.evaluate_message(last_message.content)
                if is_unclear:
                    # Add unclear message to unclear_input list
                    state.unclear_input.append(last_message.content)

        # Format messages
        formatted_messages = "\n".join([
            f"{i}. [{'User' if msg.type=='human' else 'Assistant'}]: "
            f"{msg.content if hasattr(msg, 'content') else str(msg)}"
            for i, msg in enumerate(state.messages, 1)
        ]) if state.messages else "ChÆ°a cÃ³ thÃ´ng tin nÃ o Ä‘Æ°á»£c thu tháº­p."

        prompt = EVALUATE_PROMPT.format(messages=formatted_messages)

        # Use structured output with Pydantic model
        structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(EvaluateOutput)
        evaluation = structured_llm.invoke([HumanMessage(content=prompt)])

        # Update state
        state.gaps = evaluation.gaps
        state.score = evaluation.score
        state.status = evaluation.status
        state.confidence = evaluation.confidence
        state.message = evaluation.message
        return state

    
    # def force_generate(self, state: State) -> State:
    #     prompt = f"Táº¡o má»™t brief sá»­ dá»¥ng thÃ´ng tin cÃ³ sáºµn tá»« cuá»™c trÃ² chuyá»‡n, ngay cáº£ khi chÆ°a hoÃ n chá»‰nh:\n{state.messages}"
    #     response = self.llm.invoke(prompt)
    #     state.brief = response.content
    #     state.incomplete_flag = True
    #     return state

    # def validate(self, state: State) -> State:
    #     prompt = f"""XÃ¡c thá»±c brief Ä‘Ã£ táº¡o vá» tÃ­nh hoÃ n chá»‰nh vÃ  chÃ­nh xÃ¡c so vá»›i cuá»™c trÃ² chuyá»‡n:
    #     Brief: {state.brief}
    #     Cuá»™c trÃ² chuyá»‡n: {state.messages}
    #     Output dÆ°á»›i dáº¡ng JSON vá»›i valid (bool), confidence, score."""
    #     response = self.llm.invoke(prompt)
    #     try:
    #         parsed = json.loads(response.content)
    #         state.confidence = parsed.get("confidence", 0.0)
    #         state.score = parsed.get("score", 0.0)
    #     except:
    #         pass
    #     return state

    # def retry_decision(self, state: State) -> State:
    #     state.retry_count += 1
    #     return state

    def clarify(self, state: State) -> State:
        """LÃ m rÃµ cÃ¡c thÃ´ng tin mÆ¡ há»“ hoáº·c khÃ´ng rÃµ rÃ ng trong cuá»™c há»™i thoáº¡i."""
        # Format messages
        formatted_messages = "\n".join([
            f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content}"
            for msg in state.messages
        ])

        # Format unclear inputs
        unclear_inputs = "\n".join([f"- {unclear}" for unclear in state.unclear_input]) if state.unclear_input else "KhÃ´ng cÃ³"

        # Format gaps
        formatted_gaps = "\n".join([f"- {gap}" for gap in state.gaps]) if state.gaps else "KhÃ´ng cÃ³"

        prompt = CLARIFY_PROMPT.format(
            messages=formatted_messages,
            unclear_inputs=unclear_inputs,
            gaps=formatted_gaps
        )

        # Use structured output with Pydantic model
        structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(ClarifyOutput)
        clarify_result = structured_llm.invoke([HumanMessage(content=prompt)])

        # Update state with clarified gaps for suggest node
        state.gaps = clarify_result.clarified_gaps

        # Append message to user
        state.messages.append(AIMessage(content=clarify_result.message_to_user))

        return state

    def suggest(self, state: State) -> State:
        """Gá»£i Ã½ ná»™i dung Ä‘á»ƒ tá»± Ä‘á»™ng fill cÃ¡c gaps quan trá»ng, giÃºp thu tháº­p thÃ´ng tin nhanh hÆ¡n mÃ  khÃ´ng báº¯t user nghÄ© táº¥t cáº£."""
        # Format gaps
        formatted_gaps = "\n".join([f"- {gap}" for gap in state.gaps]) if state.gaps else "KhÃ´ng cÃ³ gaps"

        # Format messages - limit length to avoid overload
        formatted_messages = "\n".join([
            f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content[:500]}"  # Limit each message to 500 chars
            for msg in state.messages[-10:]  # Only take last 10 messages
        ])

        prompt = SUGGEST_PROMPT.format(
            gaps=formatted_gaps,
            messages=formatted_messages
        )

        try:
            # Use structured output with Pydantic model
            structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(SuggestOutput)
            suggest_result = structured_llm.invoke([HumanMessage(content=prompt)])

            # Update state with prioritized gaps
            state.gaps = suggest_result.prioritized_gaps

            # Store filled gaps if any
            if suggest_result.filled_gaps:
                filled_msg = "CÃ¡c thÃ´ng tin Ä‘Æ°á»£c gá»£i Ã½ tá»± Ä‘á»™ng fill dá»±a trÃªn ngá»¯ cáº£nh:\n\n" + "\n\n".join(
                    [f"**{fg.gap_name}**\nâ€¢ GiÃ¡ trá»‹: {fg.suggested_value}\nâ€¢ LÃ½ do: {fg.reason}"
                     for fg in suggest_result.filled_gaps]
                ) + "\n\nNáº¿u khÃ´ng chÃ­nh xÃ¡c, vui lÃ²ng chá»‰nh sá»­a."
                state.messages.append(AIMessage(content=filled_msg))
        except Exception as e:
            print(f"Error in suggest: {e}")
            # Fallback: keep gaps as is if error occurs
            pass

        return state
    
    def ask_user(self, state: State) -> State:
        """Táº¡o cÃ¢u há»i Ä‘á»ƒ thu tháº­p thÃ´ng tin cho cÃ¡c gaps cÃ²n thiáº¿u."""
        # Format gaps
        formatted_gaps = "\n".join([f"- {gap}" for gap in state.gaps]) if state.gaps else "KhÃ´ng cÃ³ gaps"

        # Format messages - limit to last 10 messages
        formatted_messages = "\n".join([
            f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content[:500]}"
            for msg in state.messages[-10:]
        ])

        prompt = ASK_USER_PROMPT.format(
            gaps=formatted_gaps,
            messages=formatted_messages
        )

        try:
            # Use structured output with Pydantic model
            structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(AskUserOutput)
            ask_result = structured_llm.invoke([HumanMessage(content=prompt)])
            state.questions = ask_result.questions
            state.status = "awaiting_user"  # Set status Ä‘á»ƒ chá» user á»Ÿ wait_for_user node
        except Exception as e:
            print(f"Error in ask_user: {e}")
            state.questions = []
            state.status = "error_generating_questions"

        # KhÃ´ng há»i user á»Ÿ Ä‘Ã¢y ná»¯a, chá»‰ generate questions
        print(f"\nğŸ“ ÄÃ£ táº¡o {len(state.questions)} cÃ¢u há»i Ä‘á»ƒ thu tháº­p thÃ´ng tin.")

        return state

    def increment_iteration(self, state: State) -> State:
        """TÄƒng iteration count vÃ  checkpoint state Ä‘á»ƒ cÃ³ thá»ƒ resume sau nÃ y."""
        state.iteration_count += 1
        print(f"\n=== Iteration {state.iteration_count}/{state.max_iterations} completed ===")
        print(f"Current gaps: {len(state.gaps)}")
        print(f"Score: {state.score}, Confidence: {state.confidence}, Status: {state.status}")

        # Checkpoint is automatically saved by LangGraph MemorySaver after each node execution
        return state

    def wait_for_user(self, state: State) -> State:
        """Há»i user tá»«ng cÃ¢u má»™t vÃ  Ä‘á»£i response vá»›i timeout 10 phÃºt cho má»—i cÃ¢u."""
        import signal

        print("\n" + "="*60)
        print("ğŸ’¬ PHáº¦N Há»I ÄÃP - Thu tháº­p thÃ´ng tin")
        print("="*60)
        print("ğŸ’¡ Báº¡n cÃ³ thá»ƒ:")
        print("  - Tráº£ lá»i tá»«ng cÃ¢u há»i")
        print("  - GÃµ 'skip' Ä‘á»ƒ bá» qua cÃ¢u hiá»‡n táº¡i")
        print("  - GÃµ 'skip_all' Ä‘á»ƒ bá» qua táº¥t cáº£ vÃ  táº¡o brief vá»›i thÃ´ng tin hiá»‡n cÃ³")
        print("  - Timeout: 10 phÃºt cho má»—i cÃ¢u há»i")
        print("="*60 + "\n")

        def timeout_handler(signum, frame):
            raise TimeoutError("Timeout")

        has_responses = False
        skip_all = False

        for idx, question in enumerate(state.questions, 1):
            if skip_all:
                break

            # Append question to messages
            state.messages.append(AIMessage(content=question))

            print(f"\n[CÃ¢u há»i {idx}/{len(state.questions)}]")
            print(f"â“ {question}\n")

            try:
                # Set timeout 10 minutes for each question (Unix only)
                if hasattr(signal, 'SIGALRM'):
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(600)  # 10 minutes

                user_input = input("ğŸ‘¤ CÃ¢u tráº£ lá»i cá»§a báº¡n: ").strip()

                if hasattr(signal, 'SIGALRM'):
                    signal.alarm(0)  # Cancel alarm

                # Check user input
                if user_input.lower() == 'skip_all':
                    skip_all = True
                    print("\nâŠ˜ Báº¡n Ä‘Ã£ chá»n bá» qua táº¥t cáº£ cÃ¢u há»i cÃ²n láº¡i.")
                    break
                elif user_input.lower() == 'skip':
                    print("âŠ˜ Bá» qua cÃ¢u nÃ y.\n")
                    continue
                elif user_input:
                    state.messages.append(HumanMessage(content=user_input))
                    has_responses = True
                    print("âœ“ ÄÃ£ ghi nháº­n cÃ¢u tráº£ lá»i.\n")
                else:
                    print("âš  CÃ¢u tráº£ lá»i trá»‘ng, bá» qua.\n")

            except TimeoutError:
                print(f"\nâ° Timeout cho cÃ¢u há»i {idx}. Bá» qua cÃ¢u nÃ y.")
                continue
            except Exception as e:
                print(f"\nâŒ Lá»—i: {e}. Bá» qua cÃ¢u nÃ y.")
                continue

        # Update state based on results
        if skip_all:
            state.user_skipped = True
            state.status = "skipped_all"
            print("\n" + "="*60)
            print("âŠ˜ ÄÃ£ bá» qua táº¥t cáº£ cÃ¢u há»i. Sáº½ táº¡o brief vá»›i thÃ´ng tin hiá»‡n cÃ³.")
            print("="*60 + "\n")
        elif has_responses:
            state.user_skipped = False
            state.status = "user_responded"
            print("\n" + "="*60)
            print("âœ“ ÄÃ£ hoÃ n thÃ nh pháº§n há»i Ä‘Ã¡p. Tiáº¿p tá»¥c thu tháº­p thÃ´ng tin...")
            print("="*60 + "\n")
        else:
            state.user_skipped = True
            state.status = "no_responses"
            print("\n" + "="*60)
            print("âš  KhÃ´ng cÃ³ cÃ¢u tráº£ lá»i nÃ o. Sáº½ táº¡o brief vá»›i thÃ´ng tin hiá»‡n cÃ³.")
            print("="*60 + "\n")

        return state

    # def preview(self, state: State) -> State:
    #     print(f"Brief ÄÃ£ Táº¡o (Cá» chÆ°a hoÃ n chá»‰nh: {state.incomplete_flag}):\n{state.brief}")
    #     user_choice = input("PhÃª duyá»‡t/Chá»‰nh sá»­a/Táº¡o láº¡i? ").lower()
    #     state.user_choice = user_choice
    #     if user_choice == "edit":
    #         edit_changes = input("Nháº­p chá»‰nh sá»­a cá»§a báº¡n: ")
    #         state.edit_changes = edit_changes
    #     return state

    # def edit_mode(self, state: State) -> State:
    #     prompt = f"Ãp dá»¥ng cÃ¡c thay Ä‘á»•i sau vÃ o brief:\nThay Ä‘á»•i: {state.edit_changes}\nBrief Gá»‘c: {state.brief}"
    #     response = self.llm.invoke(prompt)
    #     state.brief = response.content
    #     state.edit_changes = ""
    #     return state

    # def finalize(self, state: State) -> State:
    #     prompt = f"Táº¡o tÃ³m táº¯t cuá»‘i cÃ¹ng tá»« brief Ä‘Ã£ phÃª duyá»‡t:\n{state.brief}"
    #     response = self.llm.invoke(prompt)
    #     state.status = "completed"
    #     print(f"TÃ³m Táº¯t Cuá»‘i CÃ¹ng:\n{response.content}")
    #     return state

    # def generate(self, state: State) -> State:
    #     prompt = f"Táº¡o báº£n nhÃ¡p brief tá»« cuá»™c trÃ² chuyá»‡n dÃ¹ cÃ³ bá» qua:\n{state.messages}"
    #     response = self.llm.invoke(prompt)
    #     state.brief = response.content
    #     return state

    # # Conditional branches
    # def evaluate_branch(self, state: State) -> str:
    #     if len(state.gaps) > 0 and state.confidence > 0.6:
    #         return "clarify"
    #     elif state.iteration_count < state.max_iterations:
    #         return "force_generate"
    #     else:
    #         return END
        
    def initialize_branch(self, state: State) -> str:
        if len(state.gaps) == 0 and len(state.messages) == 1:
            return "evaluate"
        else:
            return "collect_inputs"

    def evaluate_branch(self, state: State) -> str:
        if state.confidence <= 0.6:
            return "clarify"
        elif len(state.gaps) > 0 and state.confidence > 0.6 and state.confidence < 0.8:
            return "force_generate"
        else:
            return END

    def wait_for_user_branch(self, state: State) -> str:
        """Quyáº¿t Ä‘á»‹nh next node sau wait_for_user."""
        if state.user_skipped or state.status in ["skipped_all", "no_responses", "error_generating_questions"]:
            # User skipped/no responses â†’ generate vá»›i thÃ´ng tin hiá»‡n cÃ³
            return "generate"
        elif state.status == "user_responded":
            # User responded â†’ quay láº¡i collect_inputs Ä‘á»ƒ evaluate láº¡i
            return "collect_inputs"
        else:
            # Default: generate
            return "generate"

    def run(self, initial_context: str = "", thread_id: str | None = None) -> dict[str, Any]:
        """Cháº¡y quy trÃ¬nh lÃ m viá»‡c cá»§a gatherer agent.

        Args:
            initial_context: Ngá»¯ cáº£nh ban Ä‘áº§u hoáº·c yÃªu cáº§u cho báº£n tÃ³m táº¯t sáº£n pháº©m
            thread_id: ID Ä‘á»ƒ resume state (náº¿u None, dÃ¹ng session_id hoáº·c default)

        Returns:
            dict: Tráº¡ng thÃ¡i cuá»‘i cÃ¹ng chá»©a báº£n tÃ³m táº¯t Ä‘Ã£ táº¡o vÃ  cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡
        """
        if thread_id is None:
            thread_id = self.session_id or "default_thread"  # Default náº¿u khÃ´ng cÃ³

        initial_state = State(
            messages=[HumanMessage(content=initial_context)] if initial_context else []
        )

        config = {
            "configurable": {"thread_id": thread_id},  # Äá»ƒ checkpointer lÆ°u theo thread
            "callbacks": [self.langfuse_handler]
        }

        final_state = None
        for output in self.graph.stream(
            initial_state.model_dump() if initial_state else None,  # Náº¿u resume, pass None Ä‘á»ƒ load tá»« checkpointer
            config=config,
        ):
            final_state = output

        # Sau interrupt, báº¡n cÃ³ thá»ƒ check state vÃ  resume
        current_state = self.graph.get_state(config)
        if current_state.next:  # Náº¿u bá»‹ interrupt (paused)
            print("Graph paused at:", current_state.next)
            # Láº¥y input tá»« ngÆ°á»i dÃ¹ng
            user_input = input("Nháº­p input cá»§a báº¡n (hoáº·c 'skip' Ä‘á»ƒ bá» qua): ").strip()

            if user_input.lower() != 'skip':
                # Append input vÃ o state.messages
                updated_messages = current_state.values["messages"] + [HumanMessage(content=user_input)]
                updates = {"messages": updated_messages}
                # Update state (as_node=None Ä‘á»ƒ resume cháº¡y node tiáº¿p theo)
                self.graph.update_state(config, updates, as_node=None)
            else:
                print("\nâŠ˜ Bá» qua append input")

            # Resume stream tá»« state Ä‘Ã£ update
            for output in self.graph.stream(None, config):
                final_state = output

        return final_state or {}  # Return final_state hoáº·c empty náº¿u khÃ´ng cÃ³