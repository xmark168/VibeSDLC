"""L·ªõp n√†y ch·ª©a LangGraph Agent/workflow v√† c√°c t∆∞∆°ng t√°c v·ªõi LLM cho gatherer agent."""

import json
import os
import re
from typing import Any, Literal

from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, HumanMessage,AIMessage
from langchain_openai import ChatOpenAI
from langfuse.langchain import CallbackHandler
from langgraph.graph import END, START, StateGraph
from pydantic import BaseModel, Field

from templates.prompts.product_owner.gatherer import (
    EVALUATE_PROMPT,
    CLARIFY_PROMPT,
    SUGGEST_PROMPT,
    ASK_USER_PROMPT,
    GENERATE_PROMPT,
    VALIDATE_PROMPT,
    FINALIZE_PROMPT,
    EDIT_MODE_PROMPT,
)
from langgraph.checkpoint.memory import MemorySaver

load_dotenv()


class EvaluateOutput(BaseModel):
    gaps: list[str] = Field(description="Danh s√°ch c√°c th√¥ng tin c√≤n thi·∫øu")
    score: float = Field(description="ƒêi·ªÉm ƒë√°nh gi√° ƒë·ªô ƒë·∫ßy ƒë·ªß", ge=0.0, le=1.0)
    status: Literal["incomplete", "done"] = Field(description="Tr·∫°ng th√°i: 'incomplete' n·∫øu score < 0.8, 'done' n·∫øu score >= 0.8")
    confidence: float = Field(description="ƒê·ªô tin c·∫≠y ƒë√°nh gi√°", ge=0.0, le=1.0)
    message: str = Field(description="L√Ω do")


class EvaluateMessageOutput(BaseModel):
    is_unclear: bool = Field(description="True n·∫øu message m∆° h·ªì/kh√¥ng r√µ r√†ng, False n·∫øu r√µ r√†ng")
    reason: str = Field(description="L√Ω do ƒë√°nh gi√° message l√† unclear ho·∫∑c clear")


class ClarifyOutput(BaseModel):
    summary: str = Field(description="T√≥m t·∫Øt nh·ªØng g√¨ ƒë√£ hi·ªÉu t·ª´ cu·ªôc h·ªôi tho·∫°i")
    unclear_points: list[str] = Field(description="Danh s√°ch c√°c ƒëi·ªÉm c√≤n m∆° h·ªì ho·∫∑c c·∫ßn l√†m r√µ")
    clarified_gaps: list[str] = Field(description="Danh s√°ch gaps ƒë√£ ƒë∆∞·ª£c l√†m r√µ v√† c·∫ßn ∆∞u ti√™n thu th·∫≠p")
    message_to_user: str = Field(description="Th√¥ng ƒëi·ªáp g·ª≠i ƒë·∫øn user ƒë·ªÉ x√°c nh·∫≠n hi·ªÉu bi·∫øt v√† y√™u c·∫ßu l√†m r√µ")


class FilledGap(BaseModel):
    gap_name: str = Field(description="T√™n c·ªßa gap")
    suggested_value: str = Field(description="Gi√° tr·ªã g·ª£i √Ω ƒë·ªÉ fill gap")
    reason: str = Field(description="L√Ω do ng·∫Øn g·ªçn t·∫°i sao g·ª£i √Ω gi√° tr·ªã n√†y")


class SuggestOutput(BaseModel):
    prioritized_gaps: list[str] = Field(description="Danh s√°ch gaps c√≤n l·∫°i ch∆∞a fill ƒë∆∞·ª£c, s·∫Øp x·∫øp theo ƒë·ªô ∆∞u ti√™n")
    filled_gaps: list[FilledGap] = Field(description="Danh s√°ch c√°c gaps ƒë√£ g·ª£i √Ω fill v·ªõi gi√° tr·ªã v√† l√Ω do")


class CollectInputsOutput(BaseModel):
    total_messages: int = Field(description="T·ªïng s·ªë messages trong context")
    new_input_received: bool = Field(description="True n·∫øu c√≥ input m·ªõi t·ª´ user")
    last_message_type: str = Field(description="Lo·∫°i message cu·ªëi: human ho·∫∑c ai")
    last_message_preview: str = Field(description="Preview 200 k√Ω t·ª± ƒë·∫ßu c·ªßa message cu·ªëi")
    context_summary: str = Field(description="T√≥m t·∫Øt ng·∫Øn g·ªçn context hi·ªán t·∫°i")


class AskUserOutput(BaseModel):
    questions: list[str] = Field(description="Danh s√°ch t·ªëi ƒëa 3 c√¢u h·ªèi ƒë·ªÉ thu th·∫≠p th√¥ng tin cho c√°c gaps")


class WaitForUserOutput(BaseModel):
    has_responses: bool = Field(description="True n·∫øu user ƒë√£ tr·∫£ l·ªùi √≠t nh·∫•t 1 c√¢u h·ªèi")
    answered_count: int = Field(description="S·ªë c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c tr·∫£ l·ªùi")
    skipped_count: int = Field(description="S·ªë c√¢u h·ªèi b·ªã b·ªè qua")
    skip_all: bool = Field(description="True n·∫øu user ch·ªçn skip_all")
    user_responses: list[dict[str, str]] = Field(description="Danh s√°ch {question, answer} c·ªßa c√°c c√¢u ƒë√£ tr·∫£ l·ªùi")
    status: str = Field(description="Tr·∫°ng th√°i: user_responded, skipped_all, no_responses, ho·∫∑c error")
    message: str = Field(description="Th√¥ng ƒëi·ªáp t√≥m t·∫Øt k·∫øt qu·∫£ thu th·∫≠p")


class GenerateOutput(BaseModel):
    product_name: str = Field(description="T√™n s·∫£n ph·∫©m")
    description: str = Field(description="M√¥ t·∫£ chi ti·∫øt s·∫£n ph·∫©m")
    target_audience: list[str] = Field(description="Danh s√°ch ƒë·ªëi t∆∞·ª£ng m·ª•c ti√™u")
    key_features: list[str] = Field(description="Danh s√°ch t√≠nh nƒÉng ch√≠nh")
    benefits: list[str] = Field(description="Danh s√°ch l·ª£i √≠ch")
    competitors: list[str] = Field(default_factory=list, description="Danh s√°ch ƒë·ªëi th·ªß c·∫°nh tranh")
    completeness_note: str = Field(description="Ghi ch√∫ v·ªÅ m·ª©c ƒë·ªô ho√†n thi·ªán")


class EditModeOutput(BaseModel):
    product_name: str = Field(description="T√™n s·∫£n ph·∫©m")
    description: str = Field(description="M√¥ t·∫£ chi ti·∫øt s·∫£n ph·∫©m")
    target_audience: list[str] = Field(description="Danh s√°ch ƒë·ªëi t∆∞·ª£ng m·ª•c ti√™u")
    key_features: list[str] = Field(description="Danh s√°ch t√≠nh nƒÉng ch√≠nh")
    benefits: list[str] = Field(description="Danh s√°ch l·ª£i √≠ch")
    competitors: list[str] = Field(default_factory=list, description="Danh s√°ch ƒë·ªëi th·ªß c·∫°nh tranh")
    completeness_note: str = Field(description="Ghi ch√∫ v·ªÅ thay ƒë·ªïi ƒë√£ √°p d·ª•ng")


class ValidateOutput(BaseModel):
    is_valid: bool = Field(description="True n·∫øu brief ƒë·∫°t y√™u c·∫ßu t·ªëi thi·ªÉu")
    confidence_score: float = Field(description="ƒê·ªô tin c·∫≠y c·ªßa brief", ge=0.0, le=1.0)
    completeness_score: float = Field(description="ƒêi·ªÉm ƒë√°nh gi√° ƒë·ªô ƒë·∫ßy ƒë·ªß", ge=0.0, le=1.0)
    missing_fields: list[str] = Field(description="Danh s√°ch fields c√≤n thi·∫øu ho·∫∑c ch∆∞a ƒë·∫ßy ƒë·ªß")
    validation_message: str = Field(description="Gi·∫£i th√≠ch ng·∫Øn g·ªçn k·∫øt qu·∫£ validation")


class FinalizeOutput(BaseModel):
    product_name: str = Field(description="T√™n s·∫£n ph·∫©m")
    executive_summary: str = Field(description="T√≥m t·∫Øt ƒëi·ªÅu h√†nh 2-3 c√¢u")
    target_users: str = Field(description="ƒê·ªëi t∆∞·ª£ng m·ª•c ti√™u ch√≠nh (1 c√¢u ng·∫Øn)")
    top_features: list[str] = Field(description="Danh s√°ch 3-5 t√≠nh nƒÉng n·ªïi b·∫≠t")
    core_value: str = Field(description="Gi√° tr·ªã c·ªët l√µi mang l·∫°i (1-2 c√¢u)")
    summary_markdown: str = Field(description="T√≥m t·∫Øt ƒë·∫ßy ƒë·ªß theo format markdown")


class State(BaseModel):
    """Tr·∫°ng th√°i cho quy tr√¨nh l√†m vi·ªác c·ªßa gatherer agent."""

    messages: list[BaseMessage] = Field(default_factory=list)
    iteration_count: int = 0
    max_iterations: int = 5
    retry_count: int = 0
    gaps: list[str] = Field(default_factory=list)
    score: float = 0.0
    status: str = "initial"
    confidence: float = 0.0
    message: str = ""
    brief: dict =  Field(default_factory=dict)
    incomplete_flag: bool = False
    questions: list[str] = Field(default_factory=list)
    unclear_input: list[str] = Field(default_factory=list)
    user_choice: Literal["approve", "edit", "regenerate", "skip", ""] = ""
    edit_changes: str = ""
    user_skipped: bool = False


class GathererAgent:
    """Gatherer Agent ƒë·ªÉ thu th·∫≠p th√¥ng tin s·∫£n ph·∫©m gi√∫p t·∫°o backlog trong t∆∞∆°ng lai."""

    def __init__(self, session_id: str | None = None, user_id: str | None = None):
        """Kh·ªüi t·∫°o gatherer agent.

        Args:
            session_id: Session ID t√πy ch·ªçn ƒë·ªÉ theo d√µi
            user_id: User ID t√πy ch·ªçn ƒë·ªÉ theo d√µi
        """
        self.session_id = session_id
        self.user_id = user_id

        # Initialize Langfuse callback handler
        self.langfuse_handler = CallbackHandler()

        self.graph = self._build_graph()

    def _llm(self, model: str, temperature: str) -> ChatOpenAI:
        try:
            llm = ChatOpenAI(
                model= model,
                temperature= temperature,
                api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_BASE_URL"),
            )
            return llm
        except Exception:
            # Avoid noisy context warnings when no active span
            return ""

    def _build_graph(self) -> StateGraph:
        """X√¢y d·ª±ng quy tr√¨nh l√†m vi·ªác LangGraph."""
        graph_builder = StateGraph(State)

        # Add nodes
        graph_builder.add_node("initialize", self._initialize)
        graph_builder.add_node("evaluate", self.evaluate)
        graph_builder.add_node("clarify", self.clarify)
        graph_builder.add_node("suggest", self.suggest)
        graph_builder.add_node("ask_user", self.ask_user)
        graph_builder.add_node("increment_iteration", self.increment_iteration)
        graph_builder.add_node("wait_for_user", self.wait_for_user)
        graph_builder.add_node("generate", self.generate)
        graph_builder.add_node("validate", self.validate)
        graph_builder.add_node("retry_decision", self.retry_decision)
        graph_builder.add_node("preview", self.preview)
        graph_builder.add_node("edit_mode", self.edit_mode)
        graph_builder.add_node("finalize", self.finalize)

        # Add edges
        graph_builder.add_edge(START, "initialize")
        graph_builder.add_edge("initialize", "evaluate")  # initialize ‚Üí evaluate tr·ª±c ti·∫øp
        graph_builder.add_conditional_edges("evaluate", self.evaluate_branch)
        graph_builder.add_edge("clarify", "suggest")
        graph_builder.add_edge("suggest", "ask_user")
        graph_builder.add_edge("ask_user", "increment_iteration")
        graph_builder.add_edge("increment_iteration", "wait_for_user")
        graph_builder.add_conditional_edges("wait_for_user", self.wait_for_user_branch)
        graph_builder.add_edge("generate", "validate")
        graph_builder.add_conditional_edges("validate", self.validate_branch)
        graph_builder.add_conditional_edges("preview", self.preview_branch)
        graph_builder.add_edge("edit_mode", "validate")  # edit_mode ‚Üí validate ƒë·ªÉ re-validate
        graph_builder.add_edge("finalize", END)
        checkpointer = MemorySaver()  # Kh·ªüi t·∫°o MemorySaver
        return graph_builder.compile(
            checkpointer=checkpointer
            # Kh√¥ng c·∫ßn interrupt_before v√¨ wait_for_user ƒë√£ handle input collection
        )

    def _initialize(self, state: State) -> State:
        print(state)
        """Kh·ªüi t·∫°o tr·∫°ng th√°i."""
        return state
    def collect_inputs(self, state: State) -> State:
        """Thu th·∫≠p th√¥ng tin b·ªï sung t·ª´ ng∆∞·ªùi d√πng ƒë·ªÉ ƒëi·ªÅn v√†o c√°c kho·∫£ng tr·ªëng th√¥ng tin.

        Theo s∆° ƒë·ªì:
        - append last_user_input v√†o messages (n·∫øu c√≥ input m·ªõi t·ª´ interrupt)
        - update memory & context
        - output structured JSON
        """
        # Check if there's new input
        new_input_received = False
        last_message_type = ""
        last_message_preview = ""

        if state.messages:
            last_msg = state.messages[-1]
            last_message_type = last_msg.type
            last_message_preview = last_msg.content[:200] if hasattr(last_msg, 'content') else str(last_msg)[:200]
            new_input_received = last_msg.type == "human"

        # Generate context summary using LLM
        if len(state.messages) > 0:
            formatted_messages = "\n".join([
                f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content[:100]}"
                for msg in state.messages[-5:]  # Last 5 messages for summary
            ])

            summary_prompt = f"""T√≥m t·∫Øt ng·∫Øn g·ªçn (1-2 c√¢u) n·ªôi dung ch√≠nh c·ªßa cu·ªôc h·ªôi tho·∫°i sau:

{formatted_messages}

Ch·ªâ tr·∫£ v·ªÅ t√≥m t·∫Øt, kh√¥ng th√™m gi·∫£i th√≠ch."""

            try:
                summary_response = self._llm("gpt-4.1", 0.1).invoke([HumanMessage(content=summary_prompt)])
                context_summary = summary_response.content.strip()
            except Exception:
                context_summary = "Cu·ªôc h·ªôi tho·∫°i ƒëang trong qu√° tr√¨nh thu th·∫≠p th√¥ng tin"
        else:
            context_summary = "Ch∆∞a c√≥ th√¥ng tin ƒë∆∞·ª£c thu th·∫≠p"

        # Create structured output
        output = CollectInputsOutput(
            total_messages=len(state.messages),
            new_input_received=new_input_received,
            last_message_type=last_message_type or "none",
            last_message_preview=last_message_preview or "Ch∆∞a c√≥ message",
            context_summary=context_summary
        )

        # Print structured output
        print(f"\nüì• Structured Output t·ª´ collect_inputs:")
        print(json.dumps(output.model_dump(), ensure_ascii=False, indent=2))
        print()

        return state
    
    def evaluate_message(self, message: str) -> bool:
        """ƒê√°nh gi√° xem message cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng c√≥ unclear hay kh√¥ng b·∫±ng regex patterns."""
        message_lower = message.lower().strip()

        # Patterns cho unclear messages (Vietnamese best practices)
        unclear_patterns = [
            # ƒê·∫°i t·ª´ m∆° h·ªì
            r'\b(n√≥|c√°i ƒë√≥|c√°i n√†y|th·ª© ƒë√≥|th·ª© n√†y|ch·ªó ƒë√≥|ch·ªó n√†y|c√°i kia|th·∫±ng ƒë√≥)\b',
            # Tham chi·∫øu m∆° h·ªì
            r'\b(nh∆∞ tr√™n|nh∆∞ v·∫≠y|nh∆∞ th·∫ø|y nh∆∞|y chang|t∆∞∆°ng t·ª±|gi·ªëng v·∫≠y|nh∆∞ kia)\b',
            # Thi·∫øu th√¥ng tin c·ª• th·ªÉ (c√¢u qu√° ng·∫Øn < 10 k√Ω t·ª±)
            r'^.{1,10}$',
            # C√¢u h·ªèi m∆° h·ªì kh√¥ng c√≥ ng·ªØ c·∫£nh
            r'^\s*(sao|th·∫ø n√†o|nh∆∞ n√†o|ra sao|g√¨|√†|h·∫£|·ª´|uh|uhm)\s*\??\s*$',
            # Ch·ªâ c√≥ yes/no kh√¥ng c√≥ ng·ªØ c·∫£nh
            r'^\s*(c√≥|kh√¥ng|ok|ƒë∆∞·ª£c|r·ªìi|·ª´|uh|yes|no|yeah|nope)\s*$',
        ]

        # Check n·∫øu match b·∫•t k·ª≥ pattern n√†o
        for pattern in unclear_patterns:
            if re.search(pattern, message_lower):
                return True 

        # Clear n·∫øu kh√¥ng match pattern n√†o
        return False

    def evaluate(self, state: State) -> State:
        """ƒê√°nh gi√° ƒë·ªô ƒë·∫ßy ƒë·ªß c·ªßa cu·ªôc h·ªôi tho·∫°i ƒë·ªÉ t·∫°o b·∫£n t√≥m t·∫Øt s·ª≠ d·ª•ng structured output."""
        # Evaluate last message if it exists and is from user
        if state.messages:
            last_message = state.messages[-1]
            if last_message.type == "human":
                is_unclear = self.evaluate_message(last_message.content)
                if is_unclear:
                    # Add unclear message to unclear_input list
                    state.unclear_input.append(last_message.content)

        # Format messages
        formatted_messages = "\n".join([
            f"{i}. [{'User' if msg.type=='human' else 'Assistant'}]: "
            f"{msg.content if hasattr(msg, 'content') else str(msg)}"
            for i, msg in enumerate(state.messages, 1)
        ]) if state.messages else "Ch∆∞a c√≥ th√¥ng tin n√†o ƒë∆∞·ª£c thu th·∫≠p."

        prompt = EVALUATE_PROMPT.format(messages=formatted_messages)

        # Use structured output with Pydantic model
        structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(EvaluateOutput)
        evaluation = structured_llm.invoke([HumanMessage(content=prompt)])

        # Update state
        state.gaps = evaluation.gaps
        state.score = evaluation.score
        state.confidence = evaluation.confidence
        state.message = evaluation.message

        # Override status based on score to ensure correctness
        state.status = "done" if evaluation.score >= 0.8 else "incomplete"

        return state

    
    # def force_generate(self, state: State) -> State:
    #     prompt = f"T·∫°o m·ªôt brief s·ª≠ d·ª•ng th√¥ng tin c√≥ s·∫µn t·ª´ cu·ªôc tr√≤ chuy·ªán, ngay c·∫£ khi ch∆∞a ho√†n ch·ªânh:\n{state.messages}"
    #     response = self.llm.invoke(prompt)
    #     state.brief = response.content
    #     state.incomplete_flag = True
    #     return state

    def clarify(self, state: State) -> State:
        """L√†m r√µ c√°c th√¥ng tin m∆° h·ªì ho·∫∑c kh√¥ng r√µ r√†ng trong cu·ªôc h·ªôi tho·∫°i."""
        # Format messages
        formatted_messages = "\n".join([
            f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content}"
            for msg in state.messages
        ])

        # Format unclear inputs
        unclear_inputs = "\n".join([f"- {unclear}" for unclear in state.unclear_input]) if state.unclear_input else "Kh√¥ng c√≥"

        # Format gaps
        formatted_gaps = "\n".join([f"- {gap}" for gap in state.gaps]) if state.gaps else "Kh√¥ng c√≥"

        prompt = CLARIFY_PROMPT.format(
            messages=formatted_messages,
            unclear_inputs=unclear_inputs,
            gaps=formatted_gaps
        )

        # Use structured output with Pydantic model
        structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(ClarifyOutput)
        clarify_result = structured_llm.invoke([HumanMessage(content=prompt)])

        # Update state with clarified gaps for suggest node
        state.gaps = clarify_result.clarified_gaps

        # Append message to user
        state.messages.append(AIMessage(content=clarify_result.message_to_user))

        return state

    def suggest(self, state: State) -> State:
        """G·ª£i √Ω n·ªôi dung ƒë·ªÉ t·ª± ƒë·ªông fill c√°c gaps quan tr·ªçng, gi√∫p thu th·∫≠p th√¥ng tin nhanh h∆°n m√† kh√¥ng b·∫Øt user nghƒ© t·∫•t c·∫£."""
        # Format gaps
        formatted_gaps = "\n".join([f"- {gap}" for gap in state.gaps]) if state.gaps else "Kh√¥ng c√≥ gaps"

        # Format messages - limit length to avoid overload
        formatted_messages = "\n".join([
            f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content[:500]}"  # Limit each message to 500 chars
            for msg in state.messages[-10:]  # Only take last 10 messages
        ])

        prompt = SUGGEST_PROMPT.format(
            gaps=formatted_gaps,
            messages=formatted_messages
        )

        try:
            # Use structured output with Pydantic model
            structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(SuggestOutput)
            suggest_result = structured_llm.invoke([HumanMessage(content=prompt)])

            # Update state with prioritized gaps
            state.gaps = suggest_result.prioritized_gaps

            # Store filled gaps if any
            if suggest_result.filled_gaps:
                filled_msg = "C√°c th√¥ng tin ƒë∆∞·ª£c g·ª£i √Ω t·ª± ƒë·ªông fill d·ª±a tr√™n ng·ªØ c·∫£nh:\n\n" + "\n\n".join(
                    [f"**{fg.gap_name}**\n‚Ä¢ Gi√° tr·ªã: {fg.suggested_value}\n‚Ä¢ L√Ω do: {fg.reason}"
                     for fg in suggest_result.filled_gaps]
                ) + "\n\nN·∫øu kh√¥ng ch√≠nh x√°c, vui l√≤ng ch·ªânh s·ª≠a."
                state.messages.append(AIMessage(content=filled_msg))
        except Exception as e:
            print(f"Error in suggest: {e}")
            # Fallback: keep gaps as is if error occurs
            pass

        return state
    
    def ask_user(self, state: State) -> State:
        """T·∫°o c√¢u h·ªèi ƒë·ªÉ thu th·∫≠p th√¥ng tin cho c√°c gaps c√≤n thi·∫øu."""
        # Format gaps
        formatted_gaps = "\n".join([f"- {gap}" for gap in state.gaps]) if state.gaps else "Kh√¥ng c√≥ gaps"

        # Format messages - limit to last 10 messages
        formatted_messages = "\n".join([
            f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content[:500]}"
            for msg in state.messages[-10:]
        ])

        prompt = ASK_USER_PROMPT.format(
            gaps=formatted_gaps,
            messages=formatted_messages
        )

        try:
            # Use structured output with Pydantic model
            structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(AskUserOutput)
            ask_result = structured_llm.invoke([HumanMessage(content=prompt)])
            state.questions = ask_result.questions
            state.status = "awaiting_user"  # Set status ƒë·ªÉ ch·ªù user ·ªü wait_for_user node
        except Exception as e:
            print(f"Error in ask_user: {e}")
            state.questions = []
            state.status = "error_generating_questions"

        # Kh√¥ng h·ªèi user ·ªü ƒë√¢y n·ªØa, ch·ªâ generate questions
        print(f"\nüìù ƒê√£ t·∫°o {len(state.questions)} c√¢u h·ªèi ƒë·ªÉ thu th·∫≠p th√¥ng tin.")

        return state

    def increment_iteration(self, state: State) -> State:
        """TƒÉng iteration count v√† checkpoint state ƒë·ªÉ c√≥ th·ªÉ resume sau n√†y."""
        state.iteration_count += 1
        print(f"\n=== Iteration {state.iteration_count}/{state.max_iterations} completed ===")
        print(f"Current gaps: {len(state.gaps)}")
        print(f"Score: {state.score}, Confidence: {state.confidence}, Status: {state.status}")

        # Checkpoint is automatically saved by LangGraph MemorySaver after each node execution
        return state

    def wait_for_user(self, state: State) -> State:
        """H·ªèi user t·ª´ng c√¢u m·ªôt v√† ƒë·ª£i response v·ªõi timeout 10 ph√∫t cho m·ªói c√¢u. Tr·∫£ v·ªÅ structured output."""
        import signal

        print("\n" + "="*60)
        print("üí¨ PH·∫¶N H·ªéI ƒê√ÅP - Thu th·∫≠p th√¥ng tin")
        print("="*60)
        print("üí° B·∫°n c√≥ th·ªÉ:")
        print("  - Tr·∫£ l·ªùi t·ª´ng c√¢u h·ªèi")
        print("  - G√µ 'skip' ƒë·ªÉ b·ªè qua c√¢u hi·ªán t·∫°i")
        print("  - G√µ 'skip_all' ƒë·ªÉ b·ªè qua t·∫•t c·∫£ v√† t·∫°o brief v·ªõi th√¥ng tin hi·ªán c√≥")
        print("  - Timeout: 10 ph√∫t cho m·ªói c√¢u h·ªèi")
        print("="*60 + "\n")

        def timeout_handler(signum, frame):
            raise TimeoutError("Timeout")

        has_responses = False
        skip_all = False
        answered_count = 0
        skipped_count = 0
        user_responses = []

        for idx, question in enumerate(state.questions, 1):
            if skip_all:
                break

            # Append question to messages
            state.messages.append(AIMessage(content=question))

            print(f"\n[C√¢u h·ªèi {idx}/{len(state.questions)}]")
            print(f"‚ùì {question}\n")

            try:
                # Set timeout 10 minutes for each question (Unix only)
                if hasattr(signal, 'SIGALRM'):
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(600)  # 10 minutes

                user_input = input("üë§ C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n: ").strip()

                if hasattr(signal, 'SIGALRM'):
                    signal.alarm(0)  # Cancel alarm

                # Check user input
                if user_input.lower() == 'skip_all':
                    skip_all = True
                    print("\n‚äò B·∫°n ƒë√£ ch·ªçn b·ªè qua t·∫•t c·∫£ c√¢u h·ªèi c√≤n l·∫°i.")
                    break
                elif user_input.lower() == 'skip':
                    print("‚äò B·ªè qua c√¢u n√†y.\n")
                    skipped_count += 1
                    continue
                elif user_input:
                    state.messages.append(HumanMessage(content=user_input))
                    user_responses.append({"question": question, "answer": user_input})
                    has_responses = True
                    answered_count += 1
                    print("‚úì ƒê√£ ghi nh·∫≠n c√¢u tr·∫£ l·ªùi.\n")
                else:
                    print("‚ö† C√¢u tr·∫£ l·ªùi tr·ªëng, b·ªè qua.\n")
                    skipped_count += 1

            except TimeoutError:
                print(f"\n‚è∞ Timeout cho c√¢u h·ªèi {idx}. B·ªè qua c√¢u n√†y.")
                skipped_count += 1
                continue
            except Exception as e:
                print(f"\n‚ùå L·ªói: {e}. B·ªè qua c√¢u n√†y.")
                skipped_count += 1
                continue

        # Create structured output
        if skip_all:
            output = WaitForUserOutput(
                has_responses=has_responses,
                answered_count=answered_count,
                skipped_count=skipped_count,
                skip_all=True,
                user_responses=user_responses,
                status="skipped_all",
                message=f"ƒê√£ b·ªè qua t·∫•t c·∫£ c√¢u h·ªèi. Tr·∫£ l·ªùi: {answered_count}, B·ªè qua: {skipped_count}"
            )
            state.user_skipped = True
            state.status = "skipped_all"
            print("\n" + "="*60)
            print("‚äò ƒê√£ b·ªè qua t·∫•t c·∫£ c√¢u h·ªèi. S·∫Ω t·∫°o brief v·ªõi th√¥ng tin hi·ªán c√≥.")
            print("="*60 + "\n")
        elif has_responses:
            output = WaitForUserOutput(
                has_responses=True,
                answered_count=answered_count,
                skipped_count=skipped_count,
                skip_all=False,
                user_responses=user_responses,
                status="user_responded",
                message=f"Thu th·∫≠p th√†nh c√¥ng {answered_count} c√¢u tr·∫£ l·ªùi, b·ªè qua {skipped_count} c√¢u"
            )
            state.user_skipped = False
            state.status = "user_responded"
            print("\n" + "="*60)
            print("‚úì ƒê√£ ho√†n th√†nh ph·∫ßn h·ªèi ƒë√°p. Ti·∫øp t·ª•c thu th·∫≠p th√¥ng tin...")
            print("="*60 + "\n")
        else:
            output = WaitForUserOutput(
                has_responses=False,
                answered_count=0,
                skipped_count=skipped_count,
                skip_all=False,
                user_responses=[],
                status="no_responses",
                message=f"Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi n√†o. B·ªè qua: {skipped_count} c√¢u"
            )
            state.user_skipped = True
            state.status = "no_responses"
            print("\n" + "="*60)
            print("‚ö† Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi n√†o. S·∫Ω t·∫°o brief v·ªõi th√¥ng tin hi·ªán c√≥.")
            print("="*60 + "\n")

        # Print structured output as JSON
        print("\nüìä Structured Output:")
        print(json.dumps(output.model_dump(), ensure_ascii=False, indent=2))
        print()

        return state
    
    def generate(self, state: State) -> State:
        """T·∫°o Product Brief ho√†n ch·ªânh t·ª´ th√¥ng tin ƒë√£ thu th·∫≠p, output structured JSON."""
        # Format messages
        formatted_messages = "\n".join([
            f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content}"
            for msg in state.messages
        ])

        prompt = GENERATE_PROMPT.format(messages=formatted_messages)

        try:
            # Use structured output with Pydantic model
            structured_llm = self._llm("gpt-4.1", 0.3).with_structured_output(GenerateOutput)
            brief_output = structured_llm.invoke([HumanMessage(content=prompt)])

            # Store in state.brief as dict
            state.brief = brief_output.model_dump()

            # Print structured output
            print("\n" + "="*80)
            print("üìÑ PRODUCT BRIEF ƒê√É T·∫†O")
            print("="*80)
            print(json.dumps(state.brief, ensure_ascii=False, indent=2))
            print("="*80 + "\n")

        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o brief: {e}")
            # Fallback: t·∫°o brief ƒë∆°n gi·∫£n
            state.brief = {
                "product_name": "Ch∆∞a x√°c ƒë·ªãnh",
                "description": f"Brief ƒë∆∞·ª£c t·∫°o t·ª´ {len(state.messages)} messages",
                "target_audience": [],
                "key_features": [],
                "benefits": [],
                "competitors": [],
                "completeness_note": f"L·ªói khi generate: {str(e)}"
            }

        return state

    def validate(self, state: State) -> State:
        """Validate brief ƒë√£ t·∫°o b·∫±ng llm_reflect - ki·ªÉm tra completeness v√† calculate confidence_score.

        Theo s∆° ƒë·ªì:
        - Input: brief t·ª´ force_generate
        - S·ª≠ d·ª•ng llm_reflect ƒë·ªÉ ph√¢n t√≠ch brief
        - Output: ValidateOutput structured JSON
        - Branch logic s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü validate_branch
        """
        # Format brief for validation
        brief_text = json.dumps(state.brief, ensure_ascii=False, indent=2) if state.brief else "Ch∆∞a c√≥ brief"

        # Format messages context
        formatted_messages = "\n".join([
            f"[{'User' if msg.type=='human' else 'Assistant'}]: {msg.content[:300]}"
            for msg in state.messages[-10:]  # Last 10 messages
        ])

        prompt = VALIDATE_PROMPT.format(
            brief=brief_text,
            messages=formatted_messages
        )

        try:
            # Use structured output with Pydantic model
            structured_llm = self._llm("gpt-4.1", 0.1).with_structured_output(ValidateOutput)
            validation_result = structured_llm.invoke([HumanMessage(content=prompt)])

            # Update state
            state.confidence = validation_result.confidence_score
            state.score = validation_result.completeness_score

            # Print validation result
            print("\n" + "="*60)
            print("‚úì Validation Result:")
            print(f"  - Valid: {validation_result.is_valid}")
            print(f"  - Confidence: {state.confidence:.2f}")
            print(f"  - Completeness: {state.score:.2f}")
            print(f"  - Message: {validation_result.validation_message}")

            if validation_result.missing_fields:
                print(f"  - Missing: {', '.join(validation_result.missing_fields)}")

            print("="*60 + "\n")

            # Print structured output
            print("\nüìä Structured Output t·ª´ validate:")
            print(json.dumps(validation_result.model_dump(), ensure_ascii=False, indent=2))
            print()

        except Exception as e:
            print(f"‚ùå Error in validate: {e}")
            # Fallback: set low confidence/score to trigger retry or force to preview
            state.confidence = 0.3
            state.score = 0.3

        return state

    def retry_decision(self, state: State) -> State:
        """TƒÉng retry_count ƒë·ªÉ theo d√µi s·ªë l·∫ßn retry validate.

        Theo s∆° ƒë·ªì:
        - Input: brief kh√¥ng h·ª£p l·ªá ho·∫∑c confidence ‚â§ 0.7
        - TƒÉng retry_count
        - Branch logic s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü retry_decision_branch
        """
        state.retry_count += 1

        print("\n" + "="*60)
        print(f"üîÑ RETRY DECISION - L·∫ßn th·ª≠ {state.retry_count}")
        print(f"  - Confidence: {state.confidence:.2f}")
        print(f"  - Completeness: {state.score:.2f}")
        print("="*60 + "\n")

        return state

    def preview(self, state: State) -> State:
        """Hi·ªÉn th·ªã brief cho user v√† h·ªèi: Approve/Edit/Regenerate.

        Theo s∆° ƒë·ªì:
        - Format brief v√† show to user
        - H·ªèi user l·ª±a ch·ªçn: Approve/Edit/Regenerate (v·ªõi timeout option)
        - C·∫≠p nh·∫≠t user_choice v√†o state
        - N·∫øu ch·ªçn Edit, thu th·∫≠p edit_changes
        """
        print("\n" + "="*80)
        print("üìã PREVIEW - XEM TR∆Ø·ªöC PRODUCT BRIEF")
        print("="*80)
        print(f"üö© C·ªù ch∆∞a ho√†n ch·ªânh: {state.incomplete_flag}")
        print("\nN·ªôi dung Brief:")
        print("-"*80)
        print(json.dumps(state.brief, ensure_ascii=False, indent=2))
        print("="*80 + "\n")

        print("üí° B·∫°n c√≥ th·ªÉ:")
        print("  1. G√µ 'approve' ƒë·ªÉ ph√™ duy·ªát brief")
        print("  2. G√µ 'edit' ƒë·ªÉ ch·ªânh s·ª≠a brief")
        print("  3. G√µ 'regenerate' ƒë·ªÉ t·∫°o l·∫°i brief")
        print()

        try:
            user_choice = input("üë§ L·ª±a ch·ªçn c·ªßa b·∫°n (approve/edit/regenerate): ").strip().lower()

            if user_choice not in ["approve", "edit", "regenerate"]:
                print(f"‚ö† L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá: '{user_choice}'. M·∫∑c ƒë·ªãnh ch·ªçn 'approve'.")
                user_choice = "approve"

            state.user_choice = user_choice

            if user_choice == "edit":
                print("\nüìù Nh·∫≠p c√°c thay ƒë·ªïi b·∫°n mu·ªën √°p d·ª•ng v√†o brief:")
                edit_changes = input("üë§ Thay ƒë·ªïi: ").strip()
                state.edit_changes = edit_changes
                print(f"‚úì ƒê√£ ghi nh·∫≠n thay ƒë·ªïi: {edit_changes[:100]}...")
            elif user_choice == "approve":
                print("‚úì B·∫°n ƒë√£ ph√™ duy·ªát brief.")
            elif user_choice == "regenerate":
                print("üîÑ S·∫Ω t·∫°o l·∫°i brief.")

        except Exception as e:
            print(f"‚ùå L·ªói khi nh·∫≠n input: {e}. M·∫∑c ƒë·ªãnh ch·ªçn 'approve'.")
            state.user_choice = "approve"

        return state

    def edit_mode(self, state: State) -> State:
        """√Åp d·ª•ng c√°c thay ƒë·ªïi t·ª´ user v√†o brief v√† re-validate.

        Theo s∆° ƒë·ªì:
        - Input: edit_changes t·ª´ preview node
        - Apply user changes v√†o brief
        - Re-validate ƒë·ªÉ ƒë·∫£m b·∫£o brief v·∫´n h·ª£p l·ªá
        """
        # Format brief
        brief_text = json.dumps(state.brief, ensure_ascii=False, indent=2)

        # Use prompt from template
        prompt = EDIT_MODE_PROMPT.format(
            brief=brief_text,
            edit_changes=state.edit_changes
        )

        try:
            # Use structured output with Pydantic model
            structured_llm = self._llm("gpt-4.1", 0.3).with_structured_output(EditModeOutput)
            edited_brief = structured_llm.invoke([HumanMessage(content=prompt)])

            # Update brief
            state.brief = edited_brief.model_dump()

            # Clear edit_changes
            state.edit_changes = ""

            print("\n" + "="*60)
            print("‚úèÔ∏è ƒê√É √ÅP D·ª§NG THAY ƒê·ªîI V√ÄO BRIEF")
            print("="*60)
            print(json.dumps(state.brief, ensure_ascii=False, indent=2))
            print("="*60 + "\n")

            # Print structured output
            print("\nüìä Structured Output t·ª´ edit_mode:")
            print(json.dumps(edited_brief.model_dump(), ensure_ascii=False, indent=2))
            print()

        except Exception as e:
            print(f"‚ùå L·ªói khi √°p d·ª•ng thay ƒë·ªïi: {e}")
            # Kh√¥ng thay ƒë·ªïi brief n·∫øu c√≥ l·ªói
            state.edit_changes = ""

        return state

    def finalize(self, state: State) -> State:
        """L∆∞u brief v√† t·∫°o summary cu·ªëi c√πng v·ªõi structured output.

        Theo s∆° ƒë·ªì:
        - Input: brief ƒë√£ ƒë∆∞·ª£c approve
        - Generate summary t·ª´ brief v·ªõi structured JSON output
        - C·∫≠p nh·∫≠t status = "completed"
        - Output: FinalizeOutput structured JSON
        """
        # Format brief
        brief_text = json.dumps(state.brief, ensure_ascii=False, indent=2)

        prompt = FINALIZE_PROMPT.format(brief=brief_text)

        try:
            # Use structured output with Pydantic model
            structured_llm = self._llm("gpt-4.1", 0.3).with_structured_output(FinalizeOutput)
            finalize_result = structured_llm.invoke([HumanMessage(content=prompt)])

            # Update state
            state.status = "completed"

            # Print final output
            print("\n" + "="*80)
            print("‚úÖ HO√ÄN T·∫§T - PRODUCT BRIEF ƒê√É ƒê∆Ø·ª¢C PH√ä DUY·ªÜT")
            print("="*80)
            print("\nüìä T√ìM T·∫ÆT CU·ªêI C√ôNG:\n")
            print(finalize_result.summary_markdown)
            print("\n" + "="*80)
            print(f"üìà Th·ªëng k√™:")
            print(f"  - S·ªë l·∫ßn l·∫∑p: {state.iteration_count}/{state.max_iterations}")
            print(f"  - S·ªë l·∫ßn retry: {state.retry_count}")
            print(f"  - Confidence score: {state.confidence:.2f}")
            print(f"  - Completeness score: {state.score:.2f}")
            print(f"  - T·ªïng s·ªë messages: {len(state.messages)}")
            print("="*80 + "\n")

            # Print structured output
            print("\nüìÑ Structured Output t·ª´ finalize:")
            print(json.dumps(finalize_result.model_dump(), ensure_ascii=False, indent=2))
            print()

        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o t√≥m t·∫Øt: {e}")
            state.status = "completed_with_errors"
            print("\n" + "="*80)
            print("‚ö† HO√ÄN T·∫§T V·ªöI L·ªñI")
            print("="*80)
            print(f"Brief ƒë√£ ƒë∆∞·ª£c l∆∞u nh∆∞ng kh√¥ng th·ªÉ t·∫°o t√≥m t·∫Øt: {str(e)}")
            print("="*80 + "\n")

        return state

    

    # Conditional branches
    def evaluate_branch(self, state: State) -> str:
        """Quy·∫øt ƒë·ªãnh lu·ªìng sau evaluate node theo s∆° ƒë·ªì."""
        # Priority 1: status == done OR score >= 0.8 ‚Üí generate (check TR∆Ø·ªöC ƒë·ªÉ tr√°nh b·ªã override)
        if state.status == "done" or state.score >= 0.8:
            return "generate"
        # Priority 2: low confidence ‚Üí clarify
        elif state.confidence <= 0.6:
            return "clarify"
        # Priority 3: c√≥ gaps AND confidence > 0.6 ‚Üí suggest (continue collecting)
        elif len(state.gaps) > 0 and state.confidence > 0.6:
            return "suggest"
        else:
            # Fallback: generate n·∫øu kh√¥ng match ƒëi·ªÅu ki·ªán n√†o
            return "generate"

    def wait_for_user_branch(self, state: State) -> str:
        """Quy·∫øt ƒë·ªãnh next node sau wait_for_user."""
        if state.user_skipped or state.status in ["skipped_all", "no_responses", "error_generating_questions"]:
            # User skipped/no responses ‚Üí generate v·ªõi th√¥ng tin hi·ªán c√≥
            return "generate"
        elif state.status == "user_responded":
            # User responded ‚Üí evaluate l·∫°i v·ªõi th√¥ng tin m·ªõi
            return "evaluate"
        else:
            # Default: generate
            return "generate"

    def validate_branch(self, state: State) -> str:
        """Quy·∫øt ƒë·ªãnh lu·ªìng sau validate node theo s∆° ƒë·ªì.

        Theo s∆° ƒë·ªì:
        - N·∫øu valid AND confidence > 0.7 ‚Üí preview
        - N·∫øu invalid OR confidence ‚â§ 0.7 ‚Üí retry_decision
        """
        # Check validation result
        if state.confidence > 0.7:
            # Valid and high confidence ‚Üí preview
            return "preview"
        else:
            # Invalid or low confidence ‚Üí retry_decision
            return "retry_decision"

    def preview_branch(self, state: State) -> str:
        """Quy·∫øt ƒë·ªãnh lu·ªìng sau preview node theo s∆° ƒë·ªì.

        Theo s∆° ƒë·ªì:
        - N·∫øu user ch·ªçn "approve" ‚Üí finalize
        - N·∫øu user ch·ªçn "edit" ‚Üí edit_mode
        - N·∫øu user ch·ªçn "regenerate" ‚Üí generate
        """
        if state.user_choice == "approve":
            return "finalize"
        elif state.user_choice == "edit":
            return "edit_mode"
        elif state.user_choice == "regenerate":
            return "generate"
        else:
            # Default: finalize
            return "finalize"

    def run(self, initial_context: str = "", thread_id: str | None = None) -> dict[str, Any]:
        """Ch·∫°y quy tr√¨nh l√†m vi·ªác c·ªßa gatherer agent.

        Args:
            initial_context: Ng·ªØ c·∫£nh ban ƒë·∫ßu ho·∫∑c y√™u c·∫ßu cho b·∫£n t√≥m t·∫Øt s·∫£n ph·∫©m
            thread_id: ID ƒë·ªÉ resume state (n·∫øu None, d√πng session_id ho·∫∑c default)

        Returns:
            dict: Tr·∫°ng th√°i cu·ªëi c√πng ch·ª©a b·∫£n t√≥m t·∫Øt ƒë√£ t·∫°o v√† c√°c ch·ªâ s·ªë ƒë√°nh gi√°
        """
        if thread_id is None:
            thread_id = self.session_id or "default_thread"  # Default n·∫øu kh√¥ng c√≥

        initial_state = State(
            messages=[HumanMessage(content=initial_context)] if initial_context else []
        )

        config = {
            "configurable": {"thread_id": thread_id},  # ƒê·ªÉ checkpointer l∆∞u theo thread
            "callbacks": [self.langfuse_handler]
        }

        final_state = None
        for output in self.graph.stream(
            initial_state.model_dump() if initial_state else None,
            config=config,
        ):
            final_state = output

        return final_state or {}